<img src="https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png" alt="AWS" width="250"/>

# Amazon DynamoDB - Security Playbook <!-- omit in toc -->
## NIST Cybersecurity Framework Alignment <!-- omit in toc -->

**Generated By:**  
Freddie Wilson  
Tony DeMarco

*AWS Professional Services*

## Disclaimer
> The following applies to this document and all other documents, information, data, and responses (written or verbal) provided by Amazon Web Services, Inc. or any of its affiliates (collectively, "**AWS**") in connection with responding to this request and other related requests (collectively, this "**Response**"): This Response is expressly (a) informational only and provided solely for discussion purposes, (b) non-binding and not an offer to contract that can be accepted by any party, (c) provided "as is" with no representations or warranties whatsoever, and (d) based on AWS's current knowledge and may change at any time due to a variety of factors such as changes to your requirements or changes to AWS's service offerings. All obligations must be set forth in a separate, definitive written agreement between the parties. Neither party will have any liability for any failure or refusal to enter into a definitive agreement. All use of AWS's service offerings will be governed by the AWS Customer Agreement available at [http://aws.amazon.com/agreement/](http://aws.amazon.com/agreement/) (or other definitive written agreement between the parties governing the use of AWS's service offerings) (as applicable, the "**Agreement**"). If the parties have an applicable Nondisclosure Agreement ("**NDA**"), then the NDA will apply to all Confidential Information (as defined in the NDA) disclosed in connection with this Response. AWS's pricing is publicly available and subject to change in accordance with the Agreement. Pricing information (if any) provided in this Response is only an estimate and is expressly not a binding quote. Fees and charges will be based on actual usage of AWS services, which may vary from the estimates provided. Nothing in this Response will modify or supplement the terms of the Agreement or the NDA. No part of this Response may be disclosed without AWS's prior written consent. 

## Table of Contents <!-- omit in toc -->
- [Disclaimer](#disclaimer)
- [Overview](#overview)
- [Preventative Controls](#preventative-controls)
  - [1. Enforce least privilege for all DynamoDB users and roles](#1-enforce-least-privilege-for-all-dynamodb-users-and-roles)
  - [2. Data Protection Standards are Enforced](#2-data-protection-standards-are-enforced)
  - [3. Create a VPC Endpoint for DynamoDB](#3-create-a-vpc-endpoint-for-dynamodb)
- [Detective](#detective)
  - [1. Utilize DynamoDB streams to support data-plane logging](#1-utilize-dynamodb-streams-to-support-data-plane-logging)
  - [2. Log DynamoDB Operations with AWS CloudTrail](#2-log-dynamodb-operations-with-aws-cloudtrail)
- [Respond/Recover](#respondrecover)
  - [1. Create CloudWatch Alarms to monitor DynamoDB](#1-create-cloudwatch-alarms-to-monitor-dynamodb)
- [Endnotes](#endnotes)
- [Capital Group Control Statements](#capital-group-control-statements)
- [Glossary](#glossary)

## Overview
AWS provides a number of security features for Amazon DynamoDB which help you comply with the NIST Cybersecurity Framework. The following playbook will outline what the AWS best practices are, how they align to NIST, and how to implement these best practices within your organization.

These NIST Controls and Subcategories are not applicable to this service: PR.AT, PR.MA, PR.IP  (Unless stated), PR.AC-2, PR.AC-3, PR.DS-3, PR.DS-8, PR.PT-2, PR.PT-5, DE.DP1, DE.DP-2. DE.DP-3, DE.CM-3, DE.AE-5, RC, RS.MI.

These Capital Group control statements are not applicable to this service: [9](#capital-group-control-statements)

## Preventative Controls
### 1. Enforce least privilege for all DynamoDB users and roles
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.AC-1|Identities and credentials are issued, managed, verified, revoked, and audited for authorized devices, users and processes|
|PR.AC-3|Remote access is managed|
|PR.AC-4|Access permissions and authorizations are managed, incorporating the principles of least privilege and separation of duties|
|PR.AC-6|Identities are proofed and bound to credentials and asserted in interactions|
|PR.AC-7|Users, devices, and other assets are authenticated (e.g., single-factor, multi-factor) commensurate with the risk of the transaction (e.g., individuals’ security and privacy risks and other organizational risks)|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|5|AWS IAM User accounts are only to be created for use by services or products that do not support IAM Roles. Services are not allowed to create local accounts for human use within the service. All human user authentication will take place within CG’s Identity Provider.|
|8|AWS IAM User secrets, including passwords and secret access keys, are to be rotated every 90 days. Accounts created locally within any service must also have their secrets rotated every 90 days.|
|10|Administrative access to AWS resources will have MFA enabled|

**Why?** When you create IAM policies, follow the standard security advice of granting least privilege, or granting only the permissions required to perform a task. Determine what users (and roles) need to do and then craft policies that allow them to perform only those tasks.  
Start with a minimum set of permissions and grant additional permissions as necessary. Doing so is more secure than starting with permissions that are too lenient and then trying to tighten them later. 

**How?** AWS IAM policies are JSON documents that are used for setting permissions on users, groups, and roles within AWS services. There are many AWS-managed policies available to pick from, or you can create your own policies using the IAM policy builder, or by just writing the JSON policy. Each user and role should have its own set of permissions. The permissions for a role or user should only allow specific actions that that principal requires to complete its business purpose. The following examples show granular policies that only allow certain actions under certain circumstances, by using policy conditions.

Example 1 - Allows access to only two specific attributes in a table by adding the dynamodb:Attributes condition key. These attributes can be read, written, or evaluated in a conditional write or scan filter:
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "LimitAccessToSpecificAttributes",
            "Effect": "Allow",
            "Action": [
                "dynamodb:UpdateItem",
                "dynamodb:GetItem",
                "dynamodb:Query",
                "dynamodb:BatchGetItem",
                "dynamodb:Scan"
            ],
            "Resource": [
                "arn:aws:dynamodb:us-west-2:123456789012:table/GameScores"
            ],
            "Condition": {
                "ForAllValues:StringEquals": {
                    "dynamodb:Attributes": [
                        "UserId",
                        "TopScore"
                    ]
                },
                "StringEqualsIfExists": {
                    "dynamodb:Select": "SPECIFIC_ATTRIBUTES",
                    "dynamodb:ReturnValues": [
                        "NONE",
                        "UPDATED_OLD",
                        "UPDATED_NEW"
                    ]
                }
            }
        }
    ]
}
```

Example 2 - Grants permissions that allow a set of DynamoDB actions on the GamesScore table. It uses the dynamodb:LeadingKeys condition key to limit user actions only on the items whose UserID partition key value matches the Login with Amazon unique user ID for this app.:
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "FullAccessToUserItems",
            "Effect": "Allow",
            "Action": [
                "dynamodb:GetItem",
                "dynamodb:BatchGetItem",
                "dynamodb:Query",
                "dynamodb:PutItem",
                "dynamodb:UpdateItem",
                "dynamodb:DeleteItem",
                "dynamodb:BatchWriteItem"
            ],
            "Resource": [
                "arn:aws:dynamodb:us-west-2:123456789012:table/GameScores"
            ],
            "Condition": {
                "ForAllValues:StringEquals": {
                    "dynamodb:LeadingKeys": [
                        "${www.amazon.com:user_id}"
                    ]
                }
            }
        }
    ]
}
```

### 2. Data Protection Standards are Enforced
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.DS-1|Data-at-rest is protected|
|PR.DS-2|Data-in-transit is protected|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|1|All Data-at-rest must be encrypted and use a CG BYOK encryption key.|
|2|All Data-in-transit must be encrypted using certificates using CG Certificate Authority.|
|3|Keys storied in a Key Management System (KMS) should be created by Capital Group's hardware security module (HSM) and are a minimum of AES-256.|

**Why?** Amazon DynamoDB provides a highly durable storage infrastructure designed for mission-critical and primary data storage. Data is redundantly stored on multiple devices across multiple facilities in an Amazon DynamoDB Region.  
DynamoDB protects user data stored at rest and also data in transit between on-premises clients and DynamoDB, and between DynamoDB and other AWS resources within the same AWS Region.

DynamoDB encryption at rest provides an additional layer of data protection by securing your data in an encrypted table—including its primary key, local and global secondary indexes, streams, global tables, backups, and DynamoDB Accelerator (DAX) clusters whenever the data is stored in durable media. Organizational policies, industry or government regulations, and compliance requirements often require the use of encryption at rest to increase the data security of your applications. 

**How?** You can use the AWS Management Console or the AWS Command Line Interface (AWS CLI) to specify the encryption key on new tables and update the encryption keys on existing tables in Amazon DynamoDB. 

#### Creating an Encrypted Table (Console) <!-- omit in toc -->
1. Sign in to the AWS Management Console and open the DynamoDB console at <https://console.aws.amazon.com/dynamodb/>.
2. In the navigation pane on the left side of the console, choose **Tables**.
3. Choose **Create Table**. Enter a table name, primary key, and sort key.
4. In **Table settings**, make sure that **Use default settings** is NOT selected.
    ```
    Note
    If Use default settings is selected, tables are encrypted at rest with the AWS owned customer master key (CMK) at no additional cost.
    ```
5. Under **Encryption at rest**, choose **KMS – Customer managed CMK**. The key is stored in your account and is created, owned, and managed by you.
Choose **Create** to create the encrypted table. To confirm the encryption type, check the table details on the **Overview** tab.

#### Creating an Encrypted Table (AWS CLI) <!-- omit in toc -->
The example below shows how the `--sse-specification` option is used with the `create-table` CLI command:
```
aws dynamodb create-table \
  --table-name ExampleTable \
  --attribute-definitions \
      AttributeName=Primary,AttributeType=S \
      AttributeName=Sort,AttributeType=S \
  --key-schema \
      AttributeName=Primary,KeyType=HASH \
      AttributeName=Sort,KeyType=RANGE \
  --provisioned-throughput \
      ReadCapacityUnits=10,WriteCapacityUnits=5 \
  --sse-specification Enabled=true,SSEType=KMS,KMSMasterKeyId=abcd1234-abcd-1234-a123-ab1234a1b234
```
*The **SSEDescription** status of the table description is set to `ENABLED` and the **SSEType** is `KMS`.*
```json
"SSEDescription": {
  "SSEType": "KMS",
  "Status": "ENABLED",
  "KMSMasterKeyArn": "arn:aws:kms:us-east-1:123456789012:key/abcd1234-abcd-1234-a123-ab1234a1b234",
}
```

#### Updating an Encryption Key (Console) <!-- omit in toc -->
1. Sign in to the AWS Management Console and open the DynamoDB console at <https://console.aws.amazon.com/dynamodb/>.
2. In the navigation pane on the left side of the console, choose **Tables**.
3. Choose the table that you want to update, and then choose the **Overview** tab.
4. Choose **Manage Encryption**.
5. Choose **KMS – Customer managed CMK**. The key is stored in your account and is created, owned, and managed by you.
6. Then choose **Save** to update the encrypted table. To confirm the encryption type, check the table details under the **Overview** tab.

#### Updating an Encryption Key (AWS CLI) <!-- omit in toc -->
The following examples show how to update an encrypted table using the AWS CLI.
```
aws dynamodb update-table \
  --table-name ExampleTable \
  --sse-specification Enabled=true,SSEType=KMS,KMSMasterKeyId=abcd1234-abcd-1234-a123-ab1234a1b234
```
*The **SSEDescription** status of the table description is set to `ENABLED` and the **SSEType** is `KMS`.*
```json
"SSEDescription": {
  "SSEType": "KMS",
  "Status": "ENABLED",
  "KMSMasterKeyArn": "arn:aws:kms:us-east-1:123456789012:key/abcd1234-abcd-1234-a123-ab1234a1b234",
}
```

#### Amazon DynamoDB Accelerator (DAX) <!-- omit in toc -->  
Due to limitations in key management, Amazon DynamoDB Accelerator (DAX) is not an approved feature within Capital Group, and is therefore not allowed to be used in any environment.

### 3. Create a VPC Endpoint for DynamoDB
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.PT-4|Communications and control networks are protected|
|PR.PT-5|Mechanisms (e.g., failsafe, load balancing, hot swap) are implemented to achieve resilience requirements in normal and adverse situations|
|PR.AC-3|Remote access is managed|
|PR.AC-5|Network integrity is protected (e.g., network segregation, network segmentation)|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|6|Any AWS service used by CG should not be directly available to the Internet and the default route is always the CG gateway.|
|7|Use of AWS IAM accounts are restricted to CG networks.|

**Why?** A VPC endpoint for DynamoDB enables Amazon EC2 instances in your VPC to use their private IP addresses to access DynamoDB with no exposure to the public internet. Your EC2 instances do not require public IP addresses, and you don't need an internet gateway, a NAT device, or a virtual private gateway in your VPC. You use endpoint policies to control access to DynamoDB. Traffic between your VPC and the AWS service does not leave the Amazon network.

When you create a VPC endpoint for DynamoDB, any requests to a DynamoDB endpoint within the Region (for example, dynamodb.us-west-2.amazonaws.com) are routed to a private DynamoDB endpoint within the Amazon network. You don't need to modify your applications running on EC2 instances in your VPC. The endpoint name remains the same, but the route to DynamoDB stays entirely within the Amazon network, and does not access the public internet.

**How?**  
1. Determine your VPC identifier.
```
aws ec2 describe-vpcs
```
2. For the `--vpc-id` parameter, specify the VPC ID from the previous step. Use the `--route-table-ids` parameter to associate the endpoint with your route tables.
```
aws ec2 create-vpc-endpoint --vpc-id vpc-1abc234d --service-name com.amazonaws.us-east-1.dynamodb --route-table-ids rtb-11aa22bb
```
3. Verify that you can access DynamoDB through the VPC endpoint.
```
aws dynamodb list-tables
```

## Detective
### 1. Utilize DynamoDB streams to support data-plane logging
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.AE-2|Detected events are analyzed to understand attack targets and methods|
|DE.AE-3|Event data are aggregated and correlated from multiple sources and sensors|
|DE.AE-4|Impact of an event is determined|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|DE.CM-3|Personnel activity is monitored to detect potential cybersecurity events|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch|

**Why?** 
AWS CloudTrail does not support logging of DynamoDB data-plane operations, such as GetItem and PutItem

**How?** 
Capture DynamoDB table activity with DynamoDB Streams 

#### Capturing Table Activity with DynamoDB Streams<a name="Streams"> <!-- omit in toc -->

Many applications can benefit from the ability to capture changes to items stored in a DynamoDB table, at the point in time when such changes occur\. The following are some example use cases:
+ An application in one AWS Region modifies the data in a DynamoDB table\. A second application in another Region reads these data modifications and writes the data to another table, creating a replica that stays in sync with the original table\.
+ A popular mobile app modifies data in a DynamoDB table, at the rate of thousands of updates per second\. Another application captures and stores data about these updates, providing near\-real\-time usage metrics for the mobile app\.
+ A global multi\-player game has a multi\-master topology, storing data in multiple AWS Regions\. Each master stays in sync by consuming and replaying the changes that occur in the remote Regions\.
+ An application automatically sends notifications to the mobile devices of all friends in a group as soon as one friend uploads a new picture\.
+ A new customer adds data to a DynamoDB table\. This event invokes another application that sends a welcome email to the new customer\.

DynamoDB Streams enables solutions such as these, and many others\. DynamoDB Streams captures a time\-ordered sequence of item\-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours\. Applications can access this log and view the data items as they appeared before and after they were modified, in near\-real time\.

 Encryption at rest encrypts the data in DynamoDB streams\. 

A *DynamoDB stream* is an ordered flow of information about changes to items in a DynamoDB table\. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table\.

Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified\. A *stream record* contains information about a data modification to a single item in a DynamoDB table\. You can configure the stream so that the stream records capture additional information, such as the "before" and "after" images of modified items\.

DynamoDB Streams helps ensure the following:
+ Each stream record appears exactly once in the stream\.
+ For each item that is modified in a DynamoDB table, the stream records appear in the same sequence as the actual modifications to the item\.

DynamoDB Streams writes stream records in near\-real time so that you can build applications that consume these streams and take action based on the contents\.


#### Endpoints for DynamoDB Streams<a name="Streams.Endpoints"> <!-- omit in toc -->

AWS maintains separate endpoints for DynamoDB and DynamoDB Streams\. To work with database tables and indexes, your application must access a DynamoDB endpoint\. To read and process DynamoDB Streams records, your application must access a DynamoDB Streams endpoint in the same Region\.

The naming convention for DynamoDB Streams endpoints is `streams.dynamodb.<region>.amazonaws.com`\. For example, if you use the endpoint `dynamodb.us-west-2.amazonaws.com` to access DynamoDB, you would use the endpoint `streams.dynamodb.us-west-2.amazonaws.com` to access DynamoDB Streams\.

**Note**  
For a complete list of DynamoDB and DynamoDB Streams Regions and endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html) in the *AWS General Reference*\.

The AWS SDKs provide separate clients for DynamoDB and DynamoDB Streams\. Depending on your requirements, your application can access a DynamoDB endpoint, a DynamoDB Streams endpoint, or both at the same time\. To connect to both endpoints, your application must instantiate two clients—one for DynamoDB and one for DynamoDB Streams\.


#### Enabling a Stream<a name="Streams.Enabling"> <!-- omit in toc -->

You can enable a stream on a new table when you create it\. You can also enable or disable a stream on an existing table, or change the settings of a stream\. DynamoDB Streams operates asynchronously, so there is no performance impact on a table if you enable a stream\.

The easiest way to manage DynamoDB Streams is by using the AWS Management Console\.

1. Sign in to the AWS Management Console and open the DynamoDB console at [https://console\.aws\.amazon\.com/dynamodb/](https://console.aws.amazon.com/dynamodb/)\.

1. On the DynamoDB console dashboard, choose **Tables**\.

1. On the **Overview** tab, choose **Manage Stream**\.

1. In the **Manage Stream** window, choose the information that will be written to the stream whenever the data in the table is modified:
   + **Keys only** — Only the key attributes of the modified item\.
   + **New image** — The entire item, as it appears after it was modified\.
   + **Old image** — The entire item, as it appeared before it was modified\.
   + **New and old images** — Both the new and the old images of the item\.

   When the settings are as you want them, choose **Enable**\.

1. \(Optional\) To disable an existing stream, choose **Manage Stream** and then choose **Disable**\.

You can also use the `CreateTable` or `UpdateTable` API operations to enable or modify a stream\. The `StreamSpecification` parameter determines how the stream is configured:
+ `StreamEnabled` — Specifies whether a stream is enabled \(`true`\) or disabled \(`false`\) for the table\.
+ `StreamViewType` — Specifies the information that will be written to the stream whenever data in the table is modified:
  + `KEYS_ONLY` — Only the key attributes of the modified item\.
  + `NEW_IMAGE` — The entire item, as it appears after it was modified\.
  + `OLD_IMAGE` — The entire item, as it appeared before it was modified\.
  + `NEW_AND_OLD_IMAGES` — Both the new and the old images of the item\.

You can enable or disable a stream at any time\. However, you receive a `ResourceInUseException` if you try to enable a stream on a table that already has a stream\. You receive a `ValidationException` if you try to disable a stream on a table that doesn't have a stream\.

When you set `StreamEnabled` to `true`, DynamoDB creates a new stream with a unique stream descriptor assigned to it\. If you disable and then re\-enable a stream on the table, a new stream is created with a different stream descriptor\.

Every stream is uniquely identified by an Amazon Resource Name \(ARN\)\. The following is an example ARN for a stream on a DynamoDB table named `TestTable`\.

```
arn:aws:dynamodb:us-west-2:111122223333:table/TestTable/stream/2015-05-11T21:21:33.291
```

To determine the latest stream descriptor for a table, issue a DynamoDB `DescribeTable` request and look for the `LatestStreamArn` element in the response\.

#### Reading and Processing a Stream<a name="Streams.Processing"> <!-- omit in toc -->

To read and process a stream, your application must connect to a DynamoDB Streams endpoint and issue API requests\.

A stream consists of *stream records*\. Each stream record represents a single data modification in the DynamoDB table to which the stream belongs\. Each stream record is assigned a sequence number, reflecting the order in which the record was published to the stream\.

Stream records are organized into groups, or *shards*\. Each shard acts as a container for multiple stream records, and contains information required for accessing and iterating through these records\. The stream records within a shard are removed automatically after 24 hours\.

Shards are ephemeral: They are created and deleted automatically, as needed\. Any shard can also split into multiple new shards; this also occurs automatically\. \(It's also possible for a parent shard to have just one child shard\.\) A shard might split in response to high levels of write activity on its parent table, so that applications can process records from multiple shards in parallel\.

If you disable a stream, any shards that are open will be closed\.

Because shards have a lineage \(parent and children\), an application must always process a parent shard before it processes a child shard\. This helps ensure that the stream records are also processed in the correct order\. \(If you use the DynamoDB Streams Kinesis Adapter, this is handled for you\. Your application processes the shards and stream records in the correct order\. It automatically handles new or expired shards, in addition to shards that split while the application is running\. For more information, see [Using the DynamoDB Streams Kinesis Adapter to Process Stream Records](Streams.KCLAdapter.md)\.\)


**Note**  
If you perform a `PutItem` or `UpdateItem` operation that does not change any data in an item, DynamoDB Streams does *not* write a stream record for that operation\.

To access a stream and process the stream records within, you must do the following:
+ Determine the unique ARN of the stream that you want to access\.
+ Determine which shards in the stream contain the stream records that you are interested in\.
+ Access the shards and retrieve the stream records that you want\.

**Note**  
No more than two processes at most should be reading from the same streams shard at the same time\. Having more than two readers per shard can result in throttling\.

The DynamoDB Streams API provides the following actions for use by application programs:
+  `[ListStreams](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_streams_ListStreams.html)` — Returns a list of stream descriptors for the current account and endpoint\. You can optionally request just the stream descriptors for a particular table name\.
+ `[DescribeStream](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_streams_DescribeStream.html)` — Returns detailed information about a given stream\. The output includes a list of shards associated with the stream, including the shard IDs\.
+ `[GetShardIterator](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_streams_GetShardIterator.html)` — Returns a *shard iterator*, which describes a location within a shard\. You can request that the iterator provide access to the oldest point, the newest point, or a particular point in the stream\.
+ `[GetRecords](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_streams_GetRecords.html)` — Returns the stream records from within a given shard\. You must provide the shard iterator returned from a `GetShardIterator` request\.

For complete descriptions of these API operations, including example requests and responses, see the [Amazon DynamoDB Streams API Reference](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Operations_Amazon_DynamoDB_Streams.html)\.

#### Data Retention Limit for DynamoDB Streams<a name="Streams.DataRetention"> <!-- omit in toc -->

All data in DynamoDB Streams is subject to a 24\-hour lifetime\. You can retrieve and analyze the last 24 hours of activity for any given table\. However, data that is older than 24 hours is susceptible to trimming \(removal\) at any moment\.

If you disable a stream on a table, the data in the stream continues to be readable for 24 hours\. After this time, the data expires and the stream records are automatically deleted\. There is no mechanism for manually deleting an existing stream\. You must wait until the retention limit expires \(24 hours\), and all the stream records will be deleted\.

#### Tutorial: Using AWS Lambda with Amazon DynamoDB streams<a name="with-ddb-example"> <!-- omit in toc -->

 In this tutorial, you create a Lambda function to consume events from an Amazon DynamoDB stream\.

#### Prerequisites<a name="with-ddb-prepare"> <!-- omit in toc -->

This tutorial assumes that you have some knowledge of basic Lambda operations and the Lambda console\. If you haven't already, follow the instructions in [Getting started with AWS Lambda](getting-started.md) to create your first Lambda function\.

To follow the procedures in this guide, you will need a command line terminal or shell to run commands\. Commands are shown in listings preceded by a prompt symbol \($\) and the name of the current directory, when appropriate:

```
~/lambda-project$ this is a command
this is output
```

For long commands, an escape character \(`\`\) is used to split a command over multiple lines\.

On Linux and macOS, use your preferred shell and package manager\. On Windows 10, you can [install the Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/install-win10) to get a Windows\-integrated version of Ubuntu and Bash\.

#### Create the execution role<a name="with-ddb-create-execution-role"> <!-- omit in toc -->

Create the [execution role](lambda-intro-execution-role.md) that gives your function permission to access AWS resources\.

**To create an execution role**

1. Open the [roles page](https://console.aws.amazon.com/iam/home#/roles) in the IAM console\.

1. Choose **Create role**\.

1. Create a role with the following properties\.
   + **Trusted entity** – Lambda\.
   + **Permissions** – **AWSLambdaDynamoDBExecutionRole**\.
   + **Role name** – **lambda\-dynamodb\-role**\.

The **AWSLambdaDynamoDBExecutionRole** has the permissions that the function needs to read items from DynamoDB and write logs to CloudWatch Logs\.

#### Create the function<a name="with-ddb-example-create-function"> <!-- omit in toc -->

The following example code receives a DynamoDB event input and processes the messages that it contains\. For illustration, the code writes some of the incoming event data to CloudWatch Logs\.

**Note**  
For sample code in other languages, see [Sample function code](with-ddb-create-package.md)\.

**Example index\.js**  

```
console.log('Loading function');

exports.handler = function(event, context, callback) {
    console.log(JSON.stringify(event, null, 2));
    event.Records.forEach(function(record) {
        console.log(record.eventID);
        console.log(record.eventName);
        console.log('DynamoDB Record: %j', record.dynamodb);
    });
    callback(null, "message");
};
```

**To create the function**

1. Copy the sample code into a file named `index.js`\.

1. Create a deployment package\.

   ```
   $ zip function.zip index.js
   ```

1. Create a Lambda function with the `create-function` command\.

   ```
   $ aws lambda create-function --function-name ProcessDynamoDBRecords \
   --zip-file fileb://function.zip --handler index.handler --runtime nodejs12.x \
   --role arn:aws:iam::123456789012:role/lambda-dynamodb-role
   ```

#### Test the Lambda function<a name="with-dbb-invoke-manually"> <!-- omit in toc -->

In this step, you invoke your Lambda function manually using the `invoke` AWS Lambda CLI command and the following sample DynamoDB event\.

**Example input\.txt**  

```
{
   "Records":[
      {
         "eventID":"1",
         "eventName":"INSERT",
         "eventVersion":"1.0",
         "eventSource":"aws:dynamodb",
         "awsRegion":"us-east-1",
         "dynamodb":{
            "Keys":{
               "Id":{
                  "N":"101"
               }
            },
            "NewImage":{
               "Message":{
                  "S":"New item!"
               },
               "Id":{
                  "N":"101"
               }
            },
            "SequenceNumber":"111",
            "SizeBytes":26,
            "StreamViewType":"NEW_AND_OLD_IMAGES"
         },
         "eventSourceARN":"stream-ARN"
      },
      {
         "eventID":"2",
         "eventName":"MODIFY",
         "eventVersion":"1.0",
         "eventSource":"aws:dynamodb",
         "awsRegion":"us-east-1",
         "dynamodb":{
            "Keys":{
               "Id":{
                  "N":"101"
               }
            },
            "NewImage":{
               "Message":{
                  "S":"This item has changed"
               },
               "Id":{
                  "N":"101"
               }
            },
            "OldImage":{
               "Message":{
                  "S":"New item!"
               },
               "Id":{
                  "N":"101"
               }
            },
            "SequenceNumber":"222",
            "SizeBytes":59,
            "StreamViewType":"NEW_AND_OLD_IMAGES"
         },
         "eventSourceARN":"stream-ARN"
      },
      {
         "eventID":"3",
         "eventName":"REMOVE",
         "eventVersion":"1.0",
         "eventSource":"aws:dynamodb",
         "awsRegion":"us-east-1",
         "dynamodb":{
            "Keys":{
               "Id":{
                  "N":"101"
               }
            },
            "OldImage":{
               "Message":{
                  "S":"This item has changed"
               },
               "Id":{
                  "N":"101"
               }
            },
            "SequenceNumber":"333",
            "SizeBytes":38,
            "StreamViewType":"NEW_AND_OLD_IMAGES"
         },
         "eventSourceARN":"stream-ARN"
      }
   ]
}
```

Execute the following `invoke` command\. 

```
$ aws lambda invoke --function-name ProcessDynamoDBRecords --payload file://input.txt outputfile.txt
```

The function returns the string `message` in the response body\. 

Verify the output in the `outputfile.txt` file\.

#### Create a DynamoDB table with a stream enabled<a name="with-ddb-create-buckets"> <!-- omit in toc -->

Create an Amazon DynamoDB table with a stream enabled\.

**To create a DynamoDB table**

1. Open the [DynamoDB console](https://console.aws.amazon.com/dynamodb)\.

1. Choose **Create table**\.

1. Create a table with the following settings\.
   + **Table name** – **lambda\-dynamodb\-stream**
   + **Primary key** – **id** \(string\)

1. Choose **Create**\.

**To enable streams**

1. Open the [DynamoDB console](https://console.aws.amazon.com/dynamodb)\.

1. Choose **Tables**\.

1. Choose the **lambda\-dynamodb\-stream** table\.

1. Under **Overview**, choose **Manage stream**\.

1. Choose **Enable**\.

Write down the stream ARN\. You need this in the next step when you associate the stream with your Lambda function\. For more information on enabling streams, see [Capturing table activity with DynamoDB Streams](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html)\.

#### Add an event source in AWS Lambda<a name="with-ddb-attach-notification-configuration"> <!-- omit in toc -->

Create an event source mapping in AWS Lambda\. This event source mapping associates the DynamoDB stream with your Lambda function\. After you create this event source mapping, AWS Lambda starts polling the stream\.

Run the following AWS CLI `create-event-source-mapping` command\. After the command executes, note down the UUID\. You'll need this UUID to refer to the event source mapping in any commands, for example, when deleting the event source mapping\.

```
$ aws lambda create-event-source-mapping --function-name ProcessDynamoDBRecords \
 --batch-size 100 --starting-position LATEST --event-source DynamoDB-stream-arn
```

 This creates a mapping between the specified DynamoDB stream and the Lambda function\. You can associate a DynamoDB stream with multiple Lambda functions, and associate the same Lambda function with multiple streams\. However, the Lambda functions will share the read throughput for the stream they share\. 

You can get the list of event source mappings by running the following command\.

```
$ aws lambda list-event-source-mappings
```

The list returns all of the event source mappings you created, and for each mapping it shows the `LastProcessingResult`, among other things\. This field is used to provide an informative message if there are any problems\. Values such as `No records processed` \(indicates that AWS Lambda has not started polling or that there are no records in the stream\) and `OK` \(indicates AWS Lambda successfully read records from the stream and invoked your Lambda function\) indicate that there are no issues\. If there are issues, you receive an error message\.

If you have a lot of event source mappings, use the function name parameter to narrow down the results\.

```
$ aws lambda list-event-source-mappings --function-name ProcessDynamoDBRecords
```

#### Test the setup<a name="with-ddb-final-integration-test-no-iam"> <!-- omit in toc -->

Test the end\-to\-end experience\. As you perform table updates, DynamoDB writes event records to the stream\. As AWS Lambda polls the stream, it detects new records in the stream and executes your Lambda function on your behalf by passing events to the function\. 

1. In the DynamoDB console, add, update, and delete items to the table\. DynamoDB writes records of these actions to the stream\.

1. AWS Lambda polls the stream and when it detects updates to the stream, it invokes your Lambda function by passing in the event data it finds in the stream\.

1. Your function executes and creates logs in Amazon CloudWatch\. You can verify the logs reported in the Amazon CloudWatch console\.

#### DynamoDB Streams Low\-Level API: Java Example<a name="Streams.LowLevel.Walkthrough"> <!-- omit in toc -->

**Note**  
The code on this page is not exhaustive and does not handle all scenarios for consuming Amazon DynamoDB Streams\. The recommended way to consume stream records from DynamoDB is through the Amazon Kinesis Adapter using the Kinesis Client Library \(KCL\), as described in [Using the DynamoDB Streams Kinesis Adapter to Process Stream Records](Streams.KCLAdapter.md)\.

This section contains a Java program that shows DynamoDB Streams in action\. The program does the following:

1. Creates a DynamoDB table with a stream enabled\.

1. Describes the stream settings for this table\.

1. Modifies data in the table\.

1. Describes the shards in the stream\.

1. Reads the stream records from the shards\.

1. Cleans up\.

When you run the program, you will see output similar to the following\.

```
Issuing CreateTable request for TestTableForStreams
Waiting for TestTableForStreams to be created...
Current stream ARN for TestTableForStreams: arn:aws:dynamodb:us-east-2:123456789012:table/TestTableForStreams/stream/2018-03-20T16:49:55.208
Stream enabled: true
Update view type: NEW_AND_OLD_IMAGES

Performing write activities on TestTableForStreams
Processing item 1 of 100
Processing item 2 of 100
Processing item 3 of 100
...
Processing item 100 of 100

Shard: {ShardId: shardId-1234567890-...,SequenceNumberRange: {StartingSequenceNumber: 01234567890...,},}
    Shard iterator: EjYFEkX2a26eVTWe...
        ApproximateCreationDateTime: Tue Mar 20 09:50:00 PDT 2018,Keys: {Id={N: 1,}},NewImage: {Message={S: New item!,}, Id={N: 1,}},SequenceNumber: 100000000003218256368,SizeBytes: 24,StreamViewType: NEW_AND_OLD_IMAGES}
         {ApproximateCreationDateTime: Tue Mar 20 09:50:00 PDT 2018,Keys: {Id={N: 1,}},NewImage: {Message={S: This item has changed,}, Id={N: 1,}},OldImage: {Message={S: New item!,}, Id={N: 1,}},SequenceNumber: 200000000003218256412,SizeBytes: 56,StreamViewType: NEW_AND_OLD_IMAGES}
         {ApproximateCreationDateTime: Tue Mar 20 09:50:00 PDT 2018,Keys: {Id={N: 1,}},OldImage: {Message={S: This item has changed,}, Id={N: 1,}},SequenceNumber: 300000000003218256413,SizeBytes: 36,StreamViewType: NEW_AND_OLD_IMAGES}
...
Deleting the table...
Demo complete
```

**Example**  

```
/**
 * Copyright 2010-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * This file is licensed under the Apache License, Version 2.0 (the "License").
 * You may not use this file except in compliance with the License. A copy of
 * the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR
 * CONDITIONS OF ANY KIND, either express or implied. See the License for the
 * specific language governing permissions and limitations under the License.
*/


package com.amazon.codesamples;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import com.amazonaws.AmazonClientException;
import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;
import com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder;
import com.amazonaws.services.dynamodbv2.AmazonDynamoDBStreams;
import com.amazonaws.services.dynamodbv2.AmazonDynamoDBStreamsClientBuilder;
import com.amazonaws.services.dynamodbv2.model.AttributeAction;
import com.amazonaws.services.dynamodbv2.model.AttributeDefinition;
import com.amazonaws.services.dynamodbv2.model.AttributeValue;
import com.amazonaws.services.dynamodbv2.model.AttributeValueUpdate;
import com.amazonaws.services.dynamodbv2.model.CreateTableRequest;
import com.amazonaws.services.dynamodbv2.model.DescribeStreamRequest;
import com.amazonaws.services.dynamodbv2.model.DescribeStreamResult;
import com.amazonaws.services.dynamodbv2.model.DescribeTableResult;
import com.amazonaws.services.dynamodbv2.model.GetRecordsRequest;
import com.amazonaws.services.dynamodbv2.model.GetRecordsResult;
import com.amazonaws.services.dynamodbv2.model.GetShardIteratorRequest;
import com.amazonaws.services.dynamodbv2.model.GetShardIteratorResult;
import com.amazonaws.services.dynamodbv2.model.KeySchemaElement;
import com.amazonaws.services.dynamodbv2.model.KeyType;
import com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput;
import com.amazonaws.services.dynamodbv2.model.Record;
import com.amazonaws.services.dynamodbv2.model.Shard;
import com.amazonaws.services.dynamodbv2.model.ShardIteratorType;
import com.amazonaws.services.dynamodbv2.model.StreamSpecification;
import com.amazonaws.services.dynamodbv2.model.StreamViewType;
import com.amazonaws.services.dynamodbv2.util.TableUtils;

public class StreamsLowLevelDemo {

    public static void main(String args[]) throws InterruptedException {

        AmazonDynamoDB dynamoDBClient = AmazonDynamoDBClientBuilder
            .standard()
            .withRegion(Regions.US_EAST_2)
            .withCredentials(new DefaultAWSCredentialsProviderChain())
            .build();

        AmazonDynamoDBStreams streamsClient =
            AmazonDynamoDBStreamsClientBuilder
                .standard()
                .withRegion(Regions.US_EAST_2)
                .withCredentials(new DefaultAWSCredentialsProviderChain())
                .build();

        // Create a table, with a stream enabled
        String tableName = "TestTableForStreams";

        ArrayList<AttributeDefinition> attributeDefinitions = new ArrayList<>(
            Arrays.asList(new AttributeDefinition()
                .withAttributeName("Id")
                .withAttributeType("N")));

        ArrayList<KeySchemaElement> keySchema = new ArrayList<>(
            Arrays.asList(new KeySchemaElement()
                .withAttributeName("Id")
                .withKeyType(KeyType.HASH))); // Partition key

        StreamSpecification streamSpecification = new StreamSpecification()
            .withStreamEnabled(true)
            .withStreamViewType(StreamViewType.NEW_AND_OLD_IMAGES);

        CreateTableRequest createTableRequest = new CreateTableRequest().withTableName(tableName)
            .withKeySchema(keySchema).withAttributeDefinitions(attributeDefinitions)
            .withProvisionedThroughput(new ProvisionedThroughput()
                .withReadCapacityUnits(10L)
                .withWriteCapacityUnits(10L))
            .withStreamSpecification(streamSpecification);

        System.out.println("Issuing CreateTable request for " + tableName);
        dynamoDBClient.createTable(createTableRequest);
        System.out.println("Waiting for " + tableName + " to be created...");

        try {
            TableUtils.waitUntilActive(dynamoDBClient, tableName);
        } catch (AmazonClientException e) {
            e.printStackTrace();
        }

        // Print the stream settings for the table
        DescribeTableResult describeTableResult = dynamoDBClient.describeTable(tableName);
        String streamArn = describeTableResult.getTable().getLatestStreamArn();
        System.out.println("Current stream ARN for " + tableName + ": " +
            describeTableResult.getTable().getLatestStreamArn());
        StreamSpecification streamSpec = describeTableResult.getTable().getStreamSpecification();
        System.out.println("Stream enabled: " + streamSpec.getStreamEnabled());
        System.out.println("Update view type: " + streamSpec.getStreamViewType());
        System.out.println();

        // Generate write activity in the table

        System.out.println("Performing write activities on " + tableName);
        int maxItemCount = 100;
        for (Integer i = 1; i <= maxItemCount; i++) {
            System.out.println("Processing item " + i + " of " + maxItemCount);

            // Write a new item
            Map<String, AttributeValue> item = new HashMap<>();
            item.put("Id", new AttributeValue().withN(i.toString()));
            item.put("Message", new AttributeValue().withS("New item!"));
            dynamoDBClient.putItem(tableName, item);


            // Update the item
            Map<String, AttributeValue> key = new HashMap<>();
            key.put("Id", new AttributeValue().withN(i.toString()));
            Map<String, AttributeValueUpdate> attributeUpdates = new HashMap<>();
            attributeUpdates.put("Message", new AttributeValueUpdate()
                .withAction(AttributeAction.PUT)
                .withValue(new AttributeValue()
                    .withS("This item has changed")));
            dynamoDBClient.updateItem(tableName, key, attributeUpdates);

            // Delete the item
            dynamoDBClient.deleteItem(tableName, key);
        }

        // Get all the shard IDs from the stream.  Note that DescribeStream returns
        // the shard IDs one page at a time.
        String lastEvaluatedShardId = null;

        do {
            DescribeStreamResult describeStreamResult = streamsClient.describeStream(
                new DescribeStreamRequest()
                    .withStreamArn(streamArn)
                    .withExclusiveStartShardId(lastEvaluatedShardId));
            List<Shard> shards = describeStreamResult.getStreamDescription().getShards();

            // Process each shard on this page

            for (Shard shard : shards) {
                String shardId = shard.getShardId();
                System.out.println("Shard: " + shard);

                // Get an iterator for the current shard

                GetShardIteratorRequest getShardIteratorRequest = new GetShardIteratorRequest()
                    .withStreamArn(streamArn)
                    .withShardId(shardId)
                    .withShardIteratorType(ShardIteratorType.TRIM_HORIZON);
                GetShardIteratorResult getShardIteratorResult =
                    streamsClient.getShardIterator(getShardIteratorRequest);
                String currentShardIter = getShardIteratorResult.getShardIterator();

                // Shard iterator is not null until the Shard is sealed (marked as READ_ONLY).
                // To prevent running the loop until the Shard is sealed, which will be on average
                // 4 hours, we process only the items that were written into DynamoDB and then exit.
                int processedRecordCount = 0;
                while (currentShardIter != null && processedRecordCount < maxItemCount) {
                    System.out.println("    Shard iterator: " + currentShardIter.substring(380));

                    // Use the shard iterator to read the stream records

                    GetRecordsResult getRecordsResult = streamsClient.getRecords(new GetRecordsRequest()
                        .withShardIterator(currentShardIter));
                    List<Record> records = getRecordsResult.getRecords();
                    for (Record record : records) {
                        System.out.println("        " + record.getDynamodb());
                    }
                    processedRecordCount += records.size();
                    currentShardIter = getRecordsResult.getNextShardIterator();
                }
            }

            // If LastEvaluatedShardId is set, then there is
            // at least one more page of shard IDs to retrieve
            lastEvaluatedShardId = describeStreamResult.getStreamDescription().getLastEvaluatedShardId();

        } while (lastEvaluatedShardId != null);

        // Delete the table
        System.out.println("Deleting the table...");
        dynamoDBClient.deleteTable(tableName);

        System.out.println("Demo complete");

    }
}
```
#### Using the DynamoDB Streams Kinesis Adapter to Process Stream Records<a name="Streams.KCLAdapter"> <!-- omit in toc -->

Using the Amazon Kinesis Adapter is the recommended way to consume streams from Amazon DynamoDB\. The DynamoDB Streams API is intentionally similar to that of Kinesis Data Streams, a service for real\-time processing of streaming data at massive scale\. In both services, data streams are composed of shards, which are containers for stream records\. Both services' APIs contain `ListStreams`, `DescribeStream`, `GetShards`, and `GetShardIterator` operations\. \(Although these DynamoDB Streams actions are similar to their counterparts in Kinesis Data Streams, they are not 100 percent identical\.\)

You can write applications for Kinesis Data Streams using the Kinesis Client Library \(KCL\)\. The KCL simplifies coding by providing useful abstractions above the low\-level Kinesis Data Streams API\. For more information about the KCL, see the [Developing Consumers Using the Kinesis Client Library](https://docs.aws.amazon.com/kinesis/latest/dev/developing-consumers-with-kcl.html) in the *Amazon Kinesis Data Streams Developer Guide*\.

As a DynamoDB Streams user, you can use the design patterns found within the KCL to process DynamoDB Streams shards and stream records\. To do this, you use the DynamoDB Streams Kinesis Adapter\. The Kinesis Adapter implements the Kinesis Data Streams interface so that the KCL can be used for consuming and processing records from DynamoDB Streams\.

With the DynamoDB Streams Kinesis Adapter in place, you can begin developing against the KCL interface, with the API calls seamlessly directed at the DynamoDB Streams endpoint\.

When your application starts, it calls the KCL to instantiate a worker\. You must provide the worker with configuration information for the application, such as the stream descriptor and AWS credentials, and the name of a record processor class that you provide\. As it runs the code in the record processor, the worker performs the following tasks:
+ Connects to the stream\.
+ Enumerates the shards within the stream\.
+ Coordinates shard associations with other workers \(if any\)\.
+ Instantiates a record processor for every shard it manages\.
+ Pulls records from the stream\.
+ Pushes the records to the corresponding record processor\.
+ Checkpoints processed records\.
+ Balances shard\-worker associations when the worker instance count changes\.
+ Balances shard\-worker associations when shards are split\.

**Note**  
For a description of the KCL concepts listed here, see [Developing Consumers Using the Kinesis Client Library](https://docs.aws.amazon.com/kinesis/latest/dev/developing-consumers-with-kcl.html) in the *Amazon Kinesis Data Streams Developer Guide*\.

### 2. Log DynamoDB Operations with AWS CloudTrail
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
||DE.CM-7|Monitoring for unauthorized personnel, connections, devices, and software is performed|
|DE.CM-6|External service provider activity is monitored to detect potential cybersecurity events|
|DE.CM-3|Personnel activity is monitored to detect potential cybersecurity events|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|DE.AE-3|Event data are aggregated and correlated from multiple sources and sensors|
|DE.AE-2|Detected events are analyzed to understand attack targets and methods|

**Why?** 

**How?** 
#### Logging DynamoDB Operations by Using AWS CloudTrail<a name="logging-using-cloudtrail"> <!-- omit in toc -->

DynamoDB is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in DynamoDB\. CloudTrail captures all API calls for DynamoDB as events\. The calls captured include calls from the DynamoDB console and code calls to the DynamoDB API operations\. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for DynamoDB\. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in **Event history**\. Using the information collected by CloudTrail, you can determine the request that was made to DynamoDB, the IP address from which the request was made, who made the request, when it was made, and additional details\. 

To learn more about CloudTrail, including how to configure and enable it, see the [AWS CloudTrail User Guide](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/)\.

#### DynamoDB Information in CloudTrail<a name="service-name-info-in-cloudtrail"> <!-- omit in toc -->

CloudTrail is enabled on your AWS account when you create the account\. When supported event activity occurs in DynamoDB, that activity is recorded in a CloudTrail event along with other AWS service events in **Event history**\. You can view, search, and download recent events in your AWS account\. For more information, see [Viewing Events with CloudTrail Event History](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html)\. 

For an ongoing record of events in your AWS account, including events for DynamoDB, create a trail\. A *trail* enables CloudTrail to deliver log files to an Amazon S3 bucket\. By default, when you create a trail in the console, the trail applies to all AWS Regions\. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify\. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs\. For more information, see the following: 
+ [Overview for Creating a Trail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-and-update-a-trail.html)
+ [CloudTrail Supported Services and Integrations](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-aws-service-specific-topics.html#cloudtrail-aws-service-specific-topics-integrations)
+ [Configuring Amazon SNS Notifications for CloudTrail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/getting_notifications_top_level.html)
+ [Receiving CloudTrail Log Files from Multiple Regions](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html) and [Receiving CloudTrail Log Files from Multiple Accounts](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-receive-logs-from-multiple-accounts.html)

The following API actions are logged as events in CloudTrail files:

** Amazon DynamoDB **
+ [CreateBackup](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateBackup.html)
+ [CreateGlobalTable](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateGlobalTable.html)
+ [CreateTable](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateTable.html)
+ [DeleteBackup](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DeleteBackup.html)
+ [DeleteTable](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DeleteTable.html)
+ [DescribeBackup](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeBackup.html)
+ [DescribeContinuousBackups](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeContinuousBackups.html)
+ [DescribeGlobalTable](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeGlobalTable.html)
+ [DescribeLimits](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeLimits.html)
+ [DescribeTable](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html)
+ [DescribeTimeToLive](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTimeToLive.html)
+ [ListBackups](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_ListBackups.html)
+ [ListTables](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_ListTables.html)
+ [ListTagsOfResource](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_ListTagsOfResource.html)
+ [ListGlobalTables](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_ListGlobalTables.html)
+ [RestoreTableFromBackup](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_RestoreTableFromBackup.html)
+ [RestoreTableToPointInTime](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_RestoreTableToPointInTime.html)
+ [TagResource](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_TagResource.html)
+ [UntagResource](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UntagResource.html)
+ [UpdateGlobalTable](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UpdateGlobalTable.html)
+ [UpdateTable](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UpdateTable.html)
+ [UpdateTimeToLive](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_UpdateTimeToLive.html)
+ DescribeReservedCapacity
+ DescribeReservedCapacityOfferings
+ DescribeScalableTargets
+ RegisterScalableTarget
+ PurchaseReservedCapacityOfferings

** DynamoDB Streams **
+ [DescribeStream](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_streams_DescribeStream.html)
+ [ListStreams](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_streams_ListStreams.html)

** DynamoDB Accelerator \(DAX\) **
+ [CreateCluster](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_CreateCluster.html)
+ [CreateParameterGroup](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_CreateParameterGroup.html)
+ [CreateSubnetGroup](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_CreateSubnetGroup.html)
+ [DecreaseReplicationFactor](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_DecreaseReplicationFactor.html)
+ [DeleteCluster](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_DeleteCluster.html)
+ [DeleteParameterGroup](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_DeleteParameterGroup.html)
+ [DeleteSubnetGroup](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_DeleteSubnetGroup.html)
+ [DescribeClusters](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_DescribeClusters.html)
+ [DescribeDefaultParameters](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_DescribeDefaultParameters.html)
+ [DescribeEvents](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_DescribeEvents.html)
+ [DescribeParameterGroups](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_DescribeParameterGroups.html)
+ [DescribeParameters](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_DescribeParameters.html)
+ [DescribeSubnetGroups](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_DescribeSubnetGroups.html)
+ [IncreaseReplicationFactor](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_IncreaseReplicationFactor.html)
+ [ListTags](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_ListTags.html)
+ [RebootNode](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_RebootNode.html)
+ [TagResource](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_TagResource.html)
+ [UntagResource](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_UntagResource.html)
+ [UpdateCluster](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_UpdateCluster.html)
+ [UpdateParameterGroup](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_UpdateParameterGroup.html)
+ [UpdateSubnetGroup](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_dax_UpdateSubnetGroup.html)

Every event or log entry contains information about who generated the request\. The identity information helps you determine the following: 
+ Whether the request was made with root or AWS Identity and Access Management \(IAM\) user credentials\.
+ Whether the request was made with temporary security credentials for a role or federated user\.
+ Whether the request was made by another AWS service\.

For more information, see the [CloudTrail userIdentity Element](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-user-identity.html)\.

#### Understanding DynamoDB Log File Entries<a name="understanding-service-name-entries"> <!-- omit in toc -->

 A trail is a configuration that enables delivery of events as log files to an Amazon S3 bucket that you specify\. CloudTrail log files contain one or more log entries\. An event represents a single request from any source and includes information about the requested action, the date and time of the action, request parameters, and so on\. CloudTrail log files aren't an ordered stack trace of the public API calls, so they don't appear in any specific order\. 

The following example shows a CloudTrail log that demonstrates the `CreateTable`, `DescribeTable`, `UpdateTable`, `ListTables`, `DeleteTable`, and `CreateCluster` actions\.

```
{"Records": [
    {
        "eventVersion": "1.03",
        "userIdentity": {
            "type": "AssumedRole",
            "principalId": "AKIAIOSFODNN7EXAMPLE:bob",
            "arn": "arn:aws:sts::111122223333:assumed-role/users/bob",
            "accountId": "111122223333",
            "accessKeyId": "AKIAIOSFODNN7EXAMPLE",
            "sessionContext": {
                "attributes": {
                    "mfaAuthenticated": "false",
                    "creationDate": "2015-05-28T18:06:01Z"
                },
                "sessionIssuer": {
                    "type": "Role",
                    "principalId": "AKIAI44QH8DHBEXAMPLE",
                    "arn": "arn:aws:iam::444455556666:role/admin-role",
                    "accountId": "444455556666",
                    "userName": "bob"
                }
            }
        },
        "eventTime": "2015-05-01T07:24:55Z",
        "eventSource": "dynamodb.amazonaws.com",
        "eventName": "CreateTable",
        "awsRegion": "us-west-2",
        "sourceIPAddress": "192.0.2.0",
        "userAgent": "console.aws.amazon.com",
        "requestParameters": {
            "provisionedThroughput": {
                "writeCapacityUnits": 10,
                "readCapacityUnits": 10
            },
            "tableName": "Music",
            "keySchema": [
                {
                    "attributeName": "Artist",
                    "keyType": "HASH"
                },
                {
                    "attributeName": "SongTitle",
                    "keyType": "RANGE"
                }
            ],
            "attributeDefinitions": [
                {
                    "attributeType": "S",
                    "attributeName": "Artist"
                },
                {
                    "attributeType": "S",
                    "attributeName": "SongTitle"
                }
            ]
        },
        "responseElements": {"tableDescription": {
            "tableName": "Music",
            "attributeDefinitions": [
                {
                    "attributeType": "S",
                    "attributeName": "Artist"
                },
                {
                    "attributeType": "S",
                    "attributeName": "SongTitle"
                }
            ],
            "itemCount": 0,
            "provisionedThroughput": {
                "writeCapacityUnits": 10,
                "numberOfDecreasesToday": 0,
                "readCapacityUnits": 10
            },
            "creationDateTime": "May 1, 2015 7:24:55 AM",
            "keySchema": [
                {
                    "attributeName": "Artist",
                    "keyType": "HASH"
                },
                {
                    "attributeName": "SongTitle",
                    "keyType": "RANGE"
                }
            ],
            "tableStatus": "CREATING",
            "tableSizeBytes": 0
        }},
        "requestID": "KAVGJR1Q0I5VHF8FS8V809EV7FVV4KQNSO5AEMVJF66Q9ASUAAJG",
        "eventID": "a8b5f864-480b-43bf-bc22-9b6d77910a29",
        "eventType": "AwsApiCall",
        "apiVersion": "2012-08-10",
        "recipientAccountId": "111122223333"
    },

    {
        "eventVersion": "1.03",
        "userIdentity": {
            "type": "AssumedRole",
            "principalId": "AKIAIOSFODNN7EXAMPLE:bob",
            "arn": "arn:aws:sts::111122223333:assumed-role/users/bob",
            "accountId": "444455556666",
            "accessKeyId": "AKIAIOSFODNN7EXAMPLE",
            "sessionContext": {
                "attributes": {
                    "mfaAuthenticated": "false",
                    "creationDate": "2015-05-28T18:06:01Z"
                },
                "sessionIssuer": {
                    "type": "Role",
                    "principalId": "AKIAI44QH8DHBEXAMPLE",
                    "arn": "arn:aws:iam::444455556666:role/admin-role",
                    "accountId": "444455556666",
                    "userName": "bob"
                }
            }
        },
        "eventTime": "2015-05-04T02:43:11Z",
        "eventSource": "dynamodb.amazonaws.com",
        "eventName": "DescribeTable",
        "awsRegion": "us-west-2",
        "sourceIPAddress": "192.0.2.0",
        "userAgent": "console.aws.amazon.com",
        "requestParameters": {"tableName": "Music"},
        "responseElements": null,
        "requestID": "DISTSH6DQRLCC74L48Q51LRBHFVV4KQNSO5AEMVJF66Q9ASUAAJG",
        "eventID": "c07befa7-f402-4770-8c1b-1911601ed2af",
        "eventType": "AwsApiCall",
        "apiVersion": "2012-08-10",
        "recipientAccountId": "111122223333"
    },

    {
        "eventVersion": "1.03",
        "userIdentity": {
            "type": "AssumedRole",
            "principalId": "AKIAIOSFODNN7EXAMPLE:bob",
            "arn": "arn:aws:sts::111122223333:assumed-role/users/bob",
            "accountId": "111122223333",
            "accessKeyId": "AKIAIOSFODNN7EXAMPLE",
            "sessionContext": {
                "attributes": {
                    "mfaAuthenticated": "false",
                    "creationDate": "2015-05-28T18:06:01Z"
                },
                "sessionIssuer": {
                    "type": "Role",
                    "principalId": "AKIAI44QH8DHBEXAMPLE",
                    "arn": "arn:aws:iam::444455556666:role/admin-role",
                    "accountId": "444455556666",
                    "userName": "bob"
                }
            }
        },
        "eventTime": "2015-05-04T02:14:52Z",
        "eventSource": "dynamodb.amazonaws.com",
        "eventName": "UpdateTable",
        "awsRegion": "us-west-2",
        "sourceIPAddress": "192.0.2.0",
        "userAgent": "console.aws.amazon.com",
        "requestParameters": {"provisionedThroughput": {
            "writeCapacityUnits": 25,
            "readCapacityUnits": 25
        }},
        "responseElements": {"tableDescription": {
            "tableName": "Music",
            "attributeDefinitions": [
                {
                    "attributeType": "S",
                    "attributeName": "Artist"
                },
                {
                    "attributeType": "S",
                    "attributeName": "SongTitle"
                }
            ],
            "itemCount": 0,
            "provisionedThroughput": {
                "writeCapacityUnits": 10,
                "numberOfDecreasesToday": 0,
                "readCapacityUnits": 10,
                "lastIncreaseDateTime": "May 3, 2015 11:34:14 PM"
            },
            "creationDateTime": "May 3, 2015 11:34:14 PM",
            "keySchema": [
                {
                    "attributeName": "Artist",
                    "keyType": "HASH"
                },
                {
                    "attributeName": "SongTitle",
                    "keyType": "RANGE"
                }
            ],
            "tableStatus": "UPDATING",
            "tableSizeBytes": 0
        }},
        "requestID": "AALNP0J2L244N5O15PKISJ1KUFVV4KQNSO5AEMVJF66Q9ASUAAJG",
        "eventID": "eb834e01-f168-435f-92c0-c36278378b6e",
        "eventType": "AwsApiCall",
        "apiVersion": "2012-08-10",
        "recipientAccountId": "111122223333"
    },

    {
        "eventVersion": "1.03",
        "userIdentity": {
            "type": "AssumedRole",
            "principalId": "AKIAIOSFODNN7EXAMPLE:bob",
            "arn": "arn:aws:sts::111122223333:assumed-role/users/bob",
            "accountId": "111122223333",
            "accessKeyId": "AKIAIOSFODNN7EXAMPLE",
            "sessionContext": {
                "attributes": {
                    "mfaAuthenticated": "false",
                    "creationDate": "2015-05-28T18:06:01Z"
                },
                "sessionIssuer": {
                    "type": "Role",
                    "principalId": "AKIAI44QH8DHBEXAMPLE",
                    "arn": "arn:aws:iam::444455556666:role/admin-role",
                    "accountId": "444455556666",
                    "userName": "bob"
                }
            }
        },
        "eventTime": "2015-05-04T02:42:20Z",
        "eventSource": "dynamodb.amazonaws.com",
        "eventName": "ListTables",
        "awsRegion": "us-west-2",
        "sourceIPAddress": "192.0.2.0",
        "userAgent": "console.aws.amazon.com",
        "requestParameters": null,
        "responseElements": null,
        "requestID": "3BGHST5OVHLMTPUMAUTA1RF4M3VV4KQNSO5AEMVJF66Q9ASUAAJG",
        "eventID": "bd5bf4b0-b8a5-4bec-9edf-83605bd5e54e",
        "eventType": "AwsApiCall",
        "apiVersion": "2012-08-10",
        "recipientAccountId": "111122223333"
    },

    {
        "eventVersion": "1.03",
        "userIdentity": {
            "type": "AssumedRole",
            "principalId": "AKIAIOSFODNN7EXAMPLE:bob",
            "arn": "arn:aws:sts::111122223333:assumed-role/users/bob",
            "accountId": "111122223333",
            "accessKeyId": "AKIAIOSFODNN7EXAMPLE",
            "sessionContext": {
                "attributes": {
                    "mfaAuthenticated": "false",
                    "creationDate": "2015-05-28T18:06:01Z"
                },
                "sessionIssuer": {
                    "type": "Role",
                    "principalId": "AKIAI44QH8DHBEXAMPLE",
                    "arn": "arn:aws:iam::444455556666:role/admin-role",
                    "accountId": "444455556666",
                    "userName": "bob"
                }
            }
        },
        "eventTime": "2015-05-04T13:38:20Z",
        "eventSource": "dynamodb.amazonaws.com",
        "eventName": "DeleteTable",
        "awsRegion": "us-west-2",
        "sourceIPAddress": "192.0.2.0",
        "userAgent": "console.aws.amazon.com",
        "requestParameters": {"tableName": "Music"},
        "responseElements": {"tableDescription": {
            "tableName": "Music",
            "itemCount": 0,
            "provisionedThroughput": {
                "writeCapacityUnits": 25,
                "numberOfDecreasesToday": 0,
                "readCapacityUnits": 25
            },
            "tableStatus": "DELETING",
            "tableSizeBytes": 0
        }},
        "requestID": "4KBNVRGD25RG1KEO9UT4V3FQDJVV4KQNSO5AEMVJF66Q9ASUAAJG",
        "eventID": "a954451c-c2fc-4561-8aea-7a30ba1fdf52",
        "eventType": "AwsApiCall",
        "apiVersion": "2012-08-10",
        "recipientAccountId": "111122223333"
    },

    {
    "eventVersion": "1.05",
    "userIdentity": {
        "type": "IAMUser",
        "principalId": "AIDAIAVK7DT3VEXAMPLES",
        "arn": "arn:aws:iam::111122223333:user/bob",
        "accountId": "111122223333",
        "accessKeyId": "AKIAIOSFODNN7EXAMPLE",
        "userName": "bob"
    },
    "eventTime": "2019-12-17T23:17:34Z",
    "eventSource": "dax.amazonaws.com",
    "eventName": "CreateCluster",
    "awsRegion": "us-west-2",
    "sourceIPAddress": "205.251.233.48",
    "userAgent": "aws-cli/1.16.304 Python/3.6.9 Linux/4.9.184-0.1.ac.235.83.329.metal1.x86_64 botocore/1.13.40",
    "requestParameters": {
        "sSESpecification": {
            "enabled": true
        },
        "clusterName": "daxcluster",
        "nodeType": "dax.r4.large",
        "replicationFactor": 3,
        "iamRoleArn": "arn:aws:iam::111122223333:role/DAXServiceRoleForDynamoDBAccess"
    },
    "responseElements": {
        "cluster": {
            "securityGroups": [
                {
                    "securityGroupIdentifier": "sg-1af6e36e",
                    "status": "active"
                }
            ],
            "parameterGroup": {
                "nodeIdsToReboot": [],
                "parameterGroupName": "default.dax1.0",
                "parameterApplyStatus": "in-sync"
            },
            "clusterDiscoveryEndpoint": {
                "port": 8111
            },
            "clusterArn": "arn:aws:dax:us-west-2:111122223333:cache/daxcluster",
            "status": "creating",
            "subnetGroup": "default",
            "sSEDescription": {
                "status": "ENABLED",
                "kMSMasterKeyArn": "arn:aws:kms:us-west-2:111122223333:key/764898e4-adb1-46d6-a762-e2f4225b4fc4"
            },
            "iamRoleArn": "arn:aws:iam::111122223333:role/DAXServiceRoleForDynamoDBAccess",
            "clusterName": "daxcluster",
            "activeNodes": 0,
            "totalNodes": 3,
            "preferredMaintenanceWindow": "thu:13:00-thu:14:00",
            "nodeType": "dax.r4.large"
        }},
        "requestID": "585adc5f-ad05-4e27-8804-70ba1315f8fd",
        "eventID": "29158945-28da-4e32-88e1-56d1b90c1a0c",
        "eventType": "AwsApiCall",
        "recipientAccountId": "111122223333"
    }
]}
```

## Respond/Recover
### 1. Create CloudWatch Alarms to monitor DynamoDB
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.AE-2|Detected events are analyzed to understand attack targets and methods|
|DE.AE-3|Event data are aggregated and correlated from multiple sources and sensors|
|DE.AE-4|Impact of events is determined|
|DE.AE-5|Incident alert thresholds are established|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch|

**Why?** 
Understanding instance behavior and having the ability to be notified of anomalies to that behavior is key to securin your DynamoDB deployments

**How?** 
#### Creating CloudWatch Alarms to Monitor DynamoDB<a name="creating-alarms"> <!-- omit in toc -->

You can create a CloudWatch alarm that sends an Amazon SNS message when the alarm changes state\. An alarm watches a single metric over a time period you specify, and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods\. The action is a notification sent to an Amazon SNS topic or Auto Scaling policy\. Alarms invoke actions for sustained state changes only\. CloudWatch alarms do not invoke actions simply because they are in a particular state; the state must have changed and been maintained for a specified number of periods\.

#### How can I be notified before I consume my entire read capacity?<a name="notify-reach-capacity"> <!-- omit in toc -->

1. Create an Amazon SNS topic, `arn:aws:sns:us-east-1:123456789012:capacity-alarm`\.

   For more information, see [Set Up Amazon Simple Notification Service](http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/US_SetupSNS.html)\.

1. Create the alarm\. In this example, we assume a provisioned capacity of five read capacity units\.

   ```
   Prompt>aws cloudwatch put-metric-alarm \
       --alarm-name ReadCapacityUnitsLimitAlarm \
       --alarm-description "Alarm when read capacity reaches 80% of my provisioned read capacity" \
       --namespace AWS/DynamoDB \
       --metric-name ConsumedReadCapacityUnits \
       --dimensions Name=TableName,Value=myTable \
       --statistic Sum \
       --threshold 240 \
       --comparison-operator GreaterThanOrEqualToThreshold \
       --period 60 \                           
       --evaluation-periods 1 \
       --alarm-actions arn:aws:sns:us-east-1:123456789012:capacity-alarm
   ```

1. Test the alarm\.

   ```
   Prompt>aws cloudwatch set-alarm-state --alarm-name ReadCapacityUnitsLimitAlarm --state-reason "initializing" --state-value OK
   ```

   ```
    Prompt>aws cloudwatch set-alarm-state --alarm-name ReadCapacityUnitsLimitAlarm --state-reason "initializing" --state-value ALARM
   ```

**Note**  
The alarm is activated whenever the consumed read capacity is at least 4 units per second \(80% of provisioned read capacity of 5\) for 1 minute \(60 seconds\)\. So the `threshold` is 240 read capacity units \(4 units/sec \* 60 seconds\)\. Any time the read capacity is updated you should update the alarm calculations appropriately\. You can avoid this process by creating alarms through the DynamoDB Console\. In this way, the alarms are automatically updated for you\. 

#### How can I be notified if any requests exceed the provisioned throughput quotas of a table?<a name="notify-exceed-throughput"> <!-- omit in toc -->

1. Create an Amazon SNS topic, `arn:aws:sns:us-east-1:123456789012:requests-exceeding-throughput`\.

   For more information, see [Set Up Amazon Simple Notification Service](http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/US_SetupSNS.html)\.

1. Create the alarm\.

   ```
   Prompt>aws cloudwatch put-metric-alarm \
       --alarm-name RequestsExceedingThroughputAlarm\
       --alarm-description "Alarm when my requests are exceeding provisioned throughput quotas of a table" \
       --namespace AWS/DynamoDB \
       --metric-name ThrottledRequests \
       --dimensions Name=TableName,Value=myTable \
       --statistic Sum \
       --threshold 0 \
       --comparison-operator GreaterThanThreshold \
       --period 300 \
       --unit Count \
       --evaluation-periods 1 \
       --alarm-actions arn:aws:sns:us-east-1:123456789012:requests-exceeding-throughput
   ```

1. Test the alarm\.

   ```
   Prompt>aws cloudwatch set-alarm-state --alarm-name RequestsExceedingThroughputAlarm --state-reason "initializing" --state-value OK
   ```

   ```
   Prompt>aws cloudwatch set-alarm-state --alarm-name RequestsExceedingThroughputAlarm --state-reason "initializing" --state-value ALARM
   ```

#### How can I be notified if any system errors occurred?<a name="notify-system-errors"> <!-- omit in toc -->

1. Create an Amazon SNS topic, `arn:aws:sns:us-east-1:123456789012:notify-on-system-errors`\.

   For more information, see [Set Up Amazon Simple Notification Service](http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/US_SetupSNS.html)\.

1. Create the alarm\.

   ```
   Prompt>aws cloudwatch put-metric-alarm \
       --alarm-name SystemErrorsAlarm \
       --alarm-description "Alarm when system errors occur" \
       --namespace AWS/DynamoDB \
       --metric-name SystemErrors \
       --dimensions Name=TableName,Value=myTable \
       --statistic Sum \
       --threshold 0 \
       --comparison-operator GreaterThanThreshold \
       --period 60 \
       --unit Count \
       --evaluation-periods 1 \
       --alarm-actions arn:aws:sns:us-east-1:123456789012:notify-on-system-errors
   ```

1. Test the alarm\.

   ```
   Prompt>aws cloudwatch set-alarm-state --alarm-name SystemErrorsAlarm --state-reason "initializing" --state-value OK
   ```

   ```
   Prompt>aws cloudwatch set-alarm-state --alarm-name SystemErrorsAlarm --state-reason "initializing" --state-value ALARM
   ```
#### DynamoDB Metrics and Dimensions<a name="metrics-dimensions"> <!-- omit in toc -->

When you interact with DynamoDB, it sends the following metrics and dimensions to CloudWatch\. You can use the following procedures to view the metrics for DynamoDB\.

**To view metrics \(console\)**

Metrics are grouped first by the service namespace, and then by the various dimension combinations within each namespace\.

1. Open the CloudWatch console at [https://console\.aws\.amazon\.com/cloudwatch/](https://console.aws.amazon.com/cloudwatch/)\.

1. In the navigation pane, choose **Metrics**\.

1. Select the **DynamoDB** namespace\.

**To view metrics \(CLI\)**
+ At a command prompt, use the following command:

  ```
  1. aws cloudwatch list-metrics --namespace "AWS/DynamoDB"
  ```

CloudWatch displays the following metrics for DynamoDB:

#### DynamoDB Dimensions and Metrics<a name="dynamodb-metrics-dimensions"> <!-- omit in toc -->

The metrics and dimensions that DynamoDB sends to Amazon CloudWatch are listed here\.

#### DynamoDB Metrics<a name="dynamodb-metrics"> <!-- omit in toc -->

**Note**  
Amazon CloudWatch aggregates the following DynamoDB metrics at one\-minute intervals:  
`ConditionalCheckFailedRequests`
`ConsumedReadCapacityUnits`
`ConsumedWriteCapacityUnits`
`ReadThrottleEvents`
`ReturnedBytes`
`ReturnedItemCount`
`ReturnedRecordsCount`
`SuccessfulRequestLatency`
`SystemErrors`
`TimeToLiveDeletedItemCount`
`ThrottledRequests`
`TransactionConflict`
`UserErrors`
`WriteThrottleEvents`
For all other DynamoDB metrics, the aggregation granularity is five minutes\.

Not all statistics, such as *Average* or *Sum*, are applicable for every metric\. However, all of these values are available through the Amazon DynamoDB console, or by using the CloudWatch console, AWS CLI, or AWS SDKs for all metrics\. In the following table, each metric has a list of valid statistics that are applicable to that metric\.


| Metric | Description | 
| --- | --- | 
| AccountMaxReads |  The maximum number of read capacity units that can be used by an account\. This limit does not apply to on\-demand tables or global secondary indexes\. Units: `Count` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| AccountMaxTableLevelReads |  The maximum number of read capacity units that can be used by a table or global secondary index of an account\. For on\-demand tables this limit caps the maximum read request units a table or a global secondary index can use\. Units: `Count` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| AccountMaxTableLevelWrites |  The maximum number of write capacity units that can be used by a table or global secondary index of an account\. For on\-demand tables this limit caps the maximum write request units a table or a global secondary index can use\. Units: `Count` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| AccountMaxWrites |  The maximum number of write capacity units that can be used by an account\. This limit does not apply to on\-demand tables or global secondary indexes\. Units: `Count` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| AccountProvisionedReadCapacityUtilization |  The percentage of provisioned read capacity units utilized by an account\. Units: `Percent` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| AccountProvisionedWriteCapacityUtilization |  The percentage of provisioned write capacity units utilized by an account\. Units: `Percent` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ConditionalCheckFailedRequests |  The number of failed attempts to perform conditional writes\. The `PutItem`, `UpdateItem`, and `DeleteItem` operations let you provide a logical condition that must evaluate to true before the operation can proceed\. If this condition evaluates to false, `ConditionalCheckFailedRequests` is incremented by one\.  A failed conditional write will result in an HTTP 400 error \(Bad Request\)\. These events are reflected in the `ConditionalCheckFailedRequests` metric, but not in the `UserErrors` metric\.  Units: `Count` Dimensions: `TableName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ConsumedReadCapacityUnits |  The number of read capacity units consumed over the specified time period, so you can track how much of your provisioned throughput is used\. You can retrieve the total consumed read capacity for a table and all of its global secondary indexes, or for a particular global secondary index\. For more information, see [Read/Write Capacity Mode](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughputIntro.html)\.  Use the `Sum` statistic to calculate the consumed throughput\. For example, get the `Sum` value over a span of one minute, and divide it by the number of seconds in a minute \(60\) to calculate the average `ConsumedReadCapacityUnits` per second \(recognizing that this average does not highlight any large but brief spikes in read activity that occurred during that minute\)\. You can compare the calculated value to the provisioned throughput value that you provide DynamoDB\.  Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ConsumedWriteCapacityUnits |  The number of write capacity units consumed over the specified time period, so you can track how much of your provisioned throughput is used\. You can retrieve the total consumed write capacity for a table and all of its global secondary indexes, or for a particular global secondary index\. For more information, see [Read/Write Capacity Mode](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughputIntro.html)\.  Use the `Sum` statistic to calculate the consumed throughput\. For example, get the `Sum` value over a span of one minute, and divide it by the number of seconds in a minute \(60\) to calculate the average `ConsumedWriteCapacityUnits` per second \(recognizing that this average does not highlight any large but brief spikes in write activity that occurred during that minute\)\. You can compare the calculated value to the provisioned throughput value that you provide DynamoDB\.  Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics:  [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| MaxProvisionedTableReadCapacityUtilization |  The percentage of provisioned read capacity units utilized by the highest provisioned read table or global secondary index of an account\. Units: `Percent` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| MaxProvisionedTableWriteCapacityUtilization |  The percentage of provisioned write capacity utilized by the highest provisioned write table or global secondary index of an account\. Units: `Percent` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| OnlineIndexConsumedWriteCapacity |  The number of write capacity units consumed when adding a new global secondary index to a table\. If the write capacity of the index is too low, incoming write activity during the backfill phase might be throttled\. This can increase the time it takes to create the index\. You should monitor this statistic while the index is being built to determine whether the write capacity of the index is underprovisioned\. You can adjust the write capacity of the index using the `UpdateTable` operation, even while the index is still being built\. The `ConsumedWriteCapacityUnits` metric for the index does not include the write throughput consumed during index creation\.  Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| OnlineIndexPercentageProgress |  The percentage of completion when a new global secondary index is being added to a table\. DynamoDB must first allocate resources for the new index, and then backfill attributes from the table into the index\. For large tables, this process might take a long time\. You should monitor this statistic to view the relative progress as DynamoDB builds the index\. Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| OnlineIndexThrottleEvents |  The number of write throttle events that occur when adding a new global secondary index to a table\. These events indicate that the index creation will take longer to complete, because incoming write activity is exceeding the provisioned write throughput of the index\. You can adjust the write capacity of the index using the `UpdateTable` operation, even while the index is still being built\. The `WriteThrottleEvents` metric for the index does not include any throttle events that occur during index creation\.  Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| PendingReplicationCount |  \(This metric is for DynamoDB global tables\.\) The number of item updates that are written to one replica table, but that have not yet been written to another replica in the global table\. Units: `Count`  Dimensions: `TableName, ReceivingRegion` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ProvisionedReadCapacityUnits | The number of provisioned read capacity units for a table or a global secondary index\.The TableName dimension returns the ProvisionedReadCapacityUnits for the table, but not for any global secondary indexes\. To view ProvisionedReadCapacityUnits for a global secondary index, you must specify both TableName and GlobalSecondaryIndex\.Units: `Count`Dimensions: `TableName, GlobalSecondaryIndexName`Valid Statistics:[\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html) | 
| ProvisionedWriteCapacityUnits |  The number of provisioned write capacity units for a table or a global secondary index\. The `TableName` dimension returns the `ProvisionedWriteCapacityUnits` for the table, but not for any global secondary indexes\. To view `ProvisionedWriteCapacityUnits` for a global secondary index, you must specify both `TableName` and `GlobalSecondaryIndex`\. Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ReadThrottleEvents |  Requests to DynamoDB that exceed the provisioned read capacity units for a table or a global secondary index\. A single request can result in multiple events\. For example, a `BatchGetItem` that reads 10 items is processed as 10 `GetItem` events\. For each event, `ReadThrottleEvents` is incremented by one if that event is throttled\. The `ThrottledRequests` metric for the entire `BatchGetItem` is not incremented unless *all 10* of the `GetItem` events are throttled\. The `TableName` dimension returns the `ReadThrottleEvents` for the table, but not for any global secondary indexes\. To view `ReadThrottleEvents` for a global secondary index, you must specify both `TableName` and `GlobalSecondaryIndex`\.  Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ReplicationLatency |  \(This metric is for DynamoDB global tables\.\) The elapsed time between an updated item appearing in the DynamoDB stream for one replica table, and that item appearing in another replica in the global table\.  Units: `Milliseconds`  Dimensions: `TableName, ReceivingRegion` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ReturnedBytes |  The number of bytes returned by `GetRecords` operations \(Amazon DynamoDB Streams\) during the specified time period\. Units: `Bytes` Dimensions: `Operation, StreamLabel, TableName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ReturnedItemCount |  The number of items returned by `Query` or `Scan` operations during the specified time period\. The number of items *returned* is not necessarily the same as the number of items that were evaluated\. For example, suppose that you requested a `Scan` on a table that had 100 items, but specified a `FilterExpression` that narrowed the results so that only 15 items were returned\. In this case, the response from `Scan` would contain a `ScanCount` of 100 and a `Count` of 15 returned items\. Units: `Count` Dimensions: `TableName, Operation` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ReturnedRecordsCount |  The number of stream records returned by `GetRecords` operations \(Amazon DynamoDB Streams\) during the specified time period\. Units: `Count` Dimensions: `Operation, StreamLabel, TableName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| SuccessfulRequestLatency |  The successful requests to DynamoDB or Amazon DynamoDB Streams during the specified time period\. `SuccessfulRequestLatency` can provide two different kinds of information: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html) `SuccessfulRequestLatency` reflects activity only within DynamoDB or Amazon DynamoDB Streams, and does not take into account network latency or client\-side activity\.  Units: `Milliseconds`  Dimensions: `TableName, Operation` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| SystemErrors |  The requests to DynamoDB or Amazon DynamoDB Streams that generate an HTTP 500 status code during the specified time period\. An HTTP 500 usually indicates an internal service error\. Units: `Count` Dimensions: `TableName, Operation` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| TimeToLiveDeletedItemCount |  The number of items deleted by Time to Live \(TTL\) during the specified time period\. This metric helps you monitor the rate of TTL deletions on your table\.  Units: `Count` Dimensions: TableName Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ThrottledRequests |  Requests to DynamoDB that exceed the provisioned throughput limits on a resource \(such as a table or an index\)\. `ThrottledRequests` is incremented by one if any event within a request exceeds a provisioned throughput limit\. For example, if you update an item in a table with global secondary indexes, there are multiple events—a write to the table, and a write to each index\. If one or more of these events are throttled, then `ThrottledRequests` is incremented by one\.  In a batch request \(`BatchGetItem` or `BatchWriteItem`\), `ThrottledRequests` is incremented only if *every* request in the batch is throttled\. If any individual request within the batch is throttled, one of the following metrics is incremented:   `ReadThrottleEvents` – For a throttled `GetItem` event within `BatchGetItem`\.   `WriteThrottleEvents` – For a throttled `PutItem` or `DeleteItem` event within `BatchWriteItem`\.    To gain insight into which event is throttling a request, compare `ThrottledRequests` with the `ReadThrottleEvents` and `WriteThrottleEvents` for the table and its indexes\.  A throttled request will result in an HTTP 400 status code\. All such events are reflected in the `ThrottledRequests` metric, but not in the `UserErrors` metric\.  Units: `Count` Dimensions: `TableName, Operation` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| TransactionConflict |  Rejected item\-level requests due to transactional conflicts between concurrent requests on the same items\. For more information, see [Transaction Conflict Handling in DynamoDB](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html#transaction-conflict-handling)\.  Units: `Count` Dimensions: `TableName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| UserErrors |  Requests to DynamoDB or Amazon DynamoDB Streams that generate an HTTP 400 status code during the specified time period\. An HTTP 400 usually indicates a client\-side error, such as an invalid combination of parameters, an attempt to update a nonexistent table, or an incorrect request signature\. All such events are reflected in the `UserErrors` metric, except for the following: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html) `UserErrors` represents the aggregate of HTTP 400 errors for DynamoDB or Amazon DynamoDB Streams requests for the current AWS Region and the current AWS account\. Units: `Count` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| WriteThrottleEvents |  Requests to DynamoDB that exceed the provisioned write capacity units for a table or a global secondary index\. A single request can result in multiple events\. For example, a `PutItem` request on a table with three global secondary indexes would result in four events—the table write, and each of the three index writes\. For each event, the `WriteThrottleEvents` metric is incremented by one if that event is throttled\. For single `PutItem` requests, if any of the events are throttled, `ThrottledRequests` is also incremented by one\. For `BatchWriteItem`, the `ThrottledRequests` metric for the entire `BatchWriteItem` is not incremented unless all of the individual `PutItem` or `DeleteItem` events are throttled\. The `TableName` dimension returns the `WriteThrottleEvents` for the table, but not for any global secondary indexes\. To view `WriteThrottleEvents` for a global secondary index, you must specify both `TableName` and `GlobalSecondaryIndex`\.  Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 

#### Dimensions for DynamoDB Metrics<a name="dynamodb-metric-dimensions"> <!-- omit in toc -->

The metrics for DynamoDB are qualified by the values for the account, table name, global secondary index name, or operation\. You can use the CloudWatch console to retrieve DynamoDB data along any of the dimensions in the table below\.


|  Dimension  |  Description  | 
| --- | --- | 
|  GlobalSecondaryIndexName  |  This dimension limits the data to a global secondary index on a table\. If you specify `GlobalSecondaryIndexName`, you must also specify `TableName`\.  | 
|  Operation  |  This dimension limits the data to one of the following DynamoDB operations: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html) In addition, you can limit the data to the following Amazon DynamoDB Streams operation: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
|  ReceivingRegion  |  This dimension limits the data to a particular AWS region\. It is used with metrics originating from replica tables within a DynamoDB global table\.  | 
|  StreamLabel  |  This dimension limits the data to a specific stream label\. It is used with metrics originating from Amazon DynamoDB Streams `GetRecords` operations\.  | 
|  TableName  |  This dimension limits the data to a specific table\. This value can be any table name in the current region and the current AWS account\.  | 


## Endnotes


## Capital Group Control Statements 
1. All Data-at-rest must be encrypted and use a CG BYOK encryption key.
2. All Data-in-transit must be encrypted using certificates using CG Certificate Authority.
3. Keys storied in a Key Management System (KMS) should be created by Capital Group's hardware security module (HSM) and are a minimum of AES-256.
4. AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch.
5. AWS IAM User accounts are only to be created for use by services or products that do not support IAM Roles. Services are not allowed to create local accounts for human use within the service. All human user authentication will take place within CG’s Identity Provider.
6. Any AWS service used by CG should not be directly available to the Internet and the default route is always the CG gateway.
7. Use of AWS IAM accounts are restricted to CG networks.
8. AWS IAM User secrets, including passwords and secret access keys, are to be rotated every 90 days. Accounts created locally within any service must also have their secrets rotated every 90 days.
9. Encryption keys are rotated annually.
10. Administrative access to AWS resources will have MFA enabled

## Glossary
**Data** - Digital pieces of information stored or transmitted for use with an information system from which understandable information is derived. Items considered to be data are: Source code, meta-data, build artifacts, information input and output.  
 
**Information System** - An organized assembly of resources and procedures for the collection, processing, maintenance, use, sharing, dissemination, or disposition of information. All systems, platforms, compute instances including and not limited to physical and virtual client endpoints, physical and virtual servers, software containers, databases, Internet of Things (IoT) devices, network devices, applications (internal and external), Serverless computing instances (i.e. AWS Lambda), vendor provided appliances, and third-party platforms, connected to the Capital Group network or used by Capital Group users or customers.
 
**Log** - a record of the events occurring within information systems and networks. Logs are composed of log entries; each entry contains information related to a specific event that has occurred within a system or network.
 
**Information** - communication or representation of knowledge such as facts, data, or opinions in any medium or form, including textual, numerical, graphic, cartographic, narrative, or audiovisual. 
 
**Cloud Computing** - A model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.
 
**Vulnerability**  - Weakness in an information system, system security procedures, internal controls, or implementation that could be exploited or triggered by a threat source. Note: The term weakness is synonymous for deficiency. Weakness may result in security and/or privacy risks.
