<img src="https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png" alt="AWS" width="250"/>

# AWS CodePipeline - Security Runbook <!-- omit in toc -->
## NIST Cybersecurity Framework Alignment <!-- omit in toc -->

**Generated By:**  
Freddie Wilson  
Tony DeMarco

*AWS Professional Services*

## Disclaimer
> The following applies to this document and all other documents, information, data, and responses (written or verbal) provided by Amazon Web Services, Inc. or any of its affiliates (collectively, "**AWS**") in connection with responding to this request and other related requests (collectively, this "**Response**"): This Response is expressly (a) informational only and provided solely for discussion purposes, (b) non-binding and not an offer to contract that can be accepted by any party, (c) provided "as is" with no representations or warranties whatsoever, and (d) based on AWS's current knowledge and may change at any time due to a variety of factors such as changes to your requirements or changes to AWS's service offerings. All obligations must be set forth in a separate, definitive written agreement between the parties. Neither party will have any liability for any failure or refusal to enter into a definitive agreement. All use of AWS's service offerings will be governed by the AWS Customer Agreement available at [http://aws.amazon.com/agreement/](http://aws.amazon.com/agreement/) (or other definitive written agreement between the parties governing the use of AWS's service offerings) (as applicable, the "**Agreement**"). If the parties have an applicable Nondisclosure Agreement ("**NDA**"), then the NDA will apply to all Confidential Information (as defined in the NDA) disclosed in connection with this Response. AWS's pricing is publicly available and subject to change in accordance with the Agreement. Pricing information (if any) provided in this Response is only an estimate and is expressly not a binding quote. Fees and charges will be based on actual usage of AWS services, which may vary from the estimates provided. Nothing in this Response will modify or supplement the terms of the Agreement or the NDA. No part of this Response may be disclosed without AWS's prior written consent. 

## Table of Contents <!-- omit in toc -->
- [Disclaimer](#disclaimer)
- [Overview](#overview)
- [Preventative Controls](#preventative-controls)
  - [1. Leverage AWS managed policies for CodePipeline](#1-leverage-aws-managed-policies-for-codepipeline)
  - [2. Grant Approval permissions to an IAM User in CodePipeline](#2-grant-approval-permissions-to-an-iam-user-in-codepipeline)
  - [3. Leverage tags to restrict user access based on tag value](#3-leverage-tags-to-restrict-user-access-based-on-tag-value)
  - [4. Use GitHub OAuth tokens and personal access tokens to access your GitHub repositories and retrieve the latest changes](#4-use-github-oauth-tokens-and-personal-access-tokens-to-access-your-github-repositories-and-retrieve-the-latest-changes)
  - [5. Configure Server\-Side Encryption for Artifacts Stored in Amazon S3 for CodePipeline](#5-configure-server-side-encryption-for-artifacts-stored-in-amazon-s3-for-codepipeline)
  - [6. Limit sensitive functions (for both console and CLI access) to authorized individuals](#6-limit-sensitive-functions-for-both-console-and-cli-access-to-authorized-individuals)
  - [7. Employ MFA for sensitive S3 resources repositories](#7-employ-mfa-for-sensitive-s3-resources-repositories)
  - [8. Implement versioning for Code Respositories and implement MFA Delete](#8-implement-versioning-for-code-respositories-and-implement-mfa-delete)
  - [9. Connect your VPC to CodePipeline by defining an interface VPC endpoint for CodePipeline](#9-connect-your-vpc-to-codepipeline-by-defining-an-interface-vpc-endpoint-for-codepipeline)
- [Detective](#detective)
  - [1. Use file Integrity validation](#1-use-file-integrity-validation)
  - [2. Leverage S3 Access Logging2](#2-leverage-s3-access-logging)
  - [3. Log CodePipeline API calls with AWS CloudTrail](#3-log-codepipeline-api-calls-with-aws-cloudtrail)
- [Respond/Recover](#respondrecover)
  - [1. Detect and React to changes in Pipeline State with Amazon CloudWatch Events](#1-detect-and-react-to-changes-in-pipeline-state-with-amazon-cloudwatch-events)
- [Endnotes](#endnotes)

## Overview
AWS provides a number of security features for CodePipeline which help you comply with the NIST Cybersecurity Framework. The following runbook will outline what the AWS best practices are, how they align to NIST, and how to implement these best practices within your organization.

These NIST Controls and Subcategories are not applicable to this service: AB.CD-1, EF.GH, IJ.KL-2

These Capital Group control statements are not applicable to this service: 5,7,8,9,10

## Preventative Controls
### 1. Leverage AWS managed Policies for CodePipeline
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.AC-1|Identities and credentials are managed for authorized devices and users Identities and credentials are issued, managed, verified, revoked, and audited for authorized devices, users and processes|
|PR.AC-3|Remote access is managed|
|PR.AC-4|Access permissions are managed, incorporating the principles of least privilege and separation of duties|
|PR.AC-6|Identities are proofed and bound to credentials and asserted in interactions|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|#|Not applicable|


**Why?** 
AWS addresses many common use cases by providing standalone IAM policies that are created and administered by AWS\. Managed policies grant necessary permissions for common use cases so you can avoid having to investigate which permissions are needed\. 
**How?** 
## AWS Managed \(Predefined\) Policies for CodePipeline<a name="managed-policies"></a>

For more information, see [AWS Managed Policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies) in the *IAM User Guide*\.

The following AWS managed policies, which you can attach to users in your account, are specific to CodePipeline:
+ `AWSCodePipelineFullAccess` – Grants full access to CodePipeline\.
+ `AWSCodePipelineCustomActionAccess` – Grants permission to an IAM user to create custom actions in CodePipeline or integrate Jenkins resources for build or test actions\.
+ `AWSCodePipelineReadOnlyAccess` – Grants read\-only access to CodePipeline\.
+ `AWSCodePipelineApproverAccess` – Grants permission to an IAM user to approve or reject a manual approval action\.


### 2. Grant Approval permissions to an IAM User in CodePipeline
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.AC-1|Identities and credentials are managed for authorized devices and users Identities and credentials are issued, managed, verified, revoked, and audited for authorized devices, users and processes|
|PR.AC-3|Remote access is managed|
|PR.AC-4|Access permissions are managed, incorporating the principles of least privilege and separation of duties|
|PR.AC-6|Identities are proofed and bound to credentials and asserted in interactions|
|PR.PT-3|The principle of least functionality is incorprated by configuring systems to provide essential capabilities 


Capital Group:
|Control Statement|Description|
|------|----------------------|
|5|Local AWS IAM accounts are restricted to services and no user accounts are to be provisioned including IaaS resources.|
|8|Local IAM secrets are rotated every 90 days, including accounts IaaS resources.|

**Why?** 
Before IAM users in your organization can approve or reject approval actions, they must be granted permissions to access pipelines and to update the status of approval actions\.

**How?** 
# Grant Approval Permissions to an IAM User in CodePipeline<a name="approvals-iam-permissions"></a>

You can grant permission to access all pipelines and approval actions in your account by attaching the `AWSCodePipelineApproverAccess` managed policy to an IAM user, role, or group; or you can to grant limited permissions by specifying the individual resources that can be accessed by an IAM user, role, or group\.

**Note**  
The permissions described in this topic grant very limited access\. To enable a user, role, or group to do more than approve or reject approval actions, you can attach other managed policies\. For information about the managed policies available for CodePipeline, see [AWS Managed \(Predefined\) Policies for CodePipeline](managed-policies.md)\.

## Grant Approval Permission to All Pipelines and Approval Actions<a name="approvals-iam-permissions-all"></a>

1. Sign in to the AWS Management Console and open the IAM console at [https://console\.aws\.amazon\.com/iam/](https://console.aws.amazon.com/iam/)\.

1. In the navigation pane, choose **Groups**, **Roles**, or **Users**\.

1. Choose the group, role or IAM user to grant permissions to\.

1. Choose the **Permissions** tab\.

1. Choose **Add permissions**, and then choose **Attach existing policies directly **\.

1. Select the check box next to `AWSCodePipelineApproverAccess` managed policy, and then choose **Next: Review**\.

1. Choose **Add permissions**\.

## Specify Approval Permission for Specific Pipelines and Approval Actions<a name="approvals-iam-permissions-limited"></a>

1. Sign in to the AWS Management Console and open the IAM console at [https://console\.aws\.amazon\.com/iam/](https://console.aws.amazon.com/iam/)\.
**Important**  
Make sure you are signed in to the AWS Management Console with the same account information you used in [Getting Started with CodePipeline](getting-started-codepipeline.md)\.

1. In the navigation pane, choose **Groups** or **Users**, as appropriate\.

1. Browse to and choose the user or group you want to change\. 

1. Do one of the following:
   + If you chose **Groups**, choose the **Permissions** tab, and expand **Inline Policies**\. If no inline policies have been created yet, choose **click here**\.

     Choose **Custom Policy**, and then choose **Select**\. 

     In **Policy Name**, enter a name for this policy\. Continue to the next step to paste the policy in the **Policy Document** box\.
   + If you chose **Users**, choose the **Permissions** tab, and choose **Add inline policy**\. Choose the **JSON** tab\. Continue to the next step to paste the policy\.

1. Paste the policy into the policy box\. Specify the individual resources an IAM user can access\. For example, the following policy grants users the authority to approve or reject only the action named `MyApprovalAction` in the `MyFirstPipeline` pipeline in the US East \(Ohio\) Region \(us\-east\-2\):

   ```
   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Action": [
                   "codepipeline:ListPipelines"
               ],
               "Resource": [
                   "*"
               ],
               "Effect": "Allow"
           },
           {
               "Action": [
                   "codepipeline:GetPipeline",
                   "codepipeline:GetPipelineState",
                   "codepipeline:GetPipelineExecution"
               ],
               "Effect": "Allow",
               "Resource": "arn:aws:codepipeline:us-east-2:80398EXAMPLE:MyFirstPipeline"
           },
           {
               "Action": [
                   "codepipeline:PutApprovalResult"
               ],
               "Effect": "Allow",
               "Resource": "arn:aws:codepipeline:us-east-2:80398EXAMPLE:MyFirstPipeline/MyApprovalStage/MyApprovalAction"
           }
       ]
   }
   ```
**Note**  
The `codepipeline:ListPipelines` permission is required only if IAM users need to access the CodePipeline dashboard to view this list of pipelines\. If console access is not required, you can omit `codepipeline:ListPipelines`\.

1. Do one of the following:
   + If you chose **Groups**, choose **Validate Policy**\. Correct any errors displayed in a red box at the top of the page\. Choose **Apply Policy**\.
   + If you chose **Users**, choose **Review policy**\.

     In **Name**, enter a name for this policy\. Choose **Create policy**\.

### 3. Leverage tags to restrict user access based on tag value
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.AC-7|Users, devices, and other assets are authenticated (e.g., single-factor, multi-factor) commensurate with the risk of the transaction (e.g., individuals’ security and privacy risks and other organizational risks)|
|PR.AC-6|Identities are proofed and bound to credentials and asserted in interactions|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|#|Not applicable|

**Why?** 
Access controls provide reasonable assurance that logical access to programs, data, and compute resources is restricted to authorized and appropriate users, and such users are restricted to performing authorized and appropriate actions.

**How?** 
# Using Tags to Control Access to CodePipeline Resources<a name="tag-based-access-control"></a>

Conditions in IAM user policy statements are part of the syntax that you use to specify permissions to resources required by CodePipeline actions\. Using tags in conditions is one way to control access to resources and requests\. For information about tagging CodePipeline resources, see [Tagging Resources](tag-resources.md)\. This topic discusses tag\-based access control\.

When you design IAM policies, you might be setting granular permissions by granting access to specific resources\. As the number of resources that you manage grows, this task becomes more difficult\. Tagging resources and using tags in policy statement conditions can make this task easier\. You grant access in bulk to any resource with a certain tag\. Then you repeatedly apply this tag to relevant resources, during creation or later\.

Tags can be attached to the resource or passed in the request to services that support tagging\. In CodePipeline, resources can have tags, and some actions can include tags\. When you create an IAM policy, you can use tag condition keys to control:
+ Which users can perform actions on a pipeline resource, based on tags that it already has\.
+ Which tags can be passed in an action's request\.
+ Whether specific tag keys can be used in a request\.

For the complete syntax and semantics of tag condition keys, see [Controlling Access Using Tags](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html) in the *IAM User Guide*\.

The following examples demonstrate how to specify tag conditions in policies for CodePipeline users\.

**Example 1: Limit actions based on tags in the request**  
The `CodePipelineFullAccess` managed user policy gives users unlimited permission to perform any CodePipeline action on any resource\.  
The following policy limits this power and denies unauthorized users permission to create pipelines for specific projects\. To do that, it denies the `CreatePipeline` action if the request specifies a tag named `Project` with one of the values `ProjectA` or `ProjectB`\. \(The `aws:RequestTag` condition key is used to control which tags can be passed in an IAM request\.\) In addition, the policy prevents these unauthorized users from tampering with the resources by using the `aws:TagKeys` condition key to not allow tag modification actions to include these same tag values or to completely remove the `Project` tag\. A customer's administrator must attach this IAM policy to unauthorized IAM users, in addition to the managed user policy\.  

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Action": [
        "codepipeline:CreatePipeline",
        "codepipeline:TagResource"
      ],
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/Project": ["ProjectA", "ProjectB"]
        }
      }
    },
    {
      "Effect": "Deny",
      "Action": [
        "codepipeline:UntagResource"
      ],
      "Resource": "*",
      "Condition": {
        "ForAllValues:StringEquals": {
          "aws:TagKeys": ["Project"]
        }
      }
    }
  ]
}
```

**Example 2: Limit actions based on resource tags**  
The `CodePipelineFullAccess` managed user policy gives users unlimited permission to perform any CodePipeline action on any resource\.  
The following policy limits this power and denies unauthorized users permission to perform actions on specified project pipelines\. To do that, it denies some actions if the resource has a tag named `Project` with one of the values `ProjectA` or `ProjectB`\. \(The `aws:ResourceTag` condition key is used to control access to the resources based on the tags on those resources\.\) A customer's administrator must attach this IAM policy to unauthorized IAM users, in addition to the managed user policy\.  

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Action": [
        "codepipeline:TagResource",
        "codepipeline:UntagResource",
        "codepipeline:UpdatePipeline",
        "codepipeline:DeletePipeline",
        "codepipeline:ListTagsForResource"
      ],
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "aws:ResourceTag/Project": ["ProjectA", "ProjectB"]
        }
      }
    }
  ]
}
```

**Example 3: Allow actions based on tags in the request**  
The following policy grants users permission to create development pipelines in CodePipeline\.  
To do that, it allows the `CreatePipeline` and `TagResource` actions if the request specifies a tag named `Project` with the value `ProjectA`\. \(The `aws:RequestTag` condition key is used to control which tags can be passed in an IAM request\.\) The `aws:TagKeys` condition ensures tag key case sensitivity\. This policy is useful for IAM users who don't have the `CodePipelineFullAccess` managed user policy attached\. The managed policy gives users unlimited permission to perform any CodePipeline action on any resource\.  

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "codepipeline:CreatePipeline",
        "codepipeline:TagResource"
      ],
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "aws:RequestTag/Project": "ProjectA"
        },
        "ForAllValues:StringEquals": {
          "aws:TagKeys": ["Project"]
        }
      }
    }
  ]
}
```

**Example 4: Allow actions based on resource tags**  
The following policy grants users permission to perform actions on, and get information about, project pipelines in CodePipeline\.  
To do that, it allows specific actions if the pipeline has a tag named `Project` with the value `ProjectA`\. \(The `aws:RequestTag` condition key is used to control which tags can be passed in an IAM request\.\) The `aws:TagKeys` condition ensures tag key case sensitivity\. This policy is useful for IAM users who don't have the `CodePipelineFullAccess` managed user policy attached\. The managed policy gives users unlimited permission to perform any CodePipeline action on any resource\.  

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "codepipeline:UpdatePipeline",
        "codepipeline:DeletePipeline",
        "codepipeline:ListPipelines"
      ],
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "aws:ResourceTag/Project": "ProjectA"
        },
        "ForAllValues:StringEquals": {
          "aws:TagKeys": ["Project"]
        }
      }
    }
  ]
}
```
### 4. Use GitHub OAuth tokens and personal access tokens to access your GitHub repositories and retrieve the latest changes.
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.AC-7|Users, devices, and other assets are authenticated (e.g., single-factor, multi-factor) commensurate with the risk of the transaction (e.g., individuals’ security and privacy risks and other organizational risks)|
|PR.AC-6|Identities are proofed and bound to credentials and asserted in interactions|
|PR.DS-5|Protections against data leaks are implemented|
|PR.PT-3|Access to systems and assets is controlled, incorporating the principle of least functionality|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|#|Not applicable|

**Why?** 
Access controls provide reasonable assurance that logical access to programs, data, and compute resources is restricted to authorized and appropriate users, and such users are restricted to performing authorized and appropriate actions.

**How?** 
# Configure GitHub Authentication<a name="GitHub-authentication"></a>

CodePipeline uses GitHub OAuth tokens and personal access tokens to access your GitHub repositories and retrieve the latest changes\. There are two ways to configure authentication in GitHub:
+ AWS creates a default AWS\-managed OAuth token when you use the console to create or update pipelines\. 
+ You can create and manage your own customer\-generated personal access tokens\. You need personal access tokens when you use the CLI, SDK, or AWS CloudFormation to create or update your pipeline\. 

**Topics**
+ [View Your Authorized OAuth Apps](#GitHub-view-oauth-token)
+ [Configure Your Pipeline to Use a Personal Access Token \(GitHub and CLI\)](GitHub-create-personal-token-CLI.md)
+ [Use GitHub and the CodePipeline CLI to Create and Rotate Your GitHub Personal Access Token on a Regular Basis](#GitHub-rotate-personal-token-CLI)

### View Your Authorized OAuth Apps<a name="GitHub-view-oauth-token"></a>

CodePipeline uses OAuth tokens to integrate with GitHub\. GitHub tracks the permissions of the OAuth token for CodePipeline\.

**To view your authorized integrations in GitHub**

1. In GitHub, from the drop\-down option on your profile photo, choose **Settings**\.

1. Choose **Applications**, and then choose **Authorized OAuth Apps**\.

1. Review your authorized apps\.  
![\[Review authorized apps in GitHub.\]](http://docs.aws.amazon.com/codepipeline/latest/userguide/images/oauth-apps.png)![\[Review authorized apps in GitHub.\]](http://docs.aws.amazon.com/codepipeline/latest/userguide/)![\[Review authorized apps in GitHub.\]](http://docs.aws.amazon.com/codepipeline/latest/userguide/)

### Use GitHub and the CodePipeline CLI to Create and Rotate Your GitHub Personal Access Token on a Regular Basis<a name="GitHub-rotate-personal-token-CLI"></a>

The advantage of using tokens instead of passwords in a script is that tokens can be revoked or rotated\. You can also grant specific privileges and permissions to a personal access token\. Tokens should be stored securely and rotated or regenerated routinely\. Token rotation is recommended by [RFC\-6819 \(OAuth 2\.0 Threat Model and Security Considerations\), section 5\.1\.5\.3](https://tools.ietf.org/html/rfc6819#section-5.1.5.3)\.

For more information, see [Creating a personal access token for the command line](https://help.github.com/articles/creating-an-access-token-for-command-line-use/) on the GitHub website\. 

After you have regenerated a new personal access token, you can rotate it by using the AWS CLI or API or by using AWS CloudFormation and calling `UpdatePipeline`\.

**Note**  
 You might have to update other applications if they are using the same personal access token\. As a security best practice, do not share a single token across multiple applications\. Create a new personal access token for each application\.

Use these steps to rotate your GitHub personal access token and then update the pipeline structure with the new token\.

**Note**  
After you rotate your personal access token, remember to update any AWS CLI scripts or AWS CloudFormation templates that contain the old token information\.

1. In GitHub, from the drop\-down option on your profile photo, choose **Settings**\. 

1. Choose **Developer settings**, and then choose **Personal access tokens**\.

1. Next to your GitHub personal access token, choose **Edit**\.  
![\[Edit the personal access token.\]](http://docs.aws.amazon.com/codepipeline/latest/userguide/images/personal-token1.png)![\[Edit the personal access token.\]](http://docs.aws.amazon.com/codepipeline/latest/userguide/)![\[Edit the personal access token.\]](http://docs.aws.amazon.com/codepipeline/latest/userguide/)

1. Choose **Regenerate token**\.  
![\[Edit the personal access token by regenerating it.\]](http://docs.aws.amazon.com/codepipeline/latest/userguide/images/personal-token2.png)![\[Edit the personal access token by regenerating it.\]](http://docs.aws.amazon.com/codepipeline/latest/userguide/)![\[Edit the personal access token by regenerating it.\]](http://docs.aws.amazon.com/codepipeline/latest/userguide/)

1. Next to the regenerated token, choose the copy icon\.  
![\[Copy the rotated token.\]](http://docs.aws.amazon.com/codepipeline/latest/userguide/images/personal-token3.png)![\[Copy the rotated token.\]](http://docs.aws.amazon.com/codepipeline/latest/userguide/)![\[Copy the rotated token.\]](http://docs.aws.amazon.com/codepipeline/latest/userguide/)

1. At a terminal \(Linux, macOS, or Unix\) or command prompt \(Windows\), run the get\-pipeline command on the pipeline where you want to change the personal access token, and then copy the output of the command to a JSON file\. For example, for a pipeline named MyFirstPipeline, you would type something similar to the following:

   ```
   aws codepipeline get-pipeline --name MyFirstPipeline >pipeline.json
   ```

   The output of the command is sent to the `pipeline.json` file\.

1. Open the file in a plain\-text editor and edit the value in the `OAuthTokenField` of your GitHub action\.

   When you use the AWS CLI to create the pipeline, you can pass your GitHub personal access token in this field\. Replace the asterisks \(\*\*\*\*\) with the token you copied from GitHub\. When you run `get-pipeline` to view the action configuration, the four\-asterisk mask is displayed for this value\. For example, for a personal access token with the value `111222333444555666777888EXAMPLE`:

   ```
   "configuration": {
           "Owner": "MyGitHubUserName",
           "Repo": "test-repo",
           "Branch": "master",
           "OAuthToken": "111222333444555666777888EXAMPLE"
       }
   ```
**Note**  
When you use an AWS CloudFormation template to update the pipeline, you must first store the token as a secret in AWS Secrets Manager\. You include the value for this field as a dynamic reference to the stored secret in Secrets Manager\. For an example, see [GitHub](action-reference-GitHub.md)\.

1. If you are working with the pipeline structure retrieved using the `get-pipeline` command, you must modify the structure in the JSON file by removing the `metadata` lines from the file\. Otherwise, the `update-pipeline` command cannot use it\. Remove the section from the pipeline structure in the JSON file \(the `"metadata": { }` lines and the `"created"`, `"pipelineARN"`, and `"updated"` fields\)\.

   For example, remove the following lines from the structure: 

   ```
   "metadata": {  
     "pipelineArn": "arn:aws:codepipeline:region:account-ID:pipeline-name",
     "created": "date",
     "updated": "date"
     }
   ```

1. Save the file, and then run update\-pipeline with the `--cli-input-json` parameter to specify the JSON file you just edited\. For example, to update a pipeline named MyFirstPipeline, you would type something similar to the following:
**Important**  
Be sure to include `file://` before the file name\. It is required in this command\.

   ```
   aws codepipeline update-pipeline --cli-input-json file://pipeline.json
   ```

1. When you have finished updating your pipelines, delete the JSON files\.


### 5. Configure Server\-Side Encryption for Artifacts Stored in Amazon S3 for CodePipeline
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.DS-1|Data-at-rest is protected|
|PR.DS-5|Protections against data leaks are implemented|


Capital Group:
|Control Statement|Description|
|------|----------------------|
|1|All Data-at-rest must be encrypted and use a CG BYOK encryption key.|
|3|Keys storied in a Key Management System (KMS) should be created by Capital Group's hardware security module (HSM) and are a minimum of AES-256|
|9|Encryption keys are rotated annually.|

**Why?** 
Data in CodePipeline is encrypted at rest using service-owned KMS keys. Code artifacts are stored in a customer-owned S3 bucket and encrypted with either the default AWS-managed SSE-KMS encryption key or a customer-managed SSE-KMS key.

**How?** 
# Configure Server\-Side Encryption for Artifacts Stored in Amazon S3 for CodePipeline<a name="S3-artifact-encryption"></a>

There are two ways to configure server\-side encryption for Amazon S3 artifacts:
+ CodePipeline creates an S3 artifact bucket and default AWS\-managed SSE\-KMS encryption keys when you create a pipeline using the Create Pipeline wizard\. The master key is encrypted along with object data and managed by AWS\.
+ You can create and manage your own customer\-managed SSE\-KMS keys\.

**Important**  
CodePipeline only supports symmetric customer master keys \(CMKs\)\. Do not use an asymmetric CMK to encrypt the data in your S3 bucket\.

If you are using the default S3 key, you cannot change or delete this AWS\-managed key\. If you are using a customer\-managed key (as is required by Capital group control #1) in AWS KMS to encrypt or decrypt artifacts in the S3 bucket, you can change or rotate this key as necessary\.

Amazon S3 supports bucket policies that you can use if you require server\-side encryption for all objects that are stored in your bucket\. For example, the following bucket policy denies upload object \(`s3:PutObject`\) permission to everyone if the request does not include the `x-amz-server-side-encryption` header requesting server\-side encryption with SSE\-KMS\.

```
{
    "Version": "2012-10-17",
    "Id": "SSEAndSSLPolicy",
    "Statement": [
        {
            "Sid": "DenyUnEncryptedObjectUploads",
            "Effect": "Deny",
            "Principal": "*",
            "Action": "s3:PutObject",
            "Resource": "arn:aws:s3:::codepipeline-us-west-2-89050EXAMPLE/*",
            "Condition": {
                "StringNotEquals": {
                    "s3:x-amz-server-side-encryption": "aws:kms"
                }
            }
        },
        {
            "Sid": "DenyInsecureConnections",
            "Effect": "Deny",
            "Principal": "*",
            "Action": "s3:*",
            "Resource": "arn:aws:s3:::codepipeline-us-west-2-89050EXAMPLE/*",
            "Condition": {
                "Bool": {
                    "aws:SecureTransport": "false"
                }
            }
        }
    ]
}
```

### View Your Default Amazon S3 SSE\-KMS Encryption Keys<a name="S3-view-default-keys"></a>

When you use the **Create Pipeline** wizard to create your first pipeline, an S3 bucket is created for you in the same Region you created the pipeline\. The bucket is used to store pipeline artifacts\. When a pipeline runs, artifacts are put into and retrieved from the S3 bucket\. By default, CodePipeline uses server\-side encryption with the AWS KMS\-managed keys \(SSE\-KMS\) using the default key for Amazon S3 \(the `aws/s3` key\)\. This key is created and stored in your AWS account\. When artifacts are retrieved from the S3 bucket, CodePipeline uses the same SSE\-KMS process to decrypt the artifact\. 

**To view information about your default AWS KMS key**

1. Sign in to the AWS Management Console and open the IAM console at [https://console\.aws\.amazon\.com/iam/](https://console.aws.amazon.com/iam/)\.

1. In the service navigation pane, choose **Encryption Keys**\. \(If a welcome page appears, choose **Get Started Now**\.\)

1. In **Filter**, choose the Region for your pipeline\. For example, if the pipeline was created in `us-east-2`, make sure the filter is set to US East \(Ohio\)\.

   For more information about the Regions and endpoints available for CodePipeline, see [AWS CodePipeline Endpoints and Quotas](https://docs.aws.amazon.com/general/latest/gr/rande.html#codepipeline_region)\.

1. In the list of encryption keys, choose the key with the alias used for your pipeline \(by default, **aws/s3**\)\. Basic information about the key is displayed\.

### Configure Server\-Side Encryption for S3 Buckets Using AWS CloudFormation or the AWS CLI<a name="S3-rotate-customer-key"></a>

When you use AWS CloudFormation or the AWS CLI to create a pipeline, you must configure server\-side encryption manually\. Use the sample bucket policy above, and then create your own customer\-managed SSE\-KMS encryption keys\. You can also use your own keys instead of the default Amazon S3 key\. Some advantages to using your own key include:
+ You want to rotate the key on a schedule to meet business or security requirements for your organization\.
+ You want to create a pipeline that uses resources associated with another AWS account\. This requires the use of a customer\-managed key\. For more information, see [Create a Pipeline in CodePipeline That Uses Resources from Another AWS Account](pipelines-create-cross-account.md)\. 

Cryptographic best practices discourage extensive reuse of encryption keys\. As a best practice, rotate your key on a regular basis\. To create new cryptographic material for your AWS Key Management Service \(AWS KMS\) customer master keys \(CMKs\), you can create CMKs, and then change your applications or aliases to use the new CMKs\. Or, you can enable automatic key rotation for an existing CMK\. 

To rotate your SSE\-KMS customer master key, see [Rotating Customer Master Keys](https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html)\. 

**Important**  
CodePipeline only supports symmetric customer master keys \(CMKs\)\. Do not use an asymmetric CMK to encrypt the data in your S3 bucket\.

### 6. Limit sensitive functions (for both console and CLI access) to authorized individuals.
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.AC-4|Access permissions are managed, incorporating the principles of least privilege and separation of duties|
|PR.AC-6|Identities are proofed and bound to credentials and asserted in interactions|
|PR.AC-7|Users, devices, and other assets are authenticated (e.g., single-factor, multi-factor) commensurate with the risk of the transaction (e.g., individuals’ security and privacy risks and other organizational risks)|


Capital Group:
|Control Statement|Description|
|------|----------------------|
|1|All Data-at-rest must be encrypted and use a CG BYOK encryption key.|
|3|Keys storied in a Key Management System (KMS) should be created by Capital Group's hardware security module (HSM) and are a minimum of AES-256|
|9|Encryption keys are rotated annually.|

**Why?** 
These controls support Limiting sensitive functions, i.e., those related to retrieving artifacts from Amazon S3 buckets; accessing information about applications and deployment groups; and granting permission to approve or reject a manual approval action to a limited set of authorized individuals to maintain control of these actions.

**How?** 
# Manage Approval Actions in CodePipeline<a name="approvals"></a>

In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action\. 

If the action is approved, the pipeline execution resumes\. If the action is rejected—or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping—the result is the same as an action failing, and the pipeline execution does not continue\.

You might use manual approvals for these reasons:
+ You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline\.
+ You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released\.
+ You want someone to review new or updated text before it is published to a company website\.

## Configuration Options for Manual Approval Actions in CodePipeline<a name="approvals-configuration-options"></a>

CodePipeline provides three configuration options you can use to tell approvers about the approval action\. 

**Publish Approval Notifications** You can configure an approval action to publish a message to an Amazon Simple Notification Service topic when the pipeline stops at the action\. Amazon SNS delivers the message to every endpoint subscribed to the topic\. You must use a topic created in the same AWS region as the pipeline that will include the approval action\. When you create a topic, we recommend you give it a name that will identify its purpose, in formats such as `MyFirstPipeline-us-east-2-approval`\. 

When you publish approval notifications to Amazon SNS topics, you can choose from formats such as email or SMS recipients, SQS queues, HTTP/HTTPS endpoints, or AWS Lambda functions you invoke using Amazon SNS\. For information about Amazon SNS topic notifications, see the following topics:
+ [What Is Amazon Simple Notification Service?](https://docs.aws.amazon.com/sns/latest/dg/welcome.html)
+ [Create a Topic in Amazon SNS](https://docs.aws.amazon.com/sns/latest/dg/CreateTopic.html)
+ [Sending Amazon SNS Messages to Amazon SQS Queues](https://docs.aws.amazon.com/sns/latest/dg/SendMessageToSQS.html)
+ [Subscribing a Queue to an Amazon SNS Topic](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqssubscribe.html)
+ [Sending Amazon SNS Messages to HTTP/HTTPS Endpoints](https://docs.aws.amazon.com/sns/latest/dg/SendMessageToHttp.html)
+ [Invoking Lambda Functions Using Amazon SNS Notifications](https://docs.aws.amazon.com/sns/latest/dg/sns-lambda.html)

# JSON Data Format for Manual Approval Notifications in CodePipeline<a name="approvals-json-format"></a>

For approval actions that use Amazon SNS notifications, JSON data about the action is created and published to Amazon SNS when the pipeline stops\. You can use the JSON output to send messages to Amazon SQS queues or invoke functions in AWS Lambda\. 

The following example shows the structure of the JSON output available with CodePipeline approvals\.

```
{
    "region": "us-east-2",
    "consoleLink": "https://console.aws.amazon.com/codepipeline/home?region=us-east-2#/view/MyFirstPipeline",
    "approval": {
        "pipelineName": "MyFirstPipeline",
        "stageName": "MyApprovalStage",
        "actionName": "MyApprovalAction",
        "token": "1a2b3c4d-573f-4ea7-a67E-XAMPLETOKEN",
        "expires": "2016-07-07T20:22Z",
        "externalEntityLink": "http://example.com",
        "approvalReviewLink": "https://console.aws.amazon.com/codepipeline/home?region=us-east-2#/view/MyFirstPipeline/MyApprovalStage/MyApprovalAction/approve/1a2b3c4d-573f-4ea7-a67E-XAMPLETOKEN",
        "customData": "Review the latest changes and approve or reject within seven days."
    }
}
```
**Specify a URL for Review** As part of the configuration of the approval action, you can specify a URL to be reviewed\. The URL might be a link to a web application you want approvers to test or a page with more information about your approval request\. The URL is included in the notification that is published to the Amazon SNS topic\. Approvers can use the console or CLI to view it\. 

**Enter Comments for Approvers** When you create an approval action, you can also add comments that are displayed to those who receive the notifications or those who view the action in the console or CLI response\.

**No Configuration Options** You can also choose not to configure any of these three options\. You might not need them if, for example, you can notify someone directly that the action is ready for their review, or you simply want the pipeline to stop until you decide to approve the action yourself\. 

## Setup and Workflow Overview for Approval Actions in CodePipeline<a name="approvals-overview"></a>

The following is an overview for setting up and using manual approvals\. 

1. You grant the IAM permissions required for approving or rejecting approval actions to one or more IAM users in your organization\. 

1. \(Optional\) If you are using Amazon SNS notifications, you ensure that the service role you use in your CodePipeline operations has permission to access Amazon SNS resources\. 

1. \(Optional\) If you are using Amazon SNS notifications, you create an Amazon SNS topic and add one or more subscribers or endpoints to it\. 

1. When you use the AWS CLI to create the pipeline or after you have used the CLI or console to create the pipeline, you add an approval action to a stage in the pipeline\. 

   If you are using notifications, you include the Amazon Resource Name \(ARN\) of the Amazon SNS topic in the configuration of the action\. \(An ARN is a unique identifier for an Amazon resource\. ARNs for Amazon SNS topics are structured like *arn:aws:sns:us\-east\-2:80398EXAMPLE:MyApprovalTopic*\. For more information, see [Amazon Resource Names \(ARNs\) and AWS Service Namespaces](https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html) in the *Amazon Web Services General Reference*\.\)

1. The pipeline stops when it reaches the approval action\. If an Amazon SNS topic ARN was included in the configuration of the action, a notification is published to the Amazon SNS topic, and a message is delivered to any subscribers to the topic or subscribed endpoints, with a link to review the approval action in the console\.

1. An approver examines the target URL and reviews comments, if any\.

1. Using the console, CLI, or SDK, the approver provides a summary comment and submits a response:
   + Approved: The pipeline execution resumes\.
   + Rejected: The stage status is changed to "Failed" and the pipeline execution does not resume\. 

   If no response is submitted within seven days, the action is marked as "Failed\."


### 7. Employ MFA for sensitive S3 resources repositories.
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.AC-1|Identities and credentials are managed for authorized devices and users Identities and credentials are issued, managed, verified, revoked, and audited for authorized devices, users and processes|
|PR.AC-3|Remote access is managed|
|PR.AC-4|UAccess permissions are managed, incorporating the principles of least privilege and separation of duties|  
|PR.AC-6|Identities are proofed and bound to credentials and asserted in interactions|  
|PR.AC-7|Users, devices, and other assets are authenticated (e.g., single-factor, multi-factor) commensurate with the risk of the transaction (e.g., individuals’ security and privacy risks and other organizational risks)|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|5|Local AWS IAM accounts are restricted to services and no user accounts are to be provisioned including IaaS resources.|


**Why?** 
S3 supports MFA-protected API access, a feature that can enforce multi-factor authentication (MFA) for access to your Amazon S3 resources where CodePipeline artifacts are stored.

**How?** 
## Add a Bucket Policy to Require MFA for S3 Code repositories<a name="example-bucket-policies-use-case-7"></a>

Amazon S3 supports MFA\-protected API access, a feature that can enforce multi\-factor authentication \(MFA\) for access to your Amazon S3 resources\. Multi\-factor authentication provides an extra level of security that you can apply to your AWS environment\. It is a security feature that requires users to prove physical possession of an MFA device by providing a valid MFA code\. For more information, see [AWS Multi\-Factor Authentication](https://aws.amazon.com/mfa/)\. You can require MFA for any requests to access your Amazon S3 resources\. 

You can enforce the MFA requirement using the `aws:MultiFactorAuthAge` key in a bucket policy\. AWS Identity and Access Management \(IAM\) users can access Amazon S3 resources by using temporary credentials issued by the AWS Security Token Service \(AWS STS\)\. You provide the MFA code at the time of the AWS STS request\. 

When Amazon S3 receives a request with multi\-factor authentication, the `aws:MultiFactorAuthAge` key provides a numeric value indicating how long ago \(in seconds\) the temporary credential was created\. If the temporary credential provided in the request was not created using an MFA device, this key value is null \(absent\)\. In a bucket policy, you can add a condition to check this value, as shown in the following example bucket policy\. The policy denies any Amazon S3 operation on the `/taxdocuments` folder in the `examplebucket` bucket if the request is not authenticated using MFA\.

```
 1. {
 2.     "Version": "2012-10-17",
 3.     "Id": "123",
 4.     "Statement": [
 5.       {
 6.         "Sid": "",
 7.         "Effect": "Deny",
 8.         "Principal": "*",
 9.         "Action": "s3:*",
10.         "Resource": "arn:aws:s3:::examplebucket/taxdocuments/*",
11.         "Condition": { "Null": { "aws:MultiFactorAuthAge": true }}
12.       }
13.     ]
14.  }
```

The `Null` condition in the `Condition` block evaluates to true if the `aws:MultiFactorAuthAge` key value is null, indicating that the temporary security credentials in the request were created without the MFA key\. 

The following bucket policy is an extension of the preceding bucket policy\. It includes two policy statements\. One statement allows the `s3:GetObject` permission on a bucket \(`examplebucket`\) to everyone\. Another statement further restricts access to the `examplebucket/taxdocuments` folder in the bucket by requiring MFA\. 

```
 1. {
 2.     "Version": "2012-10-17",
 3.     "Id": "123",
 4.     "Statement": [
 5.       {
 6.         "Sid": "",
 7.         "Effect": "Deny",
 8.         "Principal": "*",
 9.         "Action": "s3:*",
10.         "Resource": "arn:aws:s3:::examplebucket/taxdocuments/*",
11.         "Condition": { "Null": { "aws:MultiFactorAuthAge": true } }
12.       },
13.       {
14.         "Sid": "",
15.         "Effect": "Allow",
16.         "Principal": "*",
17.         "Action": ["s3:GetObject"],
18.         "Resource": "arn:aws:s3:::examplebucket/*"
19.       }
20.     ]
21.  }
```

You can optionally use a numeric condition to limit the duration for which the `aws:MultiFactorAuthAge` key is valid, independent of the lifetime of the temporary security credential used in authenticating the request\. For example, the following bucket policy, in addition to requiring MFA authentication, also checks how long ago the temporary session was created\. The policy denies any operation if the `aws:MultiFactorAuthAge` key value indicates that the temporary session was created more than an hour ago \(3,600 seconds\)\. 

```
 1. {
 2.     "Version": "2012-10-17",
 3.     "Id": "123",
 4.     "Statement": [
 5.       {
 6.         "Sid": "",
 7.         "Effect": "Deny",
 8.         "Principal": "*",
 9.         "Action": "s3:*",
10.         "Resource": "arn:aws:s3:::examplebucket/taxdocuments/*",
11.         "Condition": {"Null": {"aws:MultiFactorAuthAge": true }}
12.       },
13.       {
14.         "Sid": "",
15.         "Effect": "Deny",
16.         "Principal": "*",
17.         "Action": "s3:*",
18.         "Resource": "arn:aws:s3:::examplebucket/taxdocuments/*",
19.         "Condition": {"NumericGreaterThan": {"aws:MultiFactorAuthAge": 3600 }}
20.        },
21.        {
22.          "Sid": "",
23.          "Effect": "Allow",
24.          "Principal": "*",
25.          "Action": ["s3:GetObject"],
26.          "Resource": "arn:aws:s3:::examplebucket/*"
27.        }
28.     ]
29.  }
```
### 8. Implement versioning for Code Respositories and implement MFA Delete. 
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.AC-1|Identities and credentials are managed for authorized devices and users Identities and credentials are issued, managed, verified, revoked, and audited for authorized devices, users and processes|
|PR.AC-3|Remote access is managed|
|PR.AC-4|UAccess permissions are managed, incorporating the principles of least privilege and separation of duties|  
|PR.AC-6|Identities are proofed and bound to credentials and asserted in interactions|  
|PR.AC-7|Users, devices, and other assets are authenticated (e.g., single-factor, multi-factor) commensurate with the risk of the transaction (e.g., individuals’ security and privacy risks and other organizational risks)|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|5|Local AWS IAM accounts are restricted to services and no user accounts are to be provisioned including IaaS resources.|


**Why?** 
Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures.
By enabling MFA Delete you can protect against accidental and/or unauthorized deletion of artifacts in S3. By configuring a bucket to enable MFA (multi-factor authentication) Delete, it will require additional authentication for either of the following operations: (1) change the versioning state of your bucket, or (2) permanently delete an object version.

**How?** 
# Use versioning<a name="Versioning"></a>

Versioning is a means of keeping multiple variants of an object in the same bucket\. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket\. With versioning, you can easily recover from both unintended user actions and application failures\. When you enable versioning for a bucket, if Amazon S3 receives multiple write requests for the same object simultaneously, it stores all of the objects\.

If you enable versioning for a bucket, Amazon S3 automatically generates a unique version ID for the object being stored\. In one bucket, for example, you can have two objects with the same key, but different version IDs, such as `photo.gif` \(version 111111\) and `photo.gif `\(version 121212\)\. 


Versioning\-enabled buckets enable you to recover objects from accidental deletion or overwrite\. For example:
+ If you delete an object, instead of removing it permanently, Amazon S3 inserts a delete marker, which becomes the current object version\. You can always restore the previous version\. 
+ If you overwrite an object, it results in a new object version in the bucket\. You can always restore the previous version\.

**Important**  
If you have an object expiration lifecycle policy in your non\-versioned bucket and you want to maintain the same permanent delete behavior when you enable versioning, you must add a noncurrent expiration policy\. The noncurrent expiration lifecycle policy will manage the deletes of the noncurrent object versions in the version\-enabled bucket\. \(A version\-enabled bucket maintains one current and zero or more noncurrent object versions\.\) 

Buckets can be in one of three states: unversioned \(the default\), versioning\-enabled, or versioning\-suspended\.

**Important**  
Once you version\-enable a bucket, it can never return to an unversioned state\. You can, however, suspend versioning on that bucket\.

The versioning state applies to all \(never some\) of the objects in that bucket\. The first time you enable a bucket for versioning, objects in it are thereafter always versioned and given a unique version ID\. Note the following:
+ Objects stored in your bucket before you set the versioning state have a version ID of `null`\. When you enable versioning, existing objects in your bucket do not change\. What changes is how Amazon S3 handles the objects in future requests\. 
+ The bucket owner \(or any user with appropriate permissions\) can suspend versioning to stop accruing object versions\. When you suspend versioning, existing objects in your bucket do not change\. What changes is how Amazon S3 handles objects in future requests\. 

## How to configure versioning on a bucket<a name="how-to-enable-disable-versioning-intro"></a>

You can configure bucket versioning using any of the following methods:
+ Configure versioning using the Amazon S3 console\.
+ Configure versioning programmatically using the AWS SDKs\.

  Both the console and the SDKs call the REST API that Amazon S3 provides to manage versioning\. 
**Note**  
If you need to, you can also make the Amazon S3 REST API calls directly from your code\. However, this can be cumbersome because it requires you to write code to authenticate your requests\. 

  Each bucket you create has a *versioning* subresource associated with it\. By default, your bucket is unversioned, and accordingly the versioning subresource stores empty versioning configuration\.

  ```
  <VersioningConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
  </VersioningConfiguration>
  ```

  To enable versioning, you send a request to Amazon S3 with a versioning configuration that includes a status\. 

  ```
  <VersioningConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
    <Status>Enabled</Status> 
  </VersioningConfiguration>
  ```

  To suspend versioning, you set the status value to `Suspended`\. 

The bucket owner, an AWS account that created the bucket \(root account\), and authorized users can configure the versioning state of a bucket\. 

## MFA delete<a name="MultiFactorAuthenticationDelete"></a>

You can optionally add another layer of security by configuring a bucket to enable MFA \(multi\-factor authentication\) Delete, which requires additional authentication for either of the following operations:
+ Change the versioning state of your bucket
+ Permanently delete an object version

 MFA Delete requires two forms of authentication together:
+ Your security credentials
+ The concatenation of a valid serial number, a space, and the six\-digit code displayed on an approved authentication device

MFA Delete thus provides added security in the event, for example, your security credentials are compromised\. 

To enable or disable MFA Delete, you use the same API that you use to configure versioning on a bucket\. Amazon S3 stores the MFA Delete configuration in the same *versioning* subresource that stores the bucket's versioning status\.

 MFA Delete can help prevent accidental bucket deletions by doing the following: 
+ Requiring the user who initiates the delete action to prove physical possession of an MFA device with an MFA code\.
+ Adding an extra layer of friction and security to the delete action\.

```
<VersioningConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
  <Status>VersioningState</Status>
  <MfaDelete>MfaDeleteState</MfaDelete>  
</VersioningConfiguration>
```

**Note**  
 The bucket owner, the AWS account that created the bucket \(root account\), and all authorized IAM users can enable versioning, but only the bucket owner \(root account\) can enable MFA Delete\. 

To use MFA Delete, you can use either a hardware or virtual MFA device to generate an authentication code\. The following example shows a generated authentication code displayed on a hardware device\.

**Note**  
MFA Delete and MFA\-protected API access are features intended to provide protection for different scenarios\. You configure MFA Delete on a bucket to ensure that data in your bucket cannot be accidentally deleted\. MFA\-protected API access is used to enforce another authentication factor \(MFA code\) when accessing sensitive Amazon S3 resources\. You can require any operations against these Amazon S3 resources be done with temporary credentials created using MFA\.  

# Examples of enabling bucket versioning<a name="manage-versioning-examples"></a>

**Topics**
+ [Using the AWS SDK for Java](#manage-versioning-examples-java)
+ [Using the AWS SDK for \.NET](#manage-versioning-examples-dotnet)
+ [Using other AWS SDKs](#manage-versioning-examples-sdks)

 This section provides examples of enabling versioning on a bucket\. The examples first enable versioning on a bucket and then retrieve versioning status\. 

## Using the AWS SDK for Java<a name="manage-versioning-examples-java"></a>

**Example**  
For instructions on how to create and test a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import java.io.IOException;

import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Region;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3Client;
import com.amazonaws.services.s3.model.AmazonS3Exception;
import com.amazonaws.services.s3.model.BucketVersioningConfiguration;
import com.amazonaws.services.s3.model.SetBucketVersioningConfigurationRequest;

public class BucketVersioningConfigurationExample {
    public static String bucketName = "*** bucket name ***"; 
    public static AmazonS3Client s3Client;

    public static void main(String[] args) throws IOException {
        s3Client = new AmazonS3Client(new ProfileCredentialsProvider());
        s3Client.setRegion(Region.getRegion(Regions.US_EAST_1));
        try {

            // 1. Enable versioning on the bucket.
        	BucketVersioningConfiguration configuration = 
        			new BucketVersioningConfiguration().withStatus("Enabled");
            
			SetBucketVersioningConfigurationRequest setBucketVersioningConfigurationRequest = 
					new SetBucketVersioningConfigurationRequest(bucketName,configuration);
			
			s3Client.setBucketVersioningConfiguration(setBucketVersioningConfigurationRequest);
			
			// 2. Get bucket versioning configuration information.
			BucketVersioningConfiguration conf = s3Client.getBucketVersioningConfiguration(bucketName);
			 System.out.println("bucket versioning configuration status:    " + conf.getStatus());

        } catch (AmazonS3Exception amazonS3Exception) {
            System.out.format("An Amazon S3 error occurred. Exception: %s", amazonS3Exception.toString());
        } catch (Exception ex) {
            System.out.format("Exception: %s", ex.toString());
        }        
    }
}
```

## Using the AWS SDK for \.NET<a name="manage-versioning-examples-dotnet"></a>

For information about how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\. 

**Example**  

```
using System;
using Amazon.S3;
using Amazon.S3.Model;

namespace s3.amazon.com.docsamples
{
    class BucketVersioningConfiguration
    {
        static string bucketName = "*** bucket name ***";

        public static void Main(string[] args)
        {
            using (var client = new AmazonS3Client(Amazon.RegionEndpoint.USEast1))
            {
                try
                {
                    EnableVersioningOnBucket(client);
                    string bucketVersioningStatus = RetrieveBucketVersioningConfiguration(client);
                }
                catch (AmazonS3Exception amazonS3Exception)
                {
                    if (amazonS3Exception.ErrorCode != null &&
                        (amazonS3Exception.ErrorCode.Equals("InvalidAccessKeyId")
                        ||
                        amazonS3Exception.ErrorCode.Equals("InvalidSecurity")))
                    {
                        Console.WriteLine("Check the provided AWS Credentials.");
                        Console.WriteLine(
                        "To sign up for service, go to http://aws.amazon.com/s3");
                    }
                    else
                    {
                        Console.WriteLine(
                         "Error occurred. Message:'{0}' when listing objects",
                         amazonS3Exception.Message);
                    }
                }
            }

            Console.WriteLine("Press any key to continue...");
            Console.ReadKey();
        }

        static void EnableVersioningOnBucket(IAmazonS3 client)
        {

                PutBucketVersioningRequest request = new PutBucketVersioningRequest
                {
                    BucketName = bucketName,
                    VersioningConfig = new S3BucketVersioningConfig 
                    {
                        Status = VersionStatus.Enabled
                    }
                };

                PutBucketVersioningResponse response = client.PutBucketVersioning(request);
        }


        static string RetrieveBucketVersioningConfiguration(IAmazonS3 client)
        {
                GetBucketVersioningRequest request = new GetBucketVersioningRequest
                {
                    BucketName = bucketName
                };
 
                GetBucketVersioningResponse response = client.GetBucketVersioning(request);
                return response.VersioningConfig.Status;
            }
    }
}
```
## Using other AWS SDKs<a name="manage-versioning-examples-sdks"></a>

For information about using other AWS SDKs, see [Sample Code and Libraries](https://aws.amazon.com/code/)\. 

### 9. Connect your VPC to CodePipeline by defining an interface VPC endpoint for CodePipeline. 
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.AC-1|Identities and credentials are managed for authorized devices and users Identities and credentials are issued, managed, verified, revoked, and audited for authorized devices, users and processes|
|PR.AC-3|Remote access is managed|
|PR.AC-4|UAccess permissions are managed, incorporating the principles of least privilege and separation of duties|  
|PR.AC-6|Identities are proofed and bound to credentials and asserted in interactions|  
|PR.AC-7|Users, devices, and other assets are authenticated (e.g., single-factor, multi-factor) commensurate with the risk of the transaction (e.g., individuals’ security and privacy risks and other organizational risks)|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|5|Local AWS IAM accounts are restricted to services and no user accounts are to be provisioned including IaaS resources.|


**Why?** 
CodePipeline supports Amazon VPC endpoints powered by AWS PrivateLink, an AWS technology that facilitates private communication between AWS services using an elastic network interface with private IP addresses. This means you can connect directly to CodePipeline through a private endpoint in your VPC, keeping all traffic inside your VPC and the AWS network.

**How?** 
# Create a VPC endpoint service configuration<a name="create-endpoint-service"></a>

You can create an endpoint service configuration using the Amazon VPC console or the command line\. Before you begin, ensure that you have created one or more Network Load Balancers in your VPC for your service\. For more information, see [Getting Started with Network Load Balancers](https://docs.aws.amazon.com/elasticloadbalancing/latest/network/network-load-balancer-getting-started.html) in the *User Guide for Network Load Balancers*\.

In your configuration, you can optionally specify that any interface endpoint connection requests to your service must be manually accepted by you\. If you do not accept a connection, service consumers cannot access your service\.


After you create an endpoint service configuration, you must add permissions to enable service consumers to create interface endpoints to your service\.

------
#### [ Console ]

**To create an endpoint service using the console**

1. Open the Amazon VPC console at [https://console\.aws\.amazon\.com/vpc/](https://console.aws.amazon.com/vpc/)\.

1. In the navigation pane, choose **Endpoint Services**, **Create Endpoint Service**\.

1. For **Associate Network Load Balancers**, select the Network Load Balancers to associate with the endpoint service\. 

1. For **Require acceptance for endpoint**, select the check box to accept connection requests to your service manually\. If you do not select this option, endpoint connections are automatically accepted\.

1. To associate a private DNS name with the service, select **Enable private DNS** name, and then for** Private DNS name**, enter the private DNS name\.

1. \(Optional\) Add or remove a tag\.

   \[Add a tag\] Choose **Add tag** and do the following:
   + For **Key**, enter the key name\.
   + For **Value**, enter the key value\.

   \[Remove a tag\] Choose the delete button \(“x”\) to the right of the tag’s Key and Value\.

1. Choose **Create service**\.

------
#### [ AWS CLI ]

To create an endpoint service using the AWS CLI

Use the [create\-vpc\-endpoint\-service\-configuration](https://docs.aws.amazon.com/cli/latest/reference/ec2/create-vpc-endpoint-service-configuration.html) command and specify one or more ARNs for your Network Load Balancers\. You can optionally specify if acceptance is required for connecting to your service and if the service has a private DNS name\.

```
aws ec2 create-vpc-endpoint-service-configuration --network-load-balancer-arns arn:aws:elasticloadbalancing:us-east-1:123456789012:loadbalancer/net/nlb-vpce/e94221227f1ba532 --acceptance-required --privateDnsName exampleservice.com
```

```
{
    "ServiceConfiguration": {
        "ServiceType": [
            {
                "ServiceType": "Interface"
            }
        ], 
        "NetworkLoadBalancerArns": [
            "arn:aws:elasticloadbalancing:us-east-1:123456789012:loadbalancer/net/nlb-vpce/e94221227f1ba532"
        ], 
        "ServiceName": "com.amazonaws.vpce.us-east-1.vpce-svc-03d5ebb7d9579a2b3", 
        "ServiceState": "Available", 
        "ServiceId": "vpce-svc-03d5ebb7d9579a2b3", 
        "PrivateDnsName: "exampleService.com",
        "AcceptanceRequired": true, 
        "AvailabilityZones": [
            "us-east-1d"
        ], 
        "BaseEndpointDnsNames": [
            "vpce-svc-03d5ebb7d9579a2b3.us-east-1.vpce.amazonaws.com"
        ]
    }
}
```
------
#### [  AWS Tools for Windows PowerShell ]

Use [New\-EC2VpcEndpointServiceConfiguration](https://docs.aws.amazon.com/powershell/latest/reference/items/New-EC2VpcEndpointServiceConfiguration.html)\.

------
#### [ API ]

Use [CreateVpcEndpointServiceConfiguration](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-CreateVpcEndpointServiceConfiguration.html)\.

------

## Detective
### 1. Use file Integrity validation
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.CM-7|Monitoring for unauthorized personnel, connections, devices, and software is performed|
|DE.CM-6|External service provider activity is monitored to detect potential cybersecurity events|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|DE.AE-2|Detected events are analyzed to understand attack targets and methods|
|DE.AE-3|Event data are aggregated and correlated from multiple sources and sensors|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch|


**Why?** 
Use file integrity validation to evidence whether a log file was modified, deleted, or unchanged after CloudTrail delivered it. This feature is built using industry standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing. Validated log files are invaluable in security and forensic investigations\. For example, a validated log file enables you to assert positively that the log file itself has not changed, or that particular user credentials performed specific API activity\. The CloudTrail log file integrity validation process also lets you know if a log file has been deleted or changed, or assert positively that no log files were delivered to your account during a given period of time\.

**How?** 
# Validating CloudTrail Log File Integrity<a name="cloudtrail-log-file-validation-intro"></a>

 To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it, you can use CloudTrail log file integrity validation\. This feature is built using industry standard algorithms: SHA\-256 for hashing and SHA\-256 with RSA for digital signing\. This makes it computationally infeasible to modify, delete or forge CloudTrail log files without detection\. You can use the AWS CLI to validate the files in the location where CloudTrail delivered them\. 

## How It Works<a name="cloudtrail-log-file-validation-intro-how-it-works"></a>

When you enable log file integrity validation, CloudTrail creates a hash for every log file that it delivers\. Every hour, CloudTrail also creates and delivers a file that references the log files for the last hour and contains a hash of each\. This file is called a digest file\. CloudTrail signs each digest file using the private key of a public and private key pair\. After delivery, you can use the public key to validate the digest file\. CloudTrail uses different key pairs for each AWS region\. 

 The digest files are delivered to the same Amazon S3 bucket associated with your trail as your CloudTrail log files\. If your log files are delivered from all regions or from multiple accounts into a single Amazon S3 bucket, CloudTrail will deliver the digest files from those regions and accounts into the same bucket\. 

 The digest files are put into a folder separate from the log files\. This separation of digest files and log files enables you to enforce granular security policies and permits existing log processing solutions to continue to operate without modification\. Each digest file also contains the digital signature of the previous digest file if one exists\. The signature for the current digest file is in the metadata properties of the digest file Amazon S3 object\. For more information about digest file contents, see [CloudTrail Digest File Structure](cloudtrail-log-file-validation-digest-file-structure.md)\.

### Storing log and digest files<a name="cloudtrail-log-file-validation-intro-storage"></a>

 You can store the CloudTrail log files and digest files in Amazon S3 or S3 Glacier securely, durably and inexpensively for an indefinite period of time\. To enhance the security of the digest files stored in Amazon S3, you can use [Amazon S3 MFA Delete](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html)\. 

### Enabling Validation and Validating Files<a name="cloudtrail-log-file-validation-intro-enabling-and-using"></a>

To enable log file integrity validation, you can use the AWS Management Console, the AWS CLI, or CloudTrail API\. For more information, see [Enabling Log File Integrity Validation for CloudTrail](cloudtrail-log-file-validation-enabling.md)\. 

To validate the integrity of CloudTrail log files, you can use the AWS CLI or create your own solution\. The AWS CLI will validate files in the location where CloudTrail delivered them\. If you want to validate logs that you have moved to a different location, either in Amazon S3 or elsewhere, you can create your own validation tools\. 

# Validating CloudTrail Log File Integrity with the AWS CLI<a name="cloudtrail-log-file-validation-cli"></a>

To validate logs with the AWS Command Line Interface, use the CloudTrail `validate-logs` command\. The command uses the digest files delivered to your Amazon S3 bucket to perform the validation\. For information about digest files, see [CloudTrail Digest File Structure](cloudtrail-log-file-validation-digest-file-structure.md)\. 

The AWS CLI allows you to detect the following types of changes:
+ Modification or deletion of CloudTrail log files
+ Modification or deletion of CloudTrail digest files
+ Modification or deletion of both of the above

**Note**  
The AWS CLI validates only log files that are referenced by digest files\. For more information, see [Checking Whether a Particular File was Delivered by CloudTrail ](#cloudtrail-log-file-validation-cli-validate-logs-check-file)\.

## Prerequisites<a name="cloudtrail-log-file-validation-cli-prerequisites"></a>

To validate log file integrity with the AWS CLI, the following conditions must be met:
+ You must have online connectivity to AWS\.
+ You must have read access to the Amazon S3 bucket that contains the digest and log files\. 
+ The digest and log files must not have been moved from the original Amazon S3 location where CloudTrail delivered them\.

**Note**  
Log files that have been downloaded to local disk cannot be validated with the AWS CLI\. For guidance on creating your own tools for validation, see [Custom Implementations of CloudTrail Log File Integrity Validation ](cloudtrail-log-file-custom-validation.md)\.

## validate\-logs<a name="cloudtrail-log-file-validation-cli-validate-logs"></a>

### Syntax<a name="cloudtrail-log-file-validation-cli-validate-logs-syntax"></a>

The following is the syntax for `validate-logs`\. Optional parameters are shown in brackets\.

`aws cloudtrail validate-logs --trail-arn <trailARN> --start-time <start-time> [--end-time <end-time>] [--s3-bucket <bucket-name>] [--s3-prefix <prefix>] [--verbose]` 

### Options<a name="cloudtrail-log-file-validation-cli-validate-logs-options"></a>

The following are the command\-line options for `validate-logs`\. The `--trail-arn` and `--start-time` options are required\. 

`--start-time`  
Specifies that log files delivered on or after the specified UTC timestamp value will be validated\. Example: `2015-01-08T05:21:42Z`\. 

`--end-time`  
Optionally specifies that log files delivered on or before the specified UTC timestamp value will be validated\. The default value is the current UTC time \(`Date.now()`\)\. Example: `2015-01-08T12:31:41Z`\.   
For the time range specified, the `validate-logs` command checks only the log files that are referenced in their corresponding digest files\. No other log files in the Amazon S3 bucket are checked\. For more information, see [Checking Whether a Particular File was Delivered by CloudTrail ](#cloudtrail-log-file-validation-cli-validate-logs-check-file)\. 

`--s3-bucket`  
Optionally specifies the Amazon S3 bucket where the digest files are stored\. If a bucket name is not specified, the AWS CLI will retrieve it by calling `DescribeTrails()`\. 

`--prefix`  
Optionally specifies the Amazon S3 prefix where the digest files are stored\. If not specified, the AWS CLI will retrieve it by calling `DescribeTrails()`\.   
You should use this option only if your current prefix is different from the prefix that was in use during the time range that you specify\.

`--trail-arn`  
Specifies the Amazon Resource Name \(ARN\) of the trail to be validated\. The format of a trail ARN follows\.  

```
arn:aws:cloudtrail:us-east-2:111111111111:trail/MyTrailName
```
To obtain the trail ARN for a trail, you can use the `describe-trails` command before running `validate-logs`\.  
You may want to specify the bucket name and prefix in addition to the trail ARN if log files have been delivered to more than one bucket in the time range that you specified, and you want to restrict the validation to the log files in only one of the buckets\. 

`--verbose`  
Optionally outputs validation information for every log or digest file in the specified time range\. The output indicates whether the file remains unchanged or has been modified or deleted\. In non\-verbose mode \(the default\), information is returned only for those cases in which there was a validation failure\. 

### Example<a name="cloudtrail-log-file-validation-cli-validate-logs-example"></a>

The following example validates log files from the specified start time to the present, using the Amazon S3 bucket configured for the current trail and specifying verbose output\.

```
aws cloudtrail validate-logs --start-time 2015-08-27T00:00:00Z --end-time 2015-08-28T00:00:00Z --trail-arn arn:aws:cloudtrail:us-east-2:111111111111:trail/my-trail-name --verbose
```

### How `validate-logs` Works<a name="cloudtrail-log-file-validation-cli-validate-logs-how-it-works"></a>

 The `validate-logs` command starts by validating the most recent digest file in the specified time range\. First, it verifies that the digest file has been downloaded from the location to which it claims to belong\. In other words, if the CLI downloads digest file `df1` from the S3 location `p1`, validate\-logs will verify that `p1 == df1.digestS3Bucket + '/' + df1.digestS3Object`\.

 If the signature of the digest file is valid, it checks the hash value of each of the logs referenced in the digest file\. The command then goes back in time, validating the previous digest files and their referenced log files in succession\. It continues until the specified value for `start-time` is reached, or until the digest chain ends\. If a digest file is missing or not valid, the time range that cannot be validated is indicated in the output\. 

## Validation Results<a name="cloudtrail-log-file-validation-cli-results"></a>

Validation results begin with a summary header in the following format:

```
Validating log files for trail trail_ARN  between time_stamp and time_stamp
```

Each line of the main output contains the validation results for a single digest or log file in the following format:

```
<Digest file | Log file> <S3 path> <Validation Message>
```

The following table describes the possible validation messages for log and digest files\.


****  

| File Type | Validation Message | Description | 
| --- | --- | --- | 
| Digest file | valid | The digest file signature is valid\. The log files it references can be checked\. This message is included only in verbose mode\. | 
| Digest file | INVALID: has been moved from its original location | The S3 bucket or S3 object from which the digest file was retrieved do not match the S3 bucket or S3 object locations that are recorded in the digest file itself\. | 
| Digest file | INVALID: invalid format | The format of the digest file is invalid\. The log files corresponding to the time range that the digest file represents cannot be validated\. | 
| Digest file | INVALID: not found | The digest file was not found\. The log files corresponding to the time range that the digest file represents cannot be validated\. | 
| Digest file | INVALID: public key not found for fingerprint fingerprint | The public key corresponding to the fingerprint recorded in the digest file was not found\. The digest file cannot be validated\. | 
| Digest file | INVALID: signature verification failed | The digest file signature is not valid\. Because the digest file is not valid, the log files it references cannot be validated, and no assertions can be made about the API activity in them\. | 
| Digest file | INVALID: Unable to load PKCS \#1 key with fingerprint fingerprint | Because the DER encoded public key in PKCS \#1 format having the specified fingerprint could not be loaded, the digest file cannot be validated\. | 
| Log file | valid | The log file has been validated and has not been modified since the time of delivery\. This message is included only in verbose mode\. | 
| Log file | INVALID: hash value doesn't match | The hash for the log file does not match\. The log file has been modified after delivery by CloudTrail\. | 
| Log file | INVALID: invalid format | The format of the log file is invalid\. The log file cannot be validated\. | 
| Log file | INVALID: not found | The log file was not found and cannot be validated\. | 

The output includes summary information about the results returned\.

## Example Outputs<a name="cloudtrail-log-file-validation-cli-results-examples"></a>

### Verbose<a name="cloudtrail-log-file-validation-cli-results-verbose"></a>

The following example `validate-logs` command uses the `--verbose` flag and produces the sample output that follows\. `[...]` indicates the sample output has been abbreviated\.

```
aws cloudtrail validate-logs --trail-arn arn:aws:cloudtrail:us-east-2:111111111111:trail/example-trail-name --start-time 2015-08-31T22:00:00Z --end-time 2015-09-01T19:17:29Z --verbose
```

```
Validating log files for trail arn:aws:cloudtrail:us-east-2:111111111111:trail/example-trail-name between 2015-08-31T22:00:00Z and 2015-09-01T19:17:29Z
                                       
Digest file    s3://example-bucket/AWSLogs/111111111111/CloudTrail-Digest/us-east-2/2015/09/01/111111111111_CloudTrail-Digest_us-east-2_example-trail-name_us-east-2_20150901T201728Z.json.gz	valid
Log file       s3://example-bucket/AWSLogs/111111111111/CloudTrail/us-east-2/2015/09/01/111111111111_CloudTrail_us-east-2_20150901T1925Z_WZZw1RymnjCRjxXc.json.gz	valid
Log file       s3://example-bucket/AWSLogs/111111111111/CloudTrail/us-east-2/2015/09/01/111111111111_CloudTrail_us-east-2_20150901T1915Z_POuvV87nu6pfAV2W.json.gz	valid
Log file       s3://example-bucket/AWSLogs/111111111111/CloudTrail/us-east-2/2015/09/01/111111111111_CloudTrail_us-east-2_20150901T1930Z_l2QgXhAKVm1QXiIA.json.gz	valid
Log file       s3://example-bucket/AWSLogs/111111111111/CloudTrail/us-east-2/2015/09/01/111111111111_CloudTrail_us-east-2_20150901T1920Z_eQJteBBrfpBCqOqw.json.gz	valid
Log file       s3://example-bucket/AWSLogs/111111111111/CloudTrail/us-east-2/2015/09/01/111111111111_CloudTrail_us-east-2_20150901T1950Z_9g5A6qlR2B5KaRdq.json.gz	valid
Log file       s3://example-bucket/AWSLogs/111111111111/CloudTrail/us-east-2/2015/09/01/111111111111_CloudTrail_us-east-2_20150901T1920Z_i4DNCC12BuXd6Ru7.json.gz	valid
Log file       s3://example-bucket/AWSLogs/111111111111/CloudTrail/us-east-2/2015/09/01/111111111111_CloudTrail_us-east-2_20150901T1915Z_Sg5caf2RH6Jdx0EJ.json.gz	valid
Digest file    s3://example-bucket/AWSLogs/111111111111/CloudTrail-Digest/us-east-2/2015/09/01/111111111111_CloudTrail-Digest_us-east-2_example-trail-name_us-east-2_20150901T191728Z.json.gz	valid
Log file       s3://example-bucket/AWSLogs/111111111111/CloudTrail/us-east-2/2015/09/01/111111111111_CloudTrail_us-east-2_20150901T1910Z_YYSFiuFQk4nrtnEW.json.gz	valid
[...]
Log file       s3://example-bucket/AWSLogs/144218288521/CloudTrail/us-east-2/2015/09/01/144218288521_CloudTrail_us-east-2_20150901T1055Z_0Sfy6m9f6iBzmoPF.json.gz	valid
Log file       s3://example-bucket/AWSLogs/144218288521/CloudTrail/us-east-2/2015/09/01/144218288521_CloudTrail_us-east-2_20150901T1040Z_lLa3QzVLpOed7igR.json.gz	valid

Digest file    s3://example-bucket/AWSLogs/144218288521/CloudTrail-Digest/us-east-2/2015/09/01/144218288521_CloudTrail-Digest_us-east-2_example-trail-name_us-east-2_20150901T101728Z.json.gz	INVALID: signature verification failed

Digest file    s3://example-bucket/AWSLogs/144218288521/CloudTrail-Digest/us-east-2/2015/09/01/144218288521_CloudTrail-Digest_us-east-2_example-trail-name_us-east-2_20150901T091728Z.json.gz	valid
Log file       s3://example-bucket/AWSLogs/144218288521/CloudTrail/us-east-2/2015/09/01/144218288521_CloudTrail_us-east-2_20150901T0830Z_eaFvO3dwHo4NCqqc.json.gz	valid
Digest file    s3://example-bucket/AWSLogs/144218288521/CloudTrail-Digest/us-east-2/2015/09/01/144218288521_CloudTrail-Digest_us-east-2_example-trail-name_us-east-2_20150901T081728Z.json.gz	valid
Digest file    s3://example-bucket/AWSLogs/144218288521/CloudTrail-Digest/us-east-2/2015/09/01/144218288521_CloudTrail-Digest_us-east-2_example-trail-name_us-east-2_20150901T071728Z.json.gz	valid
[...]
Log file       s3://example-bucket/AWSLogs/111111111111/CloudTrail/us-east-2/2015/08/31/111111111111_CloudTrail_us-east-2_20150831T2245Z_mbJkEO5kNcDnVhGh.json.gz	valid
Log file       s3://example-bucket/AWSLogs/111111111111/CloudTrail/us-east-2/2015/08/31/111111111111_CloudTrail_us-east-2_20150831T2225Z_IQ6kXy8sKU03RSPr.json.gz	valid
Log file       s3://example-bucket/AWSLogs/111111111111/CloudTrail/us-east-2/2015/08/31/111111111111_CloudTrail_us-east-2_20150831T2230Z_eRPVRTxHQ5498ROA.json.gz	valid
Log file       s3://example-bucket/AWSLogs/111111111111/CloudTrail/us-east-2/2015/08/31/111111111111_CloudTrail_us-east-2_20150831T2255Z_IlWawYZGvTWB5vYN.json.gz	valid
Digest file    s3://example-bucket/AWSLogs/111111111111/CloudTrail-Digest/us-east-2/2015/08/31/111111111111_CloudTrail-Digest_us-east-2_example-trail-name_us-east-2_20150831T221728Z.json.gz	valid

Results requested for 2015-08-31T22:00:00Z to 2015-09-01T19:17:29Z
Results found for 2015-08-31T22:17:28Z to 2015-09-01T20:17:28Z:

22/23 digest files valid, 1/23 digest files INVALID
63/63 log files valid
```

### Non\-verbose<a name="cloudtrail-log-file-validation-cli-results-non-verbose"></a>

The following example `validate-logs` command does not use the `--verbose` flag\. In the sample output that follows, one error was found\. Only the header, error, and summary information are returned\.

```
aws cloudtrail validate-logs --trail-arn arn:aws:cloudtrail:us-east-2:111111111111:trail/example-trail-name --start-time 2015-08-31T22:00:00Z --end-time 2015-09-01T19:17:29Z
```

```
Validating log files for trail arn:aws:cloudtrail:us-east-2:111111111111:trail/example-trail-name between 2015-08-31T22:00:00Z and 2015-09-01T19:17:29Z

Digest file	s3://example-bucket/AWSLogs/144218288521/CloudTrail-Digest/us-east-2/2015/09/01/144218288521_CloudTrail-Digest_us-east-2_example-trail-name_us-east-2_20150901T101728Z.json.gz	INVALID: signature verification failed

Results requested for 2015-08-31T22:00:00Z to 2015-09-01T19:17:29Z
Results found for 2015-08-31T22:17:28Z to 2015-09-01T20:17:28Z:

22/23 digest files valid, 1/23 digest files INVALID
63/63 log files valid
```

## Checking Whether a Particular File was Delivered by CloudTrail<a name="cloudtrail-log-file-validation-cli-validate-logs-check-file"></a>

To check if a particular file in your bucket was delivered by CloudTrail, run `validate-logs` in verbose mode for the time period that includes the file\. If the file appears in the output of `validate-logs`, then the file was delivered by CloudTrail\.

# Custom Implementations of CloudTrail Log File Integrity Validation<a name="cloudtrail-log-file-custom-validation"></a>

Because CloudTrail uses industry standard, openly available cryptographic algorithms and hash functions, you can create your own tools to validate the integrity of CloudTrail log files\. When log file integrity validation is enabled, CloudTrail delivers digest files to your Amazon S3 bucket\. You can use these files to implement your own validation solution\. For more information about digest files, see [CloudTrail Digest File Structure](cloudtrail-log-file-validation-digest-file-structure.md)\. 

This topic describes how digest files are signed, and then details the steps that you will need to take to implement a solution that validates the digest files and the log files that they reference\. 

## Understanding How CloudTrail Digest Files are Signed<a name="cloudtrail-log-file-custom-validation-how-cloudtrail-digest-files-are-signed"></a>

 CloudTrail digest files are signed with RSA digital signatures\. For each digest file, CloudTrail does the following: 

1. Creates a string for data signing based on designated digest file fields \(described in the next section\)\. 

1. Gets a private key unique to the region\.

1. Passes the SHA\-256 hash of the string and the private key to the RSA signing algorithm, which produces a digital signature\.

1. Encodes the byte code of the signature into hexadecimal format\.

1. Puts the digital signature into the `x-amz-meta-signature` metadata property of the Amazon S3 digest file object\.

### Contents of the Data Signing String<a name="cloudtrail-log-file-custom-validation-data-signing-string-summary"></a>

 The following CloudTrail objects are included in the string for data signing: 
+ The ending timestamp of the digest file in UTC extended format \(for example, `2015-05-08T07:19:37Z`\)
+ The current digest file S3 path
+ The hexadecimal\-encoded SHA\-256 hash of the current digest file
+ The hexadecimal\-encoded signature of the previous digest file

The format for calculating this string and an example string are provided later in this document\.

## Custom Validation Implementation Steps<a name="cloudtrail-log-file-custom-validation-steps"></a>

When implementing a custom validation solution, you will need to validate the digest file first, and then the log files that it references\. 

### Validate the Digest File<a name="cloudtrail-log-file-custom-validation-steps-digest"></a>

 To validate a digest file, you need its signature, the public key whose private key was used to signed it, and a data signing string that you compute\. 

1. Get the digest file\.

1.  Verify that the digest file has been retrieved from its original location\. 

1. Get the hexadecimal\-encoded signature of the digest file\.

1. Get the hexadecimal\-encoded fingerprint of the public key whose private key was used to sign the digest file\.

1. Retrieve the public keys for the time range corresponding to the digest file\.

1. From among the public keys retrieved, choose the public key whose fingerprint matches the fingerprint in the digest file\.

1. Using the digest file hash and other digest file fields, recreate the data signing string used to verify the digest file signature\.

1. Validate the signature by passing in the SHA\-256 hash of the string, the public key, and the signature as parameters to the RSA signature verification algorithm\. If the result is true, the digest file is valid\. 

### Validate the Log Files<a name="cloudtrail-log-file-custom-validation-steps-logs"></a>

If the digest file is valid, validate each of the log files that the digest file references\.

1. To validate the integrity of a log file, compute its SHA\-256 hash value on its uncompressed content and compare the results with the hash for the log file recorded in hexadecimal in the digest\. If the hashes match, the log file is valid\.

1. By using the information about the previous digest file that is included in the current digest file, validate the previous digest files and their corresponding log files in succession\.

The following sections describe these steps in detail\.

### A\. Get the Digest File<a name="cloudtrail-log-file-custom-validation-steps-get-the-digest-file"></a>

The first steps are to get the most recent digest file, verify that you have retrieved it from its original location, verify its digital signature, and get the fingerprint of the public key\.

1. Using [https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html) or the [AmazonS3Client class](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Client.html) \(for example\), get the most recent digest file from your Amazon S3 bucket for the time range that you want to validate\. 

1.  Check that the S3 bucket and S3 object used to retrieve the file match the S3 bucket S3 object locations that are recorded in the digest file itself\. 

1. Next, get the digital signature of the digest file from the `x-amz-meta-signature` metadata property of the digest file object in Amazon S3\.

1. In the digest file, get the fingerprint of the public key whose private key was used to sign the digest file from the `digestPublicKeyFingerprint` field\. 

### B\. Retrieve the Public Key for Validating the Digest File<a name="cloudtrail-log-file-custom-validation-steps-retrieve-public-key"></a>

To get the public key to validate the digest file, you can use either the AWS CLI or the CloudTrail API\. In both cases, you specify a time range \(that is, a start time and end time\) for the digest files that you want to validate\. One or more public keys may be returned for the time range that you specify\. The returned keys may have validity time ranges that overlap\.

**Note**  
Because CloudTrail uses different private/public key pairs per region, each digest file is signed with a private key unique to its region\. Therefore, when you validate a digest file from a particular region, you must retrieve its public key from the same region\.

#### Use the AWS CLI to Retrieve Public Keys<a name="cloudtrail-log-file-custom-validation-steps-retrieve-public-key-cli"></a>

To retrieve public keys for digest files by using the AWS CLI, use the `cloudtrail list-public-keys` command\. The command has the following format: 

 `aws cloudtrail list-public-keys [--start-time <start-time>] [--end-time <end-time>]` 

The start\-time and end\-time parameters are UTC timestamps and are optional\. If not specified, the current time is used, and the currently active public key or keys are returned\.

 **Sample Response** 

The response will be a list of JSON objects representing the key \(or keys\) returned: 

```
{
    "publicKeyList": [
        {
            "ValidityStartTime": "1436317441.0",
            "ValidityEndTime": "1438909441.0",
            "Value": "MIIBCgKCAQEAn11L2YZ9h7onug2ILi1MWyHiMRsTQjfWE+pHVRLk1QjfWhirG+lpOa8NrwQ/r7Ah5bNL6HepznOU9XTDSfmmnP97mqyc7z/upfZdS/AHhYcGaz7n6Wc/RRBU6VmiPCrAUojuSk6/GjvA8iOPFsYDuBtviXarvuLPlrT9kAd4Lb+rFfR5peEgBEkhlzc5HuWO7S0y+KunqxX6jQBnXGMtxmPBPP0FylgWGNdFtks/4YSKcgqwH0YDcawP9GGGDAeCIqPWIXDLG1jOjRRzWfCmD0iJUkz8vTsn4hq/5ZxRFE7UBAUiVcGbdnDdvVfhF9C3dQiDq3k7adQIziLT0cShgQIDAQAB",
            "Fingerprint": "8eba5db5bea9b640d1c96a77256fe7f2"
        },
        {
            "ValidityStartTime": "1434589460.0",
            "ValidityEndTime": "1437181460.0",
            "Value": "MIIBCgKCAQEApfYL2FiZhpN74LNWVUzhR+VheYhwhYm8w0n5Gf6i95ylW5kBAWKVEmnAQG7BvS5g9SMqFDQx52fW7NWV44IvfJ2xGXT+wT+DgR6ZQ+6yxskQNqV5YcXj4Aa5Zz4jJfsYjDuO2MDTZNIzNvBNzaBJ+r2WIWAJ/Xq54kyF63B6WE38vKuDE7nSd1FqQuEoNBFLPInvgggYe2Ym1Refe2z71wNcJ2kY+q0h1BSHrSM8RWuJIw7MXwF9iQncg9jYzUlNJomozQzAG5wSRfbplcCYNY40xvGd/aAmO0m+Y+XFMrKwtLCwseHPvj843qVno6x4BJN9bpWnoPo9sdsbGoiK3QIDAQAB",
            "Fingerprint": "8933b39ddc64d26d8e14ffbf6566fee4"
        },
        {
            "ValidityStartTime": "1434589370.0",
            "ValidityEndTime": "1437181370.0",
            "Value": "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAqlzPJbvZJ42UdcmLfPUqXYNfOs6I8lCfao/tOs8CmzPOEdtLWugB9xoIUz78qVHdKIqxbaG4jWHfJBiOSSFBM0lt8cdVo4TnRa7oG9io5pysS6DJhBBAeXsicufsiFJR+wrUNh8RSLxL4k6G1+BhLX20tJkZ/erT97tDGBujAelqseGg3vPZbTx9SMfOLN65PdLFudLP7Gat0Z9p5jw/rjpclKfo9Bfc3heeBxWGKwBBOKnFAaN9V57pOaosCvPKmHd9bg7jsQkI9Xp22IzGLsTFJZYVA3KiTAElDMu80iFXPHEq9hKNbt9e4URFam+1utKVEiLkR2disdCmPTK0VQIDAQAB",
            "Fingerprint": "31e8b5433410dfb61a9dc45cc65b22ff"
        }
    ]
}
```

#### Use the CloudTrail API to Retrieve Public Keys<a name="cloudtrail-log-file-custom-validation-steps-retrieve-public-key-api"></a>

To retrieve public keys for digest files by using the CloudTrail API, pass in start time and end time values to the `ListPublicKeys` API\. The `ListPublicKeys` API returns the public keys whose private keys were used to sign digest files within the specified time range\. For each public key, the API also returns the corresponding fingerprint\.

##### `ListPublicKeys`<a name="cloudtrail-log-file-custom-validation-steps-list-public-keys"></a>

This section describes the request parameters and response elements for the `ListPublicKeys` API\.

**Note**  
 The encoding for the binary fields for `ListPublicKeys` is subject to change\. 

 **Request Parameters** 


****  

| Name | Description | 
| --- | --- | 
|  StartTime  |  Optionally specifies, in UTC, the start of the time range to look up public keys for CloudTrail digest files\. If StartTime is not specified, the current time is used, and the current public key is returned\.  Type: DateTime   | 
|  EndTime  |  Optionally specifies, in UTC, the end of the time range to look up public keys for CloudTrail digest files\. If EndTime is not specified, the current time is used\.  Type: DateTime   | 

 **Response Elements** 

`PublicKeyList`, an array of `PublicKey` objects that contains: 


****  

|  |  | 
| --- |--- |
|  Name  |  Description  | 
|  Value  |  The DER encoded public key value in PKCS \#1 format\.  Type: Blob   | 
|  ValidityStartTime  |  The starting time of validity of the public key\. Type: DateTime   | 
|  ValidityEndTime  |  The ending time of validity of the public key\. Type: DateTime   | 
|  Fingerprint  |  The fingerprint of the public key\. The fingerprint can be used to identify the public key that you must use to validate the digest file\. Type: String   | 

### C\. Choose the Public Key to Use for Validation<a name="cloudtrail-log-file-custom-validation-steps-choose-public-key"></a>

 From among the public keys retrieved by `list-public-keys` or `ListPublicKeys`, choose the public key returned whose fingerprint matches the fingerprint recorded in the `digestPublicKeyFingerprint` field of the digest file\. This is the public key that you will use to validate the digest file\. 

### D\. Recreate the Data Signing String<a name="cloudtrail-log-file-custom-validation-steps-recreate-data-signing-string"></a>

Now that you have the signature of the digest file and associated public key, you need to calculate the data signing string\. After you have calculated the data signing string, you will have the inputs needed to verify the signature\.

 The data signing string has the following format: 

```
Data_To_Sign_String = 
  Digest_End_Timestamp_in_UTC_Extended_format + '\n' +
  Current_Digest_File_S3_Path + '\n' +
  Hex(Sha256(current-digest-file-content)) + '\n' +
  Previous_digest_signature_in_hex
```

An example `Data_To_Sign_String` follows\.

```
2015-08-12T04:01:31Z
S3-bucket-name/AWSLogs/111122223333/CloudTrail-Digest/us-east-2/2015/08/12/111122223333_us-east-2_CloudTrail-Digest_us-east-2_20150812T040131Z.json.gz
4ff08d7c6ecd6eb313257e839645d20363ee3784a2328a7d76b99b53cc9bcacd
6e8540b83c3ac86a0312d971a225361d28ed0af20d70c211a2d405e32abf529a8145c2966e3bb47362383a52441545ed091fb81
d4c7c09dd152b84e79099ce7a9ec35d2b264eb92eb6e090f1e5ec5d40ec8a0729c02ff57f9e30d5343a8591638f8b794972ce15bb3063a01972
98b0aee2c1c8af74ec620261529265e83a9834ebef6054979d3e9a6767dfa6fdb4ae153436c567d6ae208f988047ccfc8e5e41f7d0121e54ed66b1b904f80fb2ce304458a2a6b91685b699434b946c52589e9438f8ebe5a0d80522b2f043b3710b87d2cda43e5c1e0db921d8d540b9ad5f6d4$31b1f4a8ef2d758424329583897339493a082bb36e782143ee5464b4e3eb4ef6
```

After you recreate this string, you can validate the digest file\.

### E\. Validate the Digest File<a name="cloudtrail-log-file-custom-validation-steps-validate-digest-file"></a>

 Pass the SHA\-256 hash of the recreated data signing string, digital signature, and public key to the RSA signature verification algorithm\. If the output is true, the signature of the digest file is verified and the digest file is valid\. 

### F\. Validate the Log Files<a name="cloudtrail-log-file-custom-validation-steps-validate-log-files"></a>

 After you have validated the digest file, you can validate the log files it references\. The digest file contains the SHA\-256 hashes of the log files\. If one of the log files was modified after CloudTrail delivered it, the SHA\-256 hashes will change, and the signature of digest file will not match\. 

The following shows how validate the log files:

1. Do an `S3 Get` of the log file using the S3 location information in the digest file's `logFiles.s3Bucket` and `logFiles.s3Object` fields\.

1. If the `S3 Get` operation is successful, iterate through the log files listed in the digest file's logFiles array using the following steps:

   1. Retrieve the original hash of the file from the `logFiles.hashValue` field of the corresponding log in the digest file\.

   1. Hash the uncompressed contents of the log file with the hashing algorithm specified in `logFiles.hashAlgorithm`\.

   1. Compare the hash value that you generated with the one for the log in the digest file\. If the hashes match, the log file is valid\.

### G\. Validate Additional Digest and Log Files<a name="cloudtrail-log-file-custom-validation-steps-validate-additional-files"></a>

In each digest file, the following fields provide the location and signature of the previous digest file:
+  `previousDigestS3Bucket` 
+  `previousDigestS3Object` 
+  `previousDigestSignature` 

Use this information to visit previous digest files sequentially, validating the signature of each and the log files that they reference by using the steps in the previous sections\. The only difference is that for previous digest files, you do not need to retrieve the digital signature from the digest file object's Amazon S3 metadata properties\. The signature for the previous digest file is provided for you in the `previousDigestSignature` field\. 

You can go back until the starting digest file is reached, or until the chain of digest files is broken, whichever comes first\. 

## Validating Digest and Log Files Offline<a name="cloudtrail-log-file-custom-validation-offline"></a>

When validating digest and log files offline, you can generally follow the procedures described in the previous sections\. However, you must take into account the following areas:

### Handling the Most Recent Digest File<a name="cloudtrail-log-file-custom-validation-offline-most-recent-digest"></a>

The digital signature of the most recent \(that is, "current"\) digest file is in the Amazon S3 metadata properties of the digest file object\. In an offline scenario, the digital signature for the current digest file will not be available\.

Two possible ways of handling this are:
+ Since the digital signature for the previous digest file is in the current digest file, start validating from the next\-to\-last digest file\. With this method, the most recent digest file cannot be validated\.
+ As a preliminary step, obtain the signature for the current digest file from the digest file object's metadata properties \(for example, by calling the Amazon S3 [getObjectMetadata](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3.html#getObjectMetadata(com.amazonaws.services.s3.model.GetObjectMetadataRequest)) API\) and then store it securely offline\. This would allow the current digest file to be validated in addition to the previous files in the chain\.

### Path Resolution<a name="cloudtrail-log-file-custom-validation-offline-path-resolution"></a>

Fields in the downloaded digest files like `s3Object` and `previousDigestS3Object` will still be pointing to Amazon S3 online locations for log files and digest files\. An offline solution must find a way to reroute these to the current path of the downloaded log and digest files\.

### Public Keys<a name="cloudtrail-log-file-custom-validation-offline-public-keys"></a>

In order to validate offline, all of the public keys that you need for validating log files in a given time range must first be obtained online \(by calling `ListPublicKeys`, for example\) and then stored securely offline\. This step must be repeated whenever you want to validate additional files outside the initial time range that you specified\.

## Sample Validation Snippet<a name="cloudtrail-log-file-custom-validation-sample-code"></a>

 The following sample snippet provides skeleton code for validating CloudTrail digest and log files\. The skeleton code is online/offline agnostic; that is, it is up to you to decide whether to implement it with or without online connectivity to AWS\. The suggested implementation uses the [Java Cryptography Extension \(JCE\)](https://en.wikipedia.org/wiki/Java_Cryptography_Extension) and [Bouncy Castle](http://www.bouncycastle.org/) as a security provider\. 

The sample snippet shows:
+ How to create the data signing string used to validate the digest file signature\. 
+ How to verify the digest file signature\.
+ How to verify the log file hashes\.
+ A code structure for validating a chain of digest files\.

```
import java.util.Arrays;
import java.security.MessageDigest;
import java.security.KeyFactory;
import java.security.PublicKey;
import java.security.Security;
import java.security.Signature;
import java.security.spec.X509EncodedKeySpec;
import org.json.JSONObject;
import org.bouncycastle.jce.provider.BouncyCastleProvider;
import org.apache.commons.codec.binary.Hex;
 
public class DigestFileValidator {
 
    public void validateDigestFile(String digestS3Bucket, String digestS3Object, String digestSignature) {
 
        // Using the Bouncy Castle provider as a JCE security provider - http://www.bouncycastle.org/
        Security.addProvider(new BouncyCastleProvider());
 
        // Load the digest file from S3 (using Amazon S3 Client) or from your local copy
        JSONObject digestFile = loadDigestFileInMemory(digestS3Bucket, digestS3Object);
 
        // Check that the digest file has been retrieved from its original location
        if (!digestFile.getString("digestS3Bucket").equals(digestS3Bucket) ||
                !digestFile.getString("digestS3Object").equals(digestS3Object)) {
            System.err.println("Digest file has been moved from its original location.");
        } else {
            // Compute digest file hash
            MessageDigest messageDigest = MessageDigest.getInstance("SHA-256");
            messageDigest.update(convertToByteArray(digestFile));
            byte[] digestFileHash = messageDigest.digest();
            messageDigest.reset();
 
            // Compute the data to sign
            String dataToSign = String.format("%s%n%s/%s%n%s%n%s",
                                digestFile.getString("digestEndTime"),
                                digestFile.getString("digestS3Bucket"), digestFile.getString("digestS3Object"), // Constructing the S3 path of the digest file as part of the data to sign
                                Hex.encodeHexString(digestFileHash),
                                digestFile.getString("previousDigestSignature"));
 
            byte[] signatureContent = Hex.decodeHex(digestSignature);
 
            /*
                NOTE: 
                To find the right public key to verify the signature, call CloudTrail ListPublicKey API to get a list 
                of public keys, then match by the publicKeyFingerprint in the digest file. Also, the public key bytes 
                returned from ListPublicKey API are DER encoded in PKCS#1 format:
 
                PublicKeyInfo ::= SEQUENCE {
                    algorithm       AlgorithmIdentifier,
                    PublicKey       BIT STRING
                }
 
                AlgorithmIdentifier ::= SEQUENCE {
                    algorithm       OBJECT IDENTIFIER,
                    parameters      ANY DEFINED BY algorithm OPTIONAL
                }                
            */
            pkcs1PublicKeyBytes = getPublicKey(digestFile.getString("digestPublicKeyFingerprint")));
 
            // Transform the PKCS#1 formatted public key to x.509 format.
            RSAPublicKey rsaPublicKey = RSAPublicKey.getInstance(pkcs1PublicKeyBytes);
            AlgorithmIdentifier rsaEncryption = new AlgorithmIdentifier(PKCSObjectIdentifiers.rsaEncryption, null);
            SubjectPublicKeyInfo publicKeyInfo = new SubjectPublicKeyInfo(rsaEncryption, rsaPublicKey);
 
            // Create the PublicKey object needed for the signature validation
            PublicKey publicKey = KeyFactory.getInstance("RSA", "BC").generatePublic(new X509EncodedKeySpec(publicKeyInfo.getEncoded()));
 
            // Verify signature
            Signature signature = Signature.getInstance("SHA256withRSA", "BC");
            signature.initVerify(publicKey);
            signature.update(dataToSign.getBytes("UTF-8"));
 
            if (signature.verify(signatureContent)) {
                System.out.println("Digest file signature is valid, validating log files…");
                for (int i = 0; i < digestFile.getJSONArray("logFiles").length(); i++) {
 
                    JSONObject logFileMetadata = digestFile.getJSONArray("logFiles").getJSONObject(i);
 
                    // Compute log file hash
                    byte[] logFileContent = loadUncompressedLogFileInMemory(
                                                logFileMetadata.getString("s3Bucket"),
                                                logFileMetadata.getString("s3Object")
                                            );
                    messageDigest.update(logFileContent);
                     byte[] logFileHash = messageDigest.digest();
                    messageDigest.reset();
 
                    // Retrieve expected hash for the log file being processed
                    byte[] expectedHash = Hex.decodeHex(logFileMetadata.getString("hashValue"));
 
                    boolean signaturesMatch = Arrays.equals(expectedHash, logFileHash);
                    if (!signaturesMatch) {
                        System.err.println(String.format("Log file: %s/%s hash doesn't match.\tExpected: %s Actual: %s",
                               logFileMetadata.getString("s3Bucket"), logFileMetadata.getString("s3Object"),
                               Hex.encodeHexString(expectedHash), Hex.encodeHexString(logFileHash)));
                    } else {
                        System.out.println(String.format("Log file: %s/%s hash match",
                               logFileMetadata.getString("s3Bucket"), logFileMetadata.getString("s3Object")));
                    }
                }
 
            } else {
                System.err.println("Digest signature failed validation.");
            }
 
            System.out.println("Digest file validation completed.");
 
            if (chainValidationIsEnabled()) {
                // This enables the digests' chain validation
                validateDigestFile(
                        digestFile.getString("previousDigestS3Bucket"),
                        digestFile.getString("previousDigestS3Object"),
                        digestFile.getString("previousDigestSignature"));
            }
        }
    }
}
```

### 2. Leverage S3 Access Logging 
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|DE.AE-3|Event data are aggregated and correlated from multiple sources and sensors|
|DE.CM-7|Monitoring for unauthorized personnel, connections, devices, and software is performed|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch|

**Why?** 
By being able to leverage S3 access logs across all buckets for the ability to see detailed insights into the requests that are made to a bucket, and limit access as appropriate.
Server access logging provides detailed records for the requests that are made to a bucket\. Server access logs are useful for many applications\. For example, access log information can be useful in security and access audits\.

**How?** 
# Enable Amazon S3 server access logging<a name="ServerLogs"></a>

**Note**  
Server access logs do not log information regarding wrong\-region redirect errors for Regions that launched after March 20, 2019\. Wrong\-region redirect errors occur when a request for an object/bucket is made to an endpoint other than the endpoint of the Region in which the bucket exists\. 

## How to enable server access logging<a name="server-access-logging-overview"></a>

To track requests for access to your bucket, you can enable server access logging\. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and an error code, if relevant\. 

**Note**  
There is no extra charge for enabling server access logging on an Amazon S3 bucket, and you are not charged when the logs are PUT to your bucket\. However, any log files that the system delivers to your bucket accrue the usual charges for storage\. You can delete these log files at any time\. Subsequent reads and other requests to these log files are charged normally, as for any other object, including data transfer charges\.

 By default, logging is disabled\. When logging is enabled, logs are saved to a bucket in the same AWS Region as the source bucket\. 

To enable access logging, you must do the following: 
+  Turn on the log delivery by adding logging configuration on the bucket for which you want Amazon S3 to deliver access logs\. We refer to this bucket as the *source bucket*\. 
+  Grant the Amazon S3 Log Delivery group write permission on the bucket where you want the access logs saved\. We refer to this bucket as the *target bucket*\. 

**Note**  
 Amazon S3 only supports granting permission to deliver access logs via bucket ACL, not via bucket policy\.
 Adding *deny* conditions to a bucket policy may prevent Amazon S3 from delivering access logs\.
 [Default bucket encryption](bucket-encryption.html) on the destination bucket *may only be used* if **AES256 \(SSE\-S3\)** is selected\. SSE\-KMS encryption is not supported\. 
S3 Object Lock cannot be enabled on the log destination bucket\.

To turn on log delivery, you provide the following logging configuration information:
+ The name of the target bucket where you want Amazon S3 to save the access logs as objects\. You can have logs delivered to any bucket that you own that is in the same Region as the source bucket, including the source bucket itself\. 

  We recommend that you save access logs in a different bucket so that you can easily manage the logs\. If you choose to save access logs in the source bucket, we recommend that you specify a prefix for all log object keys so that the object names begin with a common string and the log objects are easier to identify\. 

  When your source bucket and target bucket are the same bucket, additional logs are created for the logs that are written to the bucket\. This behavior might not be ideal for your use case because it could result in a small increase in your storage billing\. In addition, the extra logs about logs might make it harder to find the log that you're looking for\. 

  [Key prefixes](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#keyprefix) are also useful to distinguish between source buckets when multiple buckets log to the same destination bucket\.
**Note**  
Both the source and target buckets must be owned by the same AWS account, and the buckets must both be in the same Region\.
+  \(Optional\) A prefix for Amazon S3 to assign to all log object keys\. The prefix makes it simpler for you to locate the log objects\. 

   For example, if you specify the prefix value `logs/`, each log object that Amazon S3 creates begins with the `logs/` prefix in its key, as in this example:

  ```
  logs/2013-11-01-21-32-16-E568B2907131C0C0
  ```

   The key prefix can also help when you delete the logs\. For example, you can set a lifecycle configuration rule for Amazon S3 to delete objects with a specific key prefix\. For more information, see [Deleting Amazon S3 log files](deleting-log-files-lifecycle.md)\.
+  \(Optional\) Permissions so that others can access the generated logs\. By default, the bucket owner always has full access to the log objects\. You can optionally grant access to other users\. 

For more information about enabling server access logging, see [Enabling logging using the console](enable-logging-console.md) and [Enabling logging programmatically](enable-logging-programming.md)\. 

### Additional logging considerations<a name="additional-logging-considerations"></a>

**Note**  
Both the source and target S3 buckets must be owned by the same AWS account, and the S3 buckets must both be in the same Region\.
Amazon S3 only supports granting permission to deliver access logs via bucket ACL, not via bucket policy\.
Adding deny conditions to a bucket policy may prevent Amazon S3 from delivering access logs\.
Default bucket encryption on the destination bucket may only be used if AES256 \(SSE\-S3\) is selected\. SSE\-KMS encryption is not supported

## Log object key format<a name="server-log-keyname-format"></a>

Amazon S3 uses the following object key format for the log objects it uploads in the target bucket:

```
TargetPrefixYYYY-mm-DD-HH-MM-SS-UniqueString/
```

 In the key, `YYYY`, `mm`, `DD`, `HH`, `MM`, and `SS` are the digits of the year, month, day, hour, minute, and seconds \(respectively\) when the log file was delivered, these dates and times are in Coordinated Universal time \(UTC\)\. 

A log file delivered at a specific time can contain records written at any point before that time\. There is no way to know whether all log records for a certain time interval have been delivered or not\. 

 The `UniqueString` component of the key is there to prevent overwriting of files\. It has no meaning, and log processing software should ignore it\. 

The trailing slash */* is required to denote the end of the prefix\.

## How are logs delivered?<a name="how-logs-delivered"></a>

Amazon S3 periodically collects access log records, consolidates the records in log files, and then uploads log files to your target bucket as log objects\. If you enable logging on multiple source buckets that identify the same target bucket, the target bucket will have access logs for all those source buckets\. However, each log object reports access log records for a specific source bucket\. 

Amazon S3 uses a special log delivery account, called the Log Delivery group, to write access logs\. These writes are subject to the usual access control restrictions\. You must grant the Log Delivery group write permission on the target bucket by adding a grant entry in the bucket's access control list \(ACL\)\. If you use the Amazon S3 console to enable logging on a bucket, the console both enables logging on the source bucket and updates the ACL on the target bucket to grant write permission to the Log Delivery group\.

## Best effort server log delivery<a name="LogDeliveryBestEffort"></a>

Server access log records are delivered on a best effort basis\. Most requests for a bucket that is properly configured for logging result in a delivered log record\. Most log records are delivered within a few hours of the time that they are recorded, but they can be delivered more frequently\. 

The completeness and timeliness of server logging is not guaranteed\. The log record for a particular request might be delivered long after the request was actually processed, or *it might not be delivered at all*\. The purpose of server logs is to give you an idea of the nature of traffic against your bucket\. It is rare to lose log records, but server logging is not meant to be a complete accounting of all requests\. 

It follows from the best\-effort nature of the server logging feature that the usage reports available at the AWS portal \(Billing and Cost Management reports on the [AWS Management Console](https://console.aws.amazon.com/)\) might include one or more access requests that do not appear in a delivered server log\. 

## Bucket logging status changes take effect over time<a name="BucketLoggingStatusChanges"></a>

 Changes to the logging status of a bucket take time to actually affect the delivery of log files\. For example, if you enable logging for a bucket, some requests made in the following hour might be logged, while others might not\. If you change the target bucket for logging from bucket A to bucket B, some logs for the next hour might continue to be delivered to bucket A, while others might be delivered to the new target bucket B\. In all cases, the new settings eventually take effect without any further action on your part\. 

### 3. Log CodePipeline API calls with AWS CloudTrail 
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.AE-2|Detected events are analyzed to understand attack targets and methods|
|DE.AE-3|Event data are aggregated and correlated from multiple sources and sensors|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|DE.CM-3|Personnel activity is monitored to detect potential cybersecurity events|
|DE.CM-6|External service provider activity is monitored to detect potential cybersecurity events|
|DE.CM-7|Monitoring for unauthorized personnel, connections, devices, and software is performed|
|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch|


**Why?** 
Using the information collected by CloudTrail, you can determine the request that was made, the IP address from which the request was made, who made the request, when it was made, and additional details that can help identify potenitally malicious activity. By leverage AWS CloudTrail you will have a record of actions taken by a user, role, or an AWS service in AWS CodePipeline.

**How?** 
# Log CodePipeline API Calls with AWS CloudTrail<a name="monitoring-cloudtrail-logs"></a>

AWS CodePipeline is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodePipeline;\. CloudTrail captures all API calls for CodePipeline as events\. The calls captured include calls from the CodePipeline console and code calls to the CodePipeline API operations\. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for CodePipeline\. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in **Event history**\. Using the information collected by CloudTrail, you can determine the request that was made to CodePipeline, the IP address from which the request was made, who made the request, when it was made, and additional details\. 


## CodePipeline Information in CloudTrail<a name="service-name-info-in-cloudtrail"></a>

CloudTrail is enabled on your AWS account when you create the account\. When activity occurs in CodePipeline, that activity is recorded in a CloudTrail event along with other AWS service events in **Event history**\. You can view, search, and download recent events in your AWS account\. For more information, see [Viewing Events with CloudTrail Event History](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html)\. 

For an ongoing record of events in your AWS account, including events for CodePipeline, create a trail\. A *trail* enables CloudTrail to deliver log files to an Amazon S3 bucket\. By default, when you create a trail in the console, the trail applies to all AWS Regions\. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify\. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs\. For more information, see the following: 
+ [Overview for Creating a Trail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-and-update-a-trail.html)
+ [CloudTrail Supported Services and Integrations](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-aws-service-specific-topics.html#cloudtrail-aws-service-specific-topics-integrations)
+ [Configuring Amazon SNS Notifications for CloudTrail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/getting_notifications_top_level.html)
+ [Receiving CloudTrail Log Files from Multiple Regions](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html) and [Receiving CloudTrail Log Files from Multiple Accounts](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-receive-logs-from-multiple-accounts.html)

All CodePipeline actions are logged by CloudTrail and are documented in the [CodePipeline API Reference](http://docs.aws.amazon.com/codepipeline/latest/APIReference)\. For example, calls to the `CreatePipeline`, `GetPipelineExecution` and `UpdatePipeline` actions generate entries in the CloudTrail log files\. 

Every event or log entry contains information about who generated the request\. The identity information helps you determine the following: 
+ Whether the request was made with root or AWS Identity and Access Management \(IAM\) user credentials\.
+ Whether the request was made with temporary security credentials for a role or federated user\.
+ Whether the request was made by another AWS service\.


## Understanding CodePipeline Log File Entries<a name="monitoring-cloudtrail-logs-log-entries-example"></a>

A trail is a configuration that enables delivery of events as log files to an Amazon S3 bucket that you specify\. CloudTrail log files contain one or more log entries\. An event represents a single request from any source and includes information about the requested action, the date and time of the action, request parameters, and so on\. CloudTrail log files aren't an ordered stack trace of the public API calls, so they don't appear in any specific order\. 

The following example shows a CloudTrail log entry for an update pipeline event, where a pipeline named MyFirstPipeline has been edited by the user named JaneDoe\-CodePipeline with the account ID 80398EXAMPLE\. The user changed the name of the source stage of a pipeline from `Source` to `MySourceStage`\. Because both the `requestParameters` and the `responseElements` elements in the CloudTrail log contain the entire structure of the edited pipeline, those elements have been abbreviated in the following example\. **Emphasis** has been added to the `requestParameters` portion of the pipeline where the change occurred, the previous version number of the pipeline, and the `responseElements` portion, which shows the version number incremented by 1\. Edited portions are marked with ellipses \(\.\.\.\) to illustrate where more data appears in a real log entry\.

```
{
 "eventVersion":"1.03",
  "userIdentity": {
   "type":"IAMUser",
   "principalId":"AKIAI44QH8DHBEXAMPLE",
   "arn":"arn:aws:iam::80398EXAMPLE:user/JaneDoe-CodePipeline",
   "accountId":"80398EXAMPLE",
   "accessKeyId":"AKIAIOSFODNN7EXAMPLE",
   "userName":"JaneDoe-CodePipeline",
   "sessionContext": {
	  "attributes":{
	   "mfaAuthenticated":"false",
	   "creationDate":"2015-06-17T14:44:03Z"
	   }
	  },
	"invokedBy":"signin.amazonaws.com"},
	"eventTime":"2015-06-17T19:12:20Z",
	"eventSource":"codepipeline.amazonaws.com",
	"eventName":"UpdatePipeline",
	"awsRegion":"us-east-2",
	"sourceIPAddress":"192.0.2.64",
	"userAgent":"signin.amazonaws.com",
	"requestParameters":{
	  "pipeline":{
		 "version":1,
		 "roleArn":"arn:aws:iam::80398EXAMPLE:role/AWS-CodePipeline-Service",
		 "name":"MyFirstPipeline",
		 "stages":[
		   {
		   "actions":[
		    {
          "name":"MySourceStage",
		      "actionType":{
			     "owner":"AWS",
			     "version":"1",
			     "category":"Source",
			     "provider":"S3"
			  },
			 "inputArtifacts":[],
			 "outputArtifacts":[
			  {"name":"MyApp"}
			  ],
			 "runOrder":1,
			 "configuration":{
			  "S3Bucket":"awscodepipeline-demobucket-example-date",
			  "S3ObjectKey":"sampleapp_linux.zip"
			   }
		    }
		   ],
		     "name":"Source"
		   },
		   (...)
               },
	"responseElements":{
	  "pipeline":{
	    "version":2,
	    (...)
           },
	    "requestID":"2c4af5c9-7ce8-EXAMPLE",
	    "eventID":""c53dbd42-This-Is-An-Example"",
	    "eventType":"AwsApiCall",
	    "recipientAccountId":"80398EXAMPLE"
	    }
      ]
}
```


## Respond/Recover
### 1. Detect and React to changes in Pipeline State with Amazon CloudWatch Events 
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|RS.CO-2|Events are reported consistent with established criteria|
|RS.AN-1|Notifications from detection systems are investigated|
|RS.MI-1|Incidents are contained|
|RS.AN-5|Processes are established to receive, analyze and respond to vulnerabilities disclosed to the organization from internal and external sources (e.g. internal testing, security bulletins, or security researchers)|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch|


**Why?** 
Amazon CloudWatch Events is a web service that monitors your AWS resources and the applications you run on AWS\. You can use Amazon CloudWatch Events to detect and react to changes in the state of a pipeline, stage, or action\. Then, based on rules you create, CloudWatch Events invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule\. Depending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions\.

**How?** 

# Detect and React to Changes in Pipeline State with Amazon CloudWatch Events<a name="detect-state-changes-cloudwatch-events"></a>

Amazon CloudWatch Events are composed of:
+ **Rules\.** An event in Amazon CloudWatch Events is configured by first creating a rule with a selected service as the event source\. 
+ **Targets\.** The new rule receives a selected service as the event target\. 

Examples of Amazon CloudWatch Events rules and targets:
+ A rule that sends a notification when the instance state changes, where an EC2 instance is the event source and Amazon SNS is the event target\.
+ A rule that sends a notification when the build phase changes, where a CodeBuild configuration is the event source and Amazon SNS is the event target\.
+ A rule that detects pipeline changes and invokes an AWS Lambda function\.

To configure AWS CodePipeline as an event source:

1. Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source\.

2. Create a target for your rule that uses one of the services available as targets in Amazon CloudWatch Events, such as AWS Lambda or Amazon SNS\.

3. Grant permissions to Amazon CloudWatch Events to allow it to invoke the selected target service\. 

## Understand How a Pipeline Execution State Change Rule Works<a name="create-cloudwatch-notifications"></a>

You build rules for detecting and reacting to pipeline state changes using the **Events** window in Amazon CloudWatch\. As you build your rule, the **Event Pattern Preview** box in the console \(or the `--event-pattern` output in the CLI\) displays the event fields, in JSON format\. 

You can configure notifications to be sent when the state changes for:
+ Specified pipelines or all your pipelines\. You control this by using `"detail-type":` `"CodePipeline Pipeline Execution State Change"`\.
+ Specified stages or all your stages, within a specified pipeline or all your pipelines\. You control this by using `"detail-type":` `"CodePipeline Stage Execution State Change"`\.
+ Specified actions or all actions, within a specified stage or all stages, within a specified pipeline or all your pipelines\. You control this by using `"detail-type":` `"CodePipeline Action Execution State Change"`\.

Each type of execution state change event emits notifications with specific message content, where:
+ The initial `version` entry shows the version number for the CloudWatch event\.
+ The `version` entry under pipeline `detail` shows the pipeline structure version number\.
+ The `execution-id` entry under pipeline `detail` shows the execution ID for the pipeline execution that caused the state change\. Refer to the GetPipelineExecution API call in the [AWS CodePipeline API Reference](https://docs.aws.amazon.com/codepipeline/latest/APIReference/)\.

**Pipeline execution state change message content: ** When a pipeline execution starts, it emits an event that sends notifications with the following content\. This example is for the pipeline named `"myPipeline"` in the `us-east-1` region\.

```
{
    "version": "0",
    "id": event_Id,
    "detail-type": "CodePipeline Pipeline Execution State Change",
    "source": "aws.codepipeline",
    "account": Pipeline_Account,
    "time": TimeStamp,
    "region": "us-east-1",
    "resources": [
        "arn:aws:codepipeline:us-east-1:account_ID:myPipeline"
    ],
    "detail": {
        "pipeline": "myPipeline",
        "version": "1",
        "state": "STARTED",
        "execution-id": execution_Id
    }
}
```

**Stage execution state change message content: **When a stage execution starts, it emits an event that sends notifications with the following content\. This example is for the pipeline named `"myPipeline"` in the `us-east-1` region, for the stage `"Prod"`\.

```
{
    "version": "0",
    "id": event_Id,
    "detail-type": "CodePipeline Stage Execution State Change",
    "source": "aws.codepipeline",
    "account": Pipeline_Account,
    "time": TimeStamp,
    "region": "us-east-1",
    "resources": [
        "arn:aws:codepipeline:us-east-1:account_ID:myPipeline"
    ],
    "detail": {
        "pipeline": "myPipeline",
        "version": "1",
        "execution-id": execution_Id,
        "stage": "Prod",
        "state": "STARTED"
    }
}
```

**Action execution state change message content: ** When an action execution starts, it emits an event that sends notifications with the following content\. This example is for the pipeline named `"myPipeline"` in the `us-east-1` region, for the action `"myAction"`\.

```
{
    "version": "0",
    "id": event_Id,
    "detail-type": "CodePipeline Action Execution State Change",
    "source": "aws.codepipeline",
    "account": Pipeline_Account,
    "time": TimeStamp,
    "region": "us-east-1",
    "resources": [
        "arn:aws:codepipeline:us-east-1:account_ID:myPipeline"
    ],
    "detail": {
        "pipeline": "myPipeline",
        "version": "1",
        "execution-id": execution_Id,
        "stage": "Prod",
        "action": "myAction",
        "state": "STARTED",
        "type": {
            "owner": "AWS",
            "category": "Deploy",
            "provider": "CodeDeploy",
            "version": 1
        }
    }
}
```

Valid state values:


**Pipeline\-level states**  

| Pipeline State | Description | 
| --- | --- | 
| STARTED | The pipeline execution is currently running\. | 
| SUCCEEDED | The pipeline execution was completed successfully\. | 
| RESUMED | A failed pipeline execution has been retried in response to the RetryStageExecution API call\. | 
| FAILED | The pipeline execution was not completed successfully\. | 
| CANCELED | The pipeline execution was canceled because the pipeline structure was updated\. | 
| SUPERSEDED |  While this pipeline execution was waiting for the next stage to be completed, a newer pipeline execution advanced and continued through the pipeline instead\.  | 


**Stage\-level states**  

| Stage State | Description | 
| --- | --- | 
| STARTED | The stage is currently running\. | 
| SUCCEEDED | The stage was completed successfully\. | 
| RESUMED | A failed stage has been retried in response to the RetryStageExecution API call\. | 
| FAILED | The stage was not completed successfully\. | 
| CANCELED | The stage was canceled because the pipeline structure was updated\. | 


**Action\-level states**  

| Action State | Description | 
| --- | --- | 
| STARTED | The action is currently running\. | 
| SUCCEEDED | The action was completed successfully\. | 
| FAILED | For Approval actions, the FAILED state means the action was either rejected by the reviewer or failed due to an incorrect action configuration\. | 
| CANCELED | The action was canceled because the pipeline structure was updated\. | 

### Prerequisites<a name="cloudwatch-notifications-prerequisites"></a>

Before you create event rules for use in your CodePipeline operations, you should do the following:
+ Complete the CloudWatch Events prerequisites\. For this information, see [Regional Endpoints](https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/GettingSetup_cwe.html#CWE_Prerequisites)\.
+ Familiarize yourself with events, rules, and targets in CloudWatch Events\. 
+ Create the target or targets you will use in your event rules, such as an Amazon SNS notification topic\.

### Send a Notification Whenever Pipeline State Changes \(Console\)<a name="monitoring-cloudwatch-events-about"></a>

These steps show how to use the CloudWatch console to create a rule to send notifications of changes in CodePipeline\. 

**To create a CloudWatch Events rule with CodePipeline as the event source**

1. Open the CloudWatch console at [https://console\.aws\.amazon\.com/cloudwatch/](https://console.aws.amazon.com/cloudwatch/)\.

1. In the navigation pane, choose **Events**\.

1. Choose **Create rule**\. Under **Event source**, from the **Service Name** drop\-down list, choose **CodePipeline**\.

1. From the **Event Type** drop\-down list, choose the level of state change for the notification\.
   + For a rule that applies to pipeline\-level events, choose **CodePipeline Pipeline Execution State Change**\.
   + For a rule that applies to stage\-level events, choose **CodePipeline Stage Execution State Change**\.
   + For a rule that applies to action\-level events, choose **CodePipeline Action Execution State Change**\.

1. Specify the state changes the rule applies to:
   + For a rule that applies to all state changes, choose **Any state**\.
   + For a rule that applies to some state changes only, choose **Specific state\(s\)**, and then choose one or more state values from the list\.  

1. For event patterns that are more detailed than the selectors allow, you can also use the **Edit** option in the **Event Pattern Preview** window to designate an event pattern in JSON format\. The following example shows the JSON structure edited manually to specify a pipeline named "myPipeline\."  

If not otherwise specified, then the event pattern is created for all pipelines/stages/actions and states\.

   For more detailed event patterns, you can copy and paste the following example event patterns into the **Edit** window\.
   +   
**Example**  

     Use this sample event pattern to capture failed deploy and build actions across all the pipelines\.

     ```
     {
     "source": [
         "aws.codepipeline"
       ],
       "detail-type": [
         "CodePipeline Action Execution State Change"
       ],
       "detail": {
         "state": [
           "FAILED"
         ],
         "type": {
           "category": ["Deploy", "Build"]
         }
       }
     }
     ```
   +   
**Example**  

     Use this sample event pattern to capture all rejected or failed approval actions across all the pipelines\.

     ```
     {
      "source": [
         "aws.codepipeline"
       ],
       "detail-type": [
         "CodePipeline Action Execution State Change"
       ],
       "detail": {
         "state": [
           "FAILED"
         ],
         "type": {
           "category": ["Approval"]
         }
       }
     }
     ```
   +   
**Example**  

     Use this sample event pattern to capture all the events from the specified pipelines\.

     ```
     {
     "source": [
         "aws.codepipeline"
       ],
       "detail-type": [
         "CodePipeline Pipeline Execution State Change",
         "CodePipeline Action Execution State Change",
         "CodePipeline Stage Execution State Change"
       ],
       "detail": {
         "pipeline": ["myPipeline", "my2ndPipeline"]
       }
     }
     ```

1. In the **Targets** area, choose **Add target\***\.

1. In the **Select target type** list, choose the type of target for this rule, and then configure options required by that type\. 

1. Choose **Configure details**\.

1. On the **Configure rule details** page, type a name and description for the rule, and then select the **State** box to enable to rule now\.

1. Choose **Create rule**\.

### Send a Notification Whenever Pipeline State Changes \(CLI\)<a name="monitoring-cloudwatch-events-console"></a>

These steps show how to use the CLI to create a CloudWatch Events rule to send notifications of changes in CodePipeline\. 

To use the AWS CLI to create a rule, call the put\-rule command, specifying:
+ A name that uniquely identifies the rule you are creating\. This name must be unique across all of the pipelines you create with CodePipeline associated with your AWS account\.
+ The event pattern for the source and detail fields used by the rule\. 

**To create a CloudWatch Events rule with CodePipeline as the event source**

1. Call the put\-rule command to create a rule specifying the event pattern\. \(See the preceding tables for valid states\.\)

   The following sample command uses \-\-event\-pattern to create a rule called `“MyPipelineStateChanges”` that emits the CloudWatch event when a pipeline execution fails for the pipeline named "myPipeline\."

   ```
   aws events put-rule --name "MyPipelineStateChanges" --event-pattern "{\"source\":[\"aws.codepipeline\"],\"detail-type\":[\"CodePipeline Pipeline Execution State Change\"],\"detail\":{\"pipeline\":[\"myPipeline\"],\"state\":[\"FAILED\"]}}"
   ```

1. Call the put\-targets command to add a target to your new rule, as shown in this example for an Amazon SNS topic:

   ```
   aws events put-targets --rule MyPipelineStateChanges --targets Id=1,Arn=arn:aws:sns:us-west-2:11111EXAMPLE:MyNotificationTopic
   ```

1. Add permissions for Amazon CloudWatch Events to use the designated target service to invoke the notification\. 


## Endnotes
### Endnote 1 <!-- omit in toc -->

### Endnote 2 <!-- omit in toc -->

### Endnote 3 <!-- omit in toc -->

## Capital Group Control Statements
1. All Data-at-rest must be encrypted and use a CG BYOK encryption key.
2. All Data-in-transit must be encrypted using certificates using CG Certificate Authority.
3. Keys storied in a Key Management System (KMS) should be created by Capital Group's hardware security module (HSM) and are a minimum of AES-256.
4. AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch.
5. Local AWS IAM accounts are restricted to services and no user accounts are to be provisioned including IaaS resources.
6. Any AWS service used by CG should not be directly available to the Internet and the default route is always the CG gateway.
7. Use of AWS IAM accounts are restricted to CG networks.
8. Local IAM secrets are rotated every 90 days, including accounts IaaS resources.
9. Encryption keys are rotated annually.
10. Root accounts must have 2FA/MFA enabled.
