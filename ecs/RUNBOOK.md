<img src="https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png" alt="AWS" width="250"/>

# Amazon Elastic Container Service (ECS) - Security Runbook <!-- omit in toc -->
## Capgroup Cybersecurity Control Alignment <!-- omit in toc -->

**Generated By:**  
[Rob Goss (RMG)](https://cgweb3/profile/RMG)
<br>
[Srinath Medala (SRGM)](https://cgweb3/profile/SRGM)
<br>
Security Engineering

**Last Update:** *08/21/2021*

## Table of Contents <!-- omit in toc -->
- [Overview](#overview)
- [Cloud Security Requirements](#cloud-security-requirements)
  - [1. Implement least privilege IAM Roles for Tasks](#1-implement-least-privilege-iam-roles-for-tasks)
  - [2. Using Elastic Container Registry (ECR) for storing and retrieving Docker images](#2-using-elastic-container-registry-ecr-for-storing-and-retrieving-docker-images)
  - [3. Configuring VPC endpoints for ECS](#3-configuring-vpc-endpoints-for-ecs)
  - [4. Configuring AWS Systems Manager Parameter Store or AWS Secrets Manager for reference of secrets into Container Definitions](#4-configuring-aws-systems-manager-parameter-store-or-aws-secrets-manager-for-reference-of-secrets-into-container-definitions)
  - [5. Specifying sensitive data using AWS secrets manager](#5-specifying-sensitive-data-using-aws-secrets-manager)
  - [6. Using the awslogs Log Driver](#6-using-the-awslogs-log-driver)
  - [7. Creating a Trail to log ECS API calls](#7-creating-a-trail-to-log-ecs-api-calls)
  - [8. Enable VPC Flow Logs for ECS Cluster VPC (EC2 Launch Types Only)](#8-enable-vpc-flow-logs-for-ecs-cluster-vpc-ec2-launch-types-only)
  - [9. Scan images for Vulnerabilities](#9-scan-images-for-vulnerabilities)
  - [10. Remove special permissions from images](#10-remove-specal-permissions-from-images)
  - [11. Run containers as non-root users](#11-run-containers-as-non-root-users)
  - [12. Use a read-only root file system](#12-use-a-read-only-file-system)
  - [13. ECS data in transit must enforce TLS with version 1.2 or higher](#13-ecs-data-in-transit-must-enforce-tls-with-version-1.2-or-higher)
  - [14. Make sure ECS Task network interface does not have public IP address](#14-make-sure-ecs-task-network-interface-does-not-have-public-ip-address)
  - [15. Use always Fargate launch type in ECS Cluster](#15-use-always-fargate-launch-type-in-ecs-cluster)
- [Operational Best Practices](#operational-best-practices)  
  - [1. Utilizing AWS CloudWatch Container Insights](#1-utilizing-aws-cloudwatch-container-insights)
  - [2. Utilize Amazon ECS Events and Eventbridge](#2-utilize-amazon-ecs-events-and-eventbridge)
  - [3. ECS Resources are tagged according to CG Standards](#4-ecs-resources-are-tagged-according-to-cg-standards)
  - [4. Configure tasks with CPU and Memory limits (Amazon EC2)](#5-configure-tasks-with-cpu-and-memory-limits)
- [Endnotes](#Endnotes)
- [Capital Group Glossary](#Capital-Group-Glossary)
  <br><br>
  


## Overview
Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service that helps you easily deploy, manage, and scale containerized applications. It deeply integrates with the rest of the AWS platform to provide a secure and easy-to-use solution for running container workloads in the cloud and now on your infrastructure with Amazon ECS Anywhere.


AWS provides a number of security features for Amazon Elastic Container Service (ECS) which help you comply with the NIST Cybersecurity Framework. The following Runbook will Provide implementation details to deploy the Amazon Elastic Conatainer service in accordance with NIST CSF and service applicable security controls.This runbook in its continued development will provide support to the automated configuration of hardening workloads an processes. 

These NIST Controls and Subcategories are not applicable to this service: PR.AT, PR.MA, PR.IP  (Unless stated), PR.AC-2, PR.AC-3, PR.DS-3, PR.DS-8, PR.PT-2, PR.PT-5, DE.DP1, DE.DP-2. DE.DP-3, DE.CM-3, DE.AE-5, RC, RS.MI.

These capital group control statements are not applicable to the ECS service: 5,7,8,9,10. 

## Cloud Security Requirements
<img src="/docs/img/Prevent.png" width="50">

### 1. Implement least privilege IAM Roles for Tasks
**Capital Group Controls:** 
<br>
|Control Statement|Description|
|------|----------------------|
|Control Definition Needed|Control Definition Description Needed|

<br>
___

### IAM Roles for Tasks

**Why?**

Based on IAM least privilege access model, CG Security Team recommends each task should have its own IAM role based on the access it needs. Outside various available options of IAM Roles, CG recommends to use the following 'Task Execution Role'.The task execution role is used to grant the Amazon ECS container agent permission to call specific AWS API actions on your behalf. For example, when you use AWS Fargate, Fargate needs an IAM role that allows it to pull images from Amazon ECR and write logs to CloudWatch Logs. An IAM role is also required when a task references a secret that's stored in AWS Secrets Manager, such as an image pull secret.

### Benefits of Using IAM Roles for Tasks
+ **Credential Isolation:** A container can only retrieve credentials for the IAM role that is defined in the task definition to which it belongs; a container never has access to credentials that are intended for another container that belongs to another task\.
+ **Authorization:** Unauthorized containers cannot access IAM role credentials defined for other tasks\.
+ **Auditability:** Access and event logging is available through CloudTrail to ensure retrospective auditing\. Task credentials have a context of `taskArn` that is attached to the session, so CloudTrail logs show which task is using which role\.

**How?**

With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task\. Applications must sign their AWS API requests with AWS credentials, and this feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances\. Instead of creating and distributing your AWS credentials to the containers or using the EC2 instance’s role, you can associate an IAM role with an ECS task definition or `RunTask` API operation\. The applications in the task’s containers can then use the AWS SDK or CLI to make API requests to authorized AWS services\.


### Creating an IAM Role and Policy for your Tasks

You must create an IAM policy for your tasks to use that specifies the permissions that you would like the containers in your tasks to have\. You have several ways to create a new IAM permission policy\. You can copy a complete AWS managed policy that already does some of what you're looking for and then customize it to your specific requirements\. For more information, see [Creating a New Policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create.html) in the *IAM User Guide*\.

You must also create a role for your tasks to use before you can specify it in your task definitions\. You can create the role using the **Amazon Elastic Container Service Task Role** service role in the IAM console\. Then you can attach your specific IAM policy to the role that gives the containers in your task the permissions you desire\. The procedures below describe how to do this\.

If you have multiple task definitions or services that require IAM permissions, you should consider creating a role for each specific task definition or service with the minimum required permissions for the tasks to operate so that you can minimize the access that you provide for each task\. 

The Amazon ECS Task Role trust relationship is shown below\.

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "",
      "Effect": "Allow",
      "Principal": {
        "Service": "ecs-tasks.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
```

**To create an IAM policy for your tasks**

In this example, we create a policy to allow read\-only access to an Amazon S3 bucket\. You could store database credentials or other secrets in this bucket, and the containers in your task can read the credentials from the bucket and load them into your application\.

1. Open the IAM console at [https://console\.aws\.amazon\.com/iam/](https://console.aws.amazon.com/iam/)\.

1. In the navigation pane, choose **Policies** and then choose **Create policy**\. 

1. Follow the steps under one of the following tabs, which shows you how to use the visual or JSON editors\.

------
### [ Using the visual editor ]

1. For **Service**, choose **S3**\.

1. For **Actions**, expand the **Read** option and select **GetObject**\.

1. For **Resources**, select **Add ARN** and enter the full Amazon Resource Name \(ARN\) of your Amazon S3 bucket, and then choose **Review policy**\.

1. On the **Review policy** page, for **Name** type your own unique name, such as `AmazonECSTaskS3BucketPolicy`\.

1. Choose **Create policy** to finish\.

------
### [ Using the JSON editor ]

1. In the **Policy Document** field, paste the policy to apply to your tasks\. The example below allows permission to the *my\-task\-secrets\-bucket* Amazon S3 bucket\. You can modify the policy document to suit your specific needs\.

```
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Action": [
           "s3:GetObject"
         ],
         "Resource": [
           "arn:aws:s3:::my-task-secrets-bucket/*"
         ]
       }
     ]
   }
```

1. Choose **Create policy**\. 

------

**To create an IAM role for your tasks**

1. Open the IAM console at [https://console\.aws\.amazon\.com/iam/](https://console.aws.amazon.com/iam/)\.

2. In the navigation pane, choose **Roles**, **Create role**\. 

3. For **Select type of trusted entity** section, choose **AWS service**\.

4. For **Choose the service that will use this role**, choose **Elastic Container Service**\.

5. For **Select your use case**, choose **Elastic Container Service Task** and choose **Next: Permissions**\.

6. For **Attach permissions policy**, select the policy to use for your tasks \(in this example `AmazonECSTaskS3BucketPolicy`, and then choose **Next: Tags**\.

7. For **Add tags \(optional\)**, enter any metadata tags you want to associate with the IAM role, and then choose **Next: Review**\.

8. For **Role name**, enter a name for your role\. For this example, type `AmazonECSTaskS3BucketRole` to name the role, and then choose **Create role** to finish\.


### Specifying an IAM Role for your Tasks

After you have created a role and attached a policy to that role, you can run tasks that assume the role\. You have several options to do this:
+ Specify an IAM role for your tasks in the task definition\. You can create a new task definition or a new revision of an existing task definition and specify the role you created previously\. If you use the console to create your task definition, choose your IAM role in the **Task Role** field\. If you use the AWS CLI or SDKs, specify your task role ARN using the `taskRoleArn` parameter\. 

**Note**  
This option is required if you want to use IAM task roles in an Amazon ECS service\.
+ Specify an IAM task role override when running a task\. You can specify an IAM task role override when running a task\. If you use the console to run your task, choose **Advanced Options** and then choose your IAM role in the **Task Role** field\. If you use the AWS CLI or SDKs, specify your task role ARN using the `taskRoleArn` parameter in the `overrides` JSON object\. 

**Note**  
In addition to the standard Amazon ECS permissions required to run tasks and services, IAM users also require `iam:PassRole` permissions to use IAM roles for tasks\.

## 2. Using Elastic Container Registry (ECR) for storing and retrieving Docker images

**Why?**

CG's public access requirements for cloud state that resources should be secured in environments and not be publicly accessible. ECR is a fully managed container registry that makes it easy to store, manage, share and deploy container images and artifacts in a secure manner.Amazon ECR hosts your images in a highly available and high-performance architecture, allowing you to deploy images for your container applications reliably. You can share container software privately within Capital Group or publicly worldwide for anyone to discover and download.

**How?**

### Using Amazon ECR Images with Amazon ECS

You can use your ECR images with Amazon ECS, but you need to satisfy the following prerequisites\.
+ Your container instances must be using at least version 1\.7\.0 of the Amazon ECS container agent\. The latest version of the Amazon ECS–optimized AMI supports ECR images in task definitions\. For more information, including the latest Amazon ECS–optimized AMI IDs, see [Amazon ECS Container Agent Versions](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-versions.html) in the *Amazon Elastic Container Service Developer Guide*\.
+ The Amazon ECS container instance role \(`ecsInstanceRole`\) that you use with your container instances must possess the following IAM policy permissions for Amazon ECR\.

```
  {
      "Version": "2012-10-17",
      "Statement": [
          {
              "Effect": "Allow",
              "Action": [
                  "ecr:BatchCheckLayerAvailability",
                  "ecr:BatchGetImage",
                  "ecr:GetDownloadUrlForLayer",
                  "ecr:GetAuthorizationToken"
              ],
              "Resource": "*"
          }
      ]
  }
```

  If you use the `AmazonEC2ContainerServiceforEC2Role` managed policy for your container instances, then your role has the proper permissions\. To check that your role supports Amazon ECR, see [Amazon ECS Container Instance IAM Role](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html) in the *Amazon Elastic Container Service Developer Guide*\.
+ In your ECS task definitions, make sure that you are using the full `registry/repository:tag` naming for your ECR images\. For example, `aws_account_id.dkr.ecr.region.amazonaws.com``/my-web-app:latest`\.

## 3. Configuring VPC Endpoints for ECS 

Capital Group:
|Control Statement|Description|
|------|----------------------|
|6|Any AWS service used by CG should not be directly available to the Internet and the default route is always the CG gateway.| 

**Why?**

In order to use securely communicate with AWS Services like S3, ECR, without accessing public internet, VPC Endpoints using AWS Privatelink provides a way to restrict the traffic from VPC to AWS Service using private IP addresses. 

### Amazon ECS Interface VPC Endpoints \(AWS PrivateLink\)

You can improve the security posture of your VPC by configuring Amazon ECS to use an interface VPC endpoint\. Interface endpoints are powered by AWS PrivateLink, a technology that enables you to privately access Amazon ECS APIs by using private IP addresses\. PrivateLink restricts all network traffic between your VPC and Amazon ECS to the Amazon network\. You don't need an internet gateway, a NAT device, or a virtual private gateway\.

You're not required to configure PrivateLink, but we recommend it\. For more information about PrivateLink and VPC endpoints, see [Accessing Services Through AWS PrivateLink](https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html#what-is-privatelink)\.

### Considerations for Amazon ECS VPC Endpoints

Before you set up interface VPC endpoints for Amazon ECS, be aware of the following considerations:
+ Tasks using the Fargate launch type don't require the interface VPC endpoints for Amazon ECS, but you might need interface VPC endpoints for Amazon ECR or Amazon CloudWatch Logs described in the following points\.
  + To allow your tasks to pull private images from Amazon ECR, you must create the interface VPC endpoints for Amazon ECR\. For more information, see [Interface VPC Endpoints \(AWS PrivateLink\)](https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html) in the *Amazon Elastic Container Registry User Guide*\.
**Important**  
If you configure Amazon ECR to use an interface VPC endpoint, you can create a task execution role that includes condition keys to restrict access to a specific VPC or VPC endpoint\. 
  + If your VPC doesn't have an internet gateway and your tasks use the `awslogs` log driver to send log information to CloudWatch Logs, you must create an interface VPC endpoint for CloudWatch Logs\. For more information, see [Using CloudWatch Logs with Interface VPC Endpoints](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/cloudwatch-logs-and-interface-VPC.html) in the *Amazon CloudWatch Logs User Guide*\.
+ Tasks using the EC2 launch type require that the container instances that they're launched on to run at least version `1.25.1` of the Amazon ECS container agent\. 
+ VPC endpoints currently don't support cross\-Region requests\. Ensure that you create your endpoint in the same Region where you plan to issue your API calls to Amazon ECS\.
+ VPC endpoints only support Amazon\-provided DNS through Amazon Route 53\. If you want to use your own DNS, you can use conditional DNS forwarding\. For more information, see [DHCP Options Sets](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_DHCP_Options.html) in the *Amazon VPC User Guide*\.
+ The security group attached to the VPC endpoint must allow incoming connections on port 443 from the private subnet of the VPC\.
+ Controlling access to Amazon ECS by attaching an endpoint policy to the VPC endpoint isn't currently supported\. By default, full access to the service will be allowed through the endpoint\. For more information, see [Controlling Access to Services with VPC Endpoints](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-access.html) in the *Amazon VPC User Guide*\.

**How?**

### Creating the VPC Endpoints for Amazon ECS

To create the VPC endpoint for the Amazon ECS service, use the [Creating an Interface Endpoint](https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html#create-interface-endpoint) procedure in the *Amazon VPC User Guide* to create the following endpoints\. 
### Creating an interface endpoint
#### Interface VPC endpoints \(AWS PrivateLink\)

An interface VPC endpoint \(interface endpoint\) enables you to connect to services powered by AWS PrivateLink\. These services include some AWS services, services hosted by other AWS customers and Partners in their own VPCs \(referred to as *endpoint services*\), and supported AWS Marketplace Partner services\. The owner of the service is the *service provider*, and you, as the principal creating the interface endpoint, are the *service consumer*\.
The following are the general steps for setting up an interface endpoint:

1. Choose the VPC in which to create the interface endpoint, and provide the name of the AWS service, endpoint service, or AWS Marketplace service to which you're connecting\.

1. Choose a subnet in your VPC to use the interface endpoint\. We create an *endpoint network interface* in the subnet\. You can specify more than one subnet in different Availability Zones \(as supported by the service\) to help ensure that your interface endpoint is resilient to Availability Zone failures\. In that case, we create an endpoint network interface in each subnet that you specify\. 
**Note**  
An endpoint network interface is a requester\-managed network interface\. You can view it in your account, but you cannot manage it yourself\. For more information, see [Elastic Network Interfaces](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html)\.

1. Specify the security groups to associate with the endpoint network interface\. The security group rules control the traffic to the endpoint network interface from resources in your VPC\. If you do not specify a security group, we associate the default security group for the VPC\.

1. \(Optional, AWS services and AWS Marketplace Partner services only\) Enable [private DNS](#vpce-private-dns) for the endpoint to enable you to make requests to the service using its default DNS hostname\.
**Important**  
Private DNS is enabled by default for endpoints created for AWS services and AWS Marketplace Partner services\.   
Private DNS is enabled in the other subnets which are in the same VPC and Availability Zone or Local Zone\.

1. When the service provider and the consumer are in different accounts, see [Interface endpoint Availability Zone considerations](#vpce-interface-availability-zones) for information about how to use Availability Zone IDs to identify the interface endpoint Availability Zone\.

1. After you create the interface endpoint, it's available to use when it's accepted by the service provider\. The service provider must configure the service to accept requests automatically or manually\. AWS services and AWS Marketplace services generally accept all endpoint requests automatically\. For more information about the lifecycle of an endpoint, see [Interface endpoint lifecycle](#vpce-interface-lifecycle)\.

Services cannot initiate requests to resources in your VPC through the endpoint\. An endpoint only returns responses to traffic that is initiated from resources in your VPC\. Before you integrate a service and an endpoint, review the service\-specific VPC endpoint documentation for any service\-specific configuration and limitations\. 

To create an interface endpoint, you must specify the VPC in which to create the interface endpoint, and the service to which to establish the connection\. 

For AWS services, or AWS Marketplace Partner services, you can optionally enable [private DNS](#vpce-private-dns) for the endpoint to enable you to make requests to the service using its default DNS hostname\.

**Important**  
Private DNS is enabled by default for endpoints created for AWS services and AWS Marketplace Partner services\. 

------
#### [ Console ]

**To create an interface endpoint to an AWS service using the console**

1. Open the Amazon VPC console at [https://console\.aws\.amazon\.com/vpc/](https://console.aws.amazon.com/vpc/)\.

1. In the navigation pane, choose **Endpoints**, **Create Endpoint**\.

1. For **Service category**, ensure that **AWS services** is selected\.

1. For **Service Name**, choose the service to which to connect\. For **Type**, ensure that it indicates **Interface**\.

1. Complete the following information and then choose **Create endpoint**\.
   + For **VPC**, select a VPC in which to create the endpoint\.
   + For **Subnets**, select the subnets \(Availability Zones\) in which to create the endpoint network interfaces\.

     Not all Availability Zones may be supported for all AWS services\.
   + To enable private DNS for the interface endpoint, for **Enable Private DNS Name**, select the check box\.

     This option is enabled by default\. To use the private DNS option, the following attributes of your VPC must be set to `true`: `enableDnsHostnames` and `enableDnsSupport`\. 
   + For **Security group**, select the security groups to associate with the endpoint network interfaces\.
   + \(Optional\) Add or remove a tag\.

     \[Add a tag\] Choose **Add tag** and do the following:
     + For **Key**, enter the key name\.
     + For **Value**, enter the key value\.

     \[Remove a tag\] Choose the delete button \(“x”\) to the right of the tag’s Key and Value\.

To create an interface endpoint to an endpoint service, you must have the name of the service to which to connect\. The service provider can provide you with the name\. 

**To create an interface endpoint to an endpoint service**

1. Open the Amazon VPC console at [https://console\.aws\.amazon\.com/vpc/](https://console.aws.amazon.com/vpc/)\.

1. In the navigation pane, choose **Endpoints**, **Create Endpoint**\.

1. For **Service category**, choose **Find service by name**\.

1. For **Service Name**, enter the name of the service \(for example, `com.amazonaws.vpce.us-east-1.vpce-svc-0e123abc123198abc`\) and choose **Verify**\.

1. Complete the following information and then choose **Create endpoint**\.
   + For **VPC**, select a VPC in which to create the endpoint\.
   + For **Subnets**, select the subnets \(Availability Zones\) in which to create the endpoint network interfaces\.

     Not all Availability Zones may be supported for the service\.
   + For **Security group**, select the security groups to associate with the endpoint network interfaces\.
   + \(Optional\) Add or remove a tag\.

     \[Add a tag\] Choose **Add tag** and do the following:
     + For **Key**, enter the key name\.
     + For **Value**, enter the key value\.

     \[Remove a tag\] Choose the delete button \(“x”\) to the right of the tag’s Key and Value\.

**To create an interface endpoint to an AWS Marketplace Partner service**

1. Go to the [PrivateLink](https://aws.amazon.com/marketplace/saas/privatelink) page in AWS Marketplace and subscribe to a service from a software as a service \(SaaS\) provider\. Services that support interface endpoints include an option to connect via an endpoint\.

1. Open the Amazon VPC console at [https://console\.aws\.amazon\.com/vpc/](https://console.aws.amazon.com/vpc/)\.

1. In the navigation pane, choose **Endpoints**, **Create Endpoint**\.

1. For **Service category**, choose **Your AWS Marketplace services**\.

1. Choose the AWS Marketplace service to which you've subscribed\.

1. Complete the following information and then choose **Create endpoint**\.
   + For **VPC**, select a VPC in which to create the endpoint\.
   + For **Subnets**, select the subnets \(Availability Zones\) in which to create the endpoint network interfaces\.

     Not all Availability Zones may be supported for the service\.
   + For **Security group**, select the security groups to associate with the endpoint network interfaces\.
   + \(Optional\) Add or remove a tag\.

     \[Add a tag\] Choose **Add tag** and do the following:
     + For **Key**, enter the key name\.
     + For **Value**, enter the key value\.

     \[Remove a tag\] Choose the delete button \(“x”\) to the right of the tag’s Key and Value\.

------


## 4. Configuring AWS Systems Manager Parameter Store or AWS Secrets Manager for reference of secrets into Container Definitions

___
**Why?**

For non sensitive information use Parameter Store for environmental variables. The advantge of using Parameter store is decoupling the environmental variables from task definitions. Environment variables can be updated without touching the task definition. 

### Amazon EC2 Systems Manager Parameter Store

Parameter Store is a feature of Amazon EC2 Systems Manager. It provides a centralized, encrypted store for sensitive information and has many advantages when combined with other capabilities of Systems Manager, such as Run Command and State Manager. The service is fully managed, highly available, and highly secured.

Because Parameter Store is accessible using the Systems Manager API, AWS CLI, and AWS SDKs, you can also use it as a generic secret management store. Secrets can be easily rotated and revoked. Parameter Store is integrated with AWS KMS so that specific parameters can be encrypted at rest with the default or custom KMS key. Importing KMS keys enables you to use your own keys to encrypt sensitive data.

Access to Parameter Store is enabled by IAM policies and supports resource level permissions for access. An IAM policy that grants permissions to specific parameters or a namespace can be used to limit access to these parameters. CloudTrail logs, if enabled for the service, record any attempt to access a parameter.

While Amazon S3 has many of the above features and can also be used to implement a central secret store, Parameter Store has the following added advantages:

  + Easy creation of namespaces to support different stages of the application lifecycle.
  + KMS integration that abstracts parameter encryption from the application while requiring the instance or container to have access to the KMS key and for the decryption to take place locally in memory.
  + Stored history about parameter changes.
  + A service that can be controlled separately from S3, which is likely used for many other applications.
  + A configuration data store, reducing overhead from implementing multiple systems.
  + No usage costs.

**How?**

### IAM roles for tasks

With IAM roles for Amazon ECS tasks, you can specify an IAM role to be used by the containers in a task. Applications interacting with AWS services must sign their API requests with AWS credentials. This feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances.

Instead of creating and distributing your AWS credentials to the containers or using the EC2 instance role, you can associate an IAM role with an ECS task definition or the RunTask API operation. 
You can use IAM roles for tasks to securely introduce and authenticate the application or container with the centralized Parameter Store. Access to the secret manager should include features such as:

   + Limited TTL for credentials used
   + Granular authorization policies
   + An ID to track the requests in the logs of the central secret manager
   + Integration support with the scheduler that could map between the container or task deployed and the relevant access privileges

IAM roles for tasks support this use case well, as the role credentials can be accessed only from within the container for which the role is defined. The role exposes temporary credentials and these are rotated automatically. Granular IAM policies are supported with optional conditions about source instances, source IP addresses, time of day, and other options.

The source IAM role can be identified in the CloudTrail logs based on a unique Amazon Resource Name and the access permissions can be revoked immediately at any time with the IAM API or console. As Parameter Store supports resource level permissions, a policy can be created to restrict access to specific keys and namespaces.

### Dynamic environment association

In many cases, the container image does not change when moving between environments, which supports immutable deployments and ensures that the results are reproducible. What does change is the configuration: in this context, specifically the secrets. For example, a database and its password might be different in the staging and production environments. There’s still the question of how do you point the application to retrieve the correct secret? Should it retrieve prod.app1.secret, test.app1.secret or something else?

One option can be to pass the environment type as an environment variable to the container. The application then concatenates the environment type (prod, test, etc.) with the relative key path and retrieves the relevant secret. In most cases, this leads to a number of separate ECS task definitions.

When you describe the task definition in a CloudFormation template, you could base the entry in the IAM role that provides access to Parameter Store, KMS key, and environment property on a single CloudFormation parameter, such as “environment type.” This approach could support a single task definition type that is based on a generic CloudFormation template.


## 5. Specifying sensitive data using AWS secrets manager

Capital Group:
|Control Statement|Description|
|------|----------------------|
|1|All Data at rest must be encrypted and use a CG BYOK encryption key.| 
|2|All Data in transit must be encrypted using certificates using CG Certificate Authority.|
|3|Keys storied in a Key Management System (KMS) should be created by Capital Groups hardware security module (HSM) and are a minimum of AES-256.|

**Why?**

Amazon ECS enables you to inject sensitive data into your containers by storing your sensitive data in AWS Secrets Manager secrets and then referencing them in your container definition. Sensitive data stored in Secrets Manager secrets can be exposed to a container as environment variables or as part of the log configuration.

When you inject a secret as an environment variable, you can specify a JSON key or version of a secret to inject. This process helps you control the sensitive data exposed to your container. 

Topics
  -  Considerations for Specifying Sensitive Data Using Secrets Manager
  -  Required IAM Permissions for Amazon ECS Secrets
  -  Injecting Sensitive Data as an Environment Variable
  -  Injecting Sensitive Data in a Log Configuration
  -  Creating an AWS Secrets Manager Secret
  -  Creating a Task Definition that References Sensitive Data

### Considerations for Specifying Sensitive Data Using Secrets Manager

The following should be considered when using Secrets Manager to specify sensitive data for containers.

  + For tasks that use the Fargate launch type, the following should be considered:
    + It is only supported to inject the full contents of a secret as an environment variable. Specifying a specific JSON key or version is not supported at this time.
     + To inject the full content of a secret as an environment variable or in a log configuration, you must use platform version 1.3.0 or later. For information, see AWS Fargate Platform Versions.

   + For tasks that use the EC2 launch type, the following should be considered:

       + To inject a secret using a specific JSON key or version of a secret, your container instance must have version 1.37.0 or later of the container agent. However, we recommend using the latest container agent version. For information about checking your agent version and updating to the latest version, see Updating the Amazon ECS Container Agent.

        To inject the full contents of a secret as an environment variable or to inject a secret in a log configuration, your container instance must have version 1.22.0 or later of the container agent.

  +  Sensitive data is injected into your container when the container is initially started. If the secret is subsequently updated or rotated, the container will not receive the updated value automatically. You must either launch a new task or if your task is part of a service you can update the service and use the Force new deployment option to force the service to launch a fresh task.

  + For Windows tasks that are configured to use the awslogs logging driver, you must also set the ECS_ENABLE_AWSLOGS_EXECUTIONROLE_OVERRIDE environment variable on your container instance. This can be done with User Data using the following syntax:
```
    <powershell>
    [Environment]::SetEnvironmentVariable("ECS_ENABLE_AWSLOGS_EXECUTIONROLE_OVERRIDE", $TRUE, "Machine")
    Initialize-ECSAgent -Cluster <cluster name> -EnableTaskIAMRole -LoggingDrivers '["json-file","awslogs"]'
    </powershell>
```
**How?**

### Required IAM Permissions for Amazon ECS Secrets

To use this feature, you must have the Amazon ECS task execution role and reference it in your task definition. This allows the container agent to pull the necessary Secrets Manager resources. For more information, see Amazon ECS Task Execution IAM Role.
Important

For tasks that use the EC2 launch type, you must use the ECS agent configuration variable ECS_ENABLE_AWSLOGS_EXECUTIONROLE_OVERRIDE=true to use this feature. You can add it to the ./etc/ecs/ecs.config file during container instance creation or you can add it to an existing instance and then restart the ECS agent. 

To provide access to the Secrets Manager secrets that you create, manually add the following permissions as an inline policy to the task execution role. For more information, see Adding and Removing IAM Policies.

  +  secretsmanager:GetSecretValue–Required if you are referencing a Secrets Manager secret.

  +  kms:Decrypt–Required only if your secret uses a custom KMS key and not the default key. The ARN for your custom key should be added as a resource.

The following example inline policy adds the required permissions.
```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "secretsmanager:GetSecretValue",
        "kms:Decrypt"
      ],
      "Resource": [
        "arn:aws:secretsmanager:<region>:<aws_account_id>:secret:<secret_name>",
        "arn:aws:kms:<region>:<aws_account_id>:key/<key_id>"
      ]
    }
  ]
}
```
### Injecting Sensitive Data as an Environment Variable

Within your container definition, you can specify the following:

  + The secrets object containing the name of the environment variable to set in the container

  + The Amazon Resource Name (ARN) of the Secrets Manager secret

   + Additional parameters that contain the sensitive data to present to the container

The following example shows the full syntax that must be specified for the Secrets Manager secret.

```
arn:aws:secretsmanager:region:aws_account_id:secret:secret-name:json-key:version-stage:version-id
```
### Important

If you are using AWS Fargate, it is only supported to specify the full ARN of the secret in your task definition. Specifying a specific JSON key or version is not supported at this time.

The following section describes the additional parameters. These parameters are optional, but if you do not use them, you must include the colons : to use the default values. Examples are provided below for more context.

*json-key*
Specifies the name of the key in a key-value pair with the value that you want to set as the environment variable value. Only values in JSON format are supported. If you do not specify a JSON key, then the full contents of the secret is used.

*version-stage*
Specifies the staging label of the version of a secret that you want to use. If a version staging label is specified, you cannot specify a version ID. If no version stage is specified, the default behavior is to retrieve the secret with the AWSCURRENT staging label.

Staging labels are used to keep track of different versions of a secret when they are either updated or rotated. Each version of a secret has one or more staging labels and an ID. 

*version-id*
Specifies the unique identifier of the version of a secret that you want to use. If a version ID is specified, you cannot specify a version staging label. If no version ID is specified, the default behavior is to retrieve the secret with the AWSCURRENT staging label.

Version IDs are used to keep track of different versions of a secret when they are either updated or rotated. Each version of a secret has an ID. 

For a full tutorial on creating a Secrets Manager secret and injecting it into a container as an environment variable, see Tutorial: Specifying Sensitive Data Using Secrets Manager Secrets.

### Example Container Definitions

The following examples show ways in which you can reference Secrets Manager secrets in your container definitions.

**Example referencing a full secret**

The following is a snippet of a task definition showing the format when referencing the full text of a Secrets Manager secret.

```
{
  "containerDefinitions": [{
    "secrets": [{
      "name": "environment_variable_name",
      "valueFrom": "arn:aws:secretsmanager:region:aws_account_id:secret:secret_name-AbCdEf"
    }]
  }]
}

```
**Example referencing a specific key within a secret**

The following shows an example output from a get-secret-value command that displays the contents of a secret along with the version staging label and version ID associated with it.

```
{
    "ARN": "arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf",
    "Name": "appauthexample",
    "VersionId": "871d9eca-18aa-46a9-8785-981dd39ab30c",
    "SecretString": "{\"username1\":\"password1\",\"username2\":\"password2\",\"username3\":\"password3\"}",
    "VersionStages": [
        "AWSCURRENT"
    ],
    "CreatedDate": 1581968848.921
}
```
Reference a specific key from the previous output in a container definition by specifying the key name at the end of the ARN.
```
{
  "containerDefinitions": [{
    "secrets": [{
      "name": "environment_variable_name",
      "valueFrom": "arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf:username1::"
    }]
  }]
}
```
 **Example referencing a specific secret version**

The following shows an example output from a *describe-secret* command that displays the unencrypted contents of a secret along with the metadata for all versions of the secret.
```
{
    "ARN": "arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf",
    "Name": "appauthexample",
    "Description": "Example of a secret containing application authorization data.",
    "RotationEnabled": false,
    "LastChangedDate": 1581968848.926,
    "LastAccessedDate": 1581897600.0,
    "Tags": [],
    "VersionIdsToStages": {
        "871d9eca-18aa-46a9-8785-981dd39ab30c": [
            "AWSCURRENT"
        ],
        "9d4cb84b-ad69-40c0-a0ab-cead36b967e8": [
            "AWSPREVIOUS"
        ]
    }
}
```
Reference a specific version staging label from the previous output in a container definition by specifying the key name at the end of the ARN.

```
{
  "containerDefinitions": [{
    "secrets": [{
      "name": "environment_variable_name",
      "valueFrom": "arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf::AWSPREVIOUS:"
    }]
  }]
}
```
Reference a specific version ID from the previous output in a container definition by specifying the key name at the end of the ARN.

```
{
  "containerDefinitions": [{
    "secrets": [{
      "name": "environment_variable_name",
      "valueFrom": "arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf::9d4cb84b-ad69-40c0-a0ab-cead36b967e8"
    }]
  }]
}
```
**Example referencing a specific key and version staging label of a secret**

The following shows how to reference both a specific key within a secret and a specific version staging label.

```
{
  "containerDefinitions": [{
    "secrets": [{
      "name": "environment_variable_name",
      "valueFrom": "arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf:username1:AWSPREVIOUS:"
    }]
  }]
}
```
To specify a specific key and version ID, use the following syntax.
```
{
  "containerDefinitions": [{
    "secrets": [{
      "name": "environment_variable_name",
      "valueFrom": "arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf:username1::9d4cb84b-ad69-40c0-a0ab-cead36b967e8"
    }]
  }]
}
```
### Injecting Sensitive Data in a Log Configuration

Within your container definition, when specifying a logConfiguration you can specify secretOptions with the name of the log driver option to set in the container and the full ARN of the Secrets Manager secret containing the sensitive data to present to the container.

The following is a snippet of a task definition showing the format when referencing an Secrets Manager secret.

```
{
  "containerDefinitions": [{
    "logConfiguration": [{
      "logDriver": "splunk",
      "options": {
        "splunk-url": "https://cloud.splunk.com:8080"
      },
      "secretOptions": [{
        "name": "splunk-token",
        "valueFrom": "arn:aws:secretsmanager:region:aws_account_id:secret:secret_name-AbCdEf"
      }]
    }]
  }]
}
```
### Creating an AWS Secrets Manager Secret

You can use the Secrets Manager console to create a secret for your sensitive data. For more information, see Creating a Basic Secret in the AWS Secrets Manager User Guide.

**To create a basic secret**

Use Secrets Manager to create a secret for your sensitive data.

 1.  Open the Secrets Manager console at https://console.aws.amazon.com/secretsmanager/

      1.  Choose **Store a new secret.**

   1.   For **Select secret type,** choose **Other type of secrets.**

   2.  Specify the details of your custom secret as **Key** and **Value** pairs. For example, you can specify a key of UserName, and then supply the appropriate user name as its value. Add a second key with the name of Password and the password text as its value. You could also add entries for Database name, Server address, TCP port, and so on. You can add as many pairs as you need to store the information you require.
  
Alternatively, you can choose the **Plaintext** tab and enter the secret value in any way you like.
       4.  Choose the AWS KMS encryption key that you want to use to encrypt the protected text in the secret. If you don't choose one, Secrets Manager checks to see if there's a default key for the account, and uses it if it exists. If a default key doesn't exist, Secrets Manager creates one for you automatically. You can also choose **Add new key** to create a custom CMK specifically for this secret. To create your own AWS KMS CMK, you must have permissions to create CMKs in your account.
       5.  Choose **Next.** 
       6.  For **Secret name,** type an optional path and name, such as production/MyAwesomeAppSecret or development/TestSecret, and choose **Next.** You can optionally add a description to help you remember the purpose of this secret later.
    The secret name must be ASCII letters, digits, or any of the following characters: /_+=.@-
       7.  (Optional) At this point, you can configure rotation for your secret. For this procedure, leave it at **Disable automatic rotation** and choose **Next.**
       8.  Review your settings, and then choose **Store secret** to save everything you entered as a new secret in Secrets Manager.

### Creating a Task Definition that References Sensitive Data

You can use the Amazon ECS console to create a task definition that references an Secrets Manager secret.

**To create a task definition that specifies a secret**

1.   Open the Amazon ECS console at https://console.aws.amazon.com/ecs/
1.  In the navigation pane, choose **Task Definitions,** **Create new Task Definition.**
1.  On the **Select launch type compatibility page,** choose the launch type for your tasks and choose **Next step.**
Note

This step only applies to Regions that currently support Amazon ECS using AWS Fargate. 

1.  For **Task Definition Name,** type a name for your task definition. Up to 255 letters (uppercase and lowercase), numbers, hyphens, and underscores are allowed.

1.  For **Task execution role,** either select your existing task execution role or choose **Create new role** to have one created for you. This role authorizes Amazon ECS to pull private images for your task. 

**Important**

If the **Task execution role** field does not appear, choose **Configure via JSON** and manually add the executionRoleArn field to specify your task execution role. The following code shows the syntax:

```
"executionRoleArn": "arn:aws:iam::aws_account_id:role/ecsTaskExecutionRole"
```

6.  For each container to create in your task definition, complete the following steps:
  a.  Under **Container Definitions,** choose **Add container**.
  b.  For **Container name,** type a name for your container. Up to 255 letters (uppercase and lowercase), numbers, hyphens, and underscores are allowed.
  c.  For **Image,** type the image name or path to your private image. Up to 255 letters (uppercase and lowercase), numbers, hyphens, and underscores are allowed.
  d.  Expand **Advanced container configuration.**
  e. For sensitive data to inject as environment variables, under **Environment,** for      **Environment variables,** complete the following fields:
    1.  For **Key,** enter the name of the environment variable to set in the container. This corresponds to the name field in the **secrets** section of a container definition.
    1.   For **Value,** choose **ValueFrom.** For **Add value,** enter the ARN of the Secrets Manager secret that contains the data to present to your container as an environment variable.

   f.  For sensitive data referenced in the log configuration for a container, under **Storage and Logging,** for **Log configuration,** complete the following fields:
      i.  Clear the **Auto-configure CloudWatch Logs** option.
      ii.  Under **Log options,** for **Key,** enter the name of the log configuration option to set.
      iii.  For **Value,** choose **ValueFrom.** For **Add value,** enter the full ARN of the Secrets Manager secret that contains the data to present to your log configuration as a log option.
  g.  Fill out the remaining required fields and any optional fields to use in your container definitions. More container definition parameters are available in the **Advanced container configuration** menu. 
  h. Choose **Add.**

7.  When your containers are added, choose **Create.**

## 6. Using the awslogs Log Driver
Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch.|

**Why?**

You need need use awslogs log driver in order send all the logs from Fargae and EC2 Instance type to centralized logging location. These logs are used are used but the CG Security Engineering Teams to detect malicious activity.

**How?**

You can configure the containers in your tasks to send log information to CloudWatch Logs\. If you are using the Fargate launch type for your tasks, this allows you to view the logs from your containers\. If you are using the EC2 launch type, this enables you to view different logs from your containers in one convenient location, and it prevents your container logs from taking up disk space on your container instances\. This topic helps you get started using the `awslogs` log driver in your task definitions\.

**Note**  
The type of information that is logged by the containers in your task depends mostly on their `ENTRYPOINT` command\. By default, the logs that are captured show the command output that you would normally see in an interactive terminal if you ran the container locally, which are the `STDOUT` and `STDERR` I/O streams\. The `awslogs` log driver simply passes these logs from Docker to CloudWatch Logs\. For more information on how Docker logs are processed, including alternative ways to capture different file data or streams, see [View logs for a container or service](https://docs.docker.com/config/containers/logging/) in the Docker documentation\.

For more information about CloudWatch Logs, see [Monitoring Log Files](https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/WhatIsCloudWatchLogs.html) in the *Amazon CloudWatch User Guide*\.

### Enabling the awslogs Log Driver for Your Containers

If you are using the Fargate launch type for your tasks, all you need to do to enable the `awslogs` log driver is add the required `logConfiguration` parameters to your task definition\.

If you are using the EC2 launch type for your tasks and want to enable the `awslogs` log driver, your Amazon ECS container instances require at least version 1\.9\.0 of the container agent\. 

**Note**  
If you are not using the Amazon ECS\-optimized AMI \(with at least version 1\.9\.0\-1 of the `ecs-init` package\) for your container instances, you also need to specify that the `awslogs` logging driver is available on the container instance when you start the agent by using the following environment variable in your docker run statement or environment variable file\.  

```
ECS_AVAILABLE_LOGGING_DRIVERS='["json-file","awslogs"]'
```

Your Amazon ECS container instances also require `logs:CreateLogStream` and `logs:PutLogEvents` permission on the IAM role with which you launch your container instances\. If you created your Amazon ECS container instance role before `awslogs` log driver support was enabled in Amazon ECS, then you might need to add this permission\. If your container instances use the managed IAM policy for container instances, then your container instances should have the correct permissions\. 

### Creating a Log Group

The `awslogs` log driver can send log streams to an existing log group in CloudWatch Logs or it can create a new log group on your behalf\. The AWS Management Console provides an auto\-configure option which creates a log group on your behalf using the task definition family name with `ecs` as the prefix\. Alternatively, you can manually specify your log configuration options and specify the `awslogs-create-group` option with a value of `true` which will create the log groups on your behalf\.

**Note**  
To use the `awslogs-create-group` option to have your log group created, your IAM policy must include the `logs:CreateLogGroup` permission\.

### Using the Auto\-configuration Feature to Create a Log Group

When registering a task definition in the Amazon ECS console, you have the option to allow Amazon ECS to auto\-configure your CloudWatch logs\. This option creates a log group on your behalf using the task definition family name with `ecs` as the prefix\.

**To use log group auto\-configuration option in the Amazon ECS console**

1. Open the Amazon ECS console at [https://console\.aws\.amazon\.com/ecs/](https://console.aws.amazon.com/ecs/)\.
2. In the left navigation pane, choose **Task Definitions**, **Create new Task Definition**\.
3. Select your compatibility option and choose **Next Step**\.
4. Choose **Add container**\.
5. In the **Storage and Logging** section, for **Log configuration**, choose **Auto\-configure CloudWatch Logs**\.
6. Enter your awslogs log driver options\. For more information, see [Specifying a Log Configuration in your Task Definition](#specify-log-config)\.
7. Complete the rest of the task definition wizard\.

### Available awslogs Log Driver Options

The `awslogs` log driver supports the following options in Amazon ECS task definitions\. For more information, see [CloudWatch Logs logging driver](https://docs.docker.com/config/containers/logging/awslogs/)\.

`awslogs-create-group`  
Required: No  
Specify whether you want the log group automatically created\. If this option is not specified, it defaults to `false`\.  
Your IAM policy must include the `logs:CreateLogGroup` permission before you attempt to use `awslogs-create-group`\.

`awslogs-region`  
Required: Yes  
Specify the region to which the `awslogs` log driver should send your Docker logs\. You can choose to send all of your logs from clusters in different regions to a single region in CloudWatch Logs so that they are all visible in one location, or you can separate them by region for more granularity\. Be sure that the specified log group exists in the region that you specify with this option\.

`awslogs-group`  
Required: Yes  
You must specify a log group to which the `awslogs` log driver sends its log streams\. For more information, see [Creating a Log Group](#create_awslogs_loggroups)\.

`awslogs-stream-prefix`  
Required: Optional for the EC2 launch type, required for the Fargate launch type\.  
The `awslogs-stream-prefix` option allows you to associate a log stream with the specified prefix, the container name, and the ID of the Amazon ECS task to which the container belongs\. If you specify a prefix with this option, then the log stream takes the following format:  

```
prefix-name/container-name/ecs-task-id
```
If you do not specify a prefix with this option, then the log stream is named after the container ID that is assigned by the Docker daemon on the container instance\. Because it is difficult to trace logs back to the container that sent them with just the Docker container ID \(which is only available on the container instance\), we recommend that you specify a prefix with this option\.  
For Amazon ECS services, you could use the service name as the prefix, which would allow you to trace log streams to the service that the container belongs to, the name of the container that sent them, and the ID of the task to which the container belongs\.  
You must specify a stream\-prefix for your logs in order to have your logs appear in the Log pane when using the Amazon ECS console\.

`awslogs-datetime-format`  
Required: No  
This option defines a multiline start pattern in Python `strftime` format\. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern\. Thus the matched line is the delimiter between log messages\.  
One example of a use case for using this format is for parsing output such as a stack dump, which might otherwise be logged in multiple entries\. The correct pattern allows it to be captured in a single entry\.  
For more information, see [awslogs\-datetime\-format](https://docs.docker.com/config/containers/logging/awslogs/#awslogs-datetime-format)\.  
This option always takes precedence if both `awslogs-datetime-format` and `awslogs-multiline-pattern` are configured\.  
Multiline logging performs regular expression parsing and matching of all log messages, which may have a negative impact on logging performance\.

`awslogs-multiline-pattern`  
Required: No  
This option defines a multiline start pattern using a regular expression\. A log message consists of a line that matches the pattern and any following lines that don’t match the pattern\. Thus the matched line is the delimiter between log messages\.  
For more information, see [awslogs\-multiline\-pattern](https://docs.docker.com/config/containers/logging/awslogs/#awslogs-multiline-pattern)\.  
This option is ignored if `awslogs-datetime-format` is also configured\.  
Multiline logging performs regular expression parsing and matching of all log messages\. This may have a negative impact on logging performance\.

### Specifying a Log Configuration in your Task Definition

Before your containers can send logs to CloudWatch, you must specify the `awslogs` log driver for containers in your task definition\. This section describes the log configuration for a container to use the `awslogs` log driver\. 

The task definition JSON shown below has a `logConfiguration` object specified for each container; one for the WordPress container that sends logs to a log group called `awslogs-wordpress`, and one for a MySQL container that sends logs to a log group called `awslogs-mysql`\. Both containers use the `awslogs-example` log stream prefix\.

```
{
    "containerDefinitions": [
        {
            "name": "wordpress",
            "links": [
                "mysql"
            ],
            "image": "wordpress",
            "essential": true,
            "portMappings": [
                {
                    "containerPort": 80,
                    "hostPort": 80
                }
            ],
            "logConfiguration": {
                "logDriver": "awslogs",
                "options": {
                    "awslogs-group": "awslogs-wordpress",
                    "awslogs-region": "us-west-2",
                    "awslogs-stream-prefix": "awslogs-example"
                }
            },
            "memory": 500,
            "cpu": 10
        },
        {
            "environment": [
                {
                    "name": "MYSQL_ROOT_PASSWORD",
                    "value": "password"
                }
            ],
            "name": "mysql",
            "image": "mysql",
            "cpu": 10,
            "memory": 500,
            "essential": true,
            "logConfiguration": {
                "logDriver": "awslogs",
                "options": {
                    "awslogs-group": "awslogs-mysql",
                    "awslogs-region": "us-west-2",
                    "awslogs-stream-prefix": "awslogs-example"
                }
            }
        }
    ],
    "family": "awslogs-example"
}
```

In the Amazon ECS console, the log configuration for the `wordpress` container is specified as shown in the image below\. 

![\[Console log configuration\]](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/images/awslogs-console-config.png)

After you have registered a task definition with the `awslogs` log driver in a container definition log configuration, you can run a task or create a service with that task definition to start sending logs to CloudWatch Logs\. For more information, see [Running Tasks](ecs_run_task.md) and [Creating a service](create-service.md)\.

### Viewing awslogs Container Logs in CloudWatch Logs

For tasks using the EC2 launch type, after your container instance role has the proper permissions to send logs to CloudWatch Logs, your container agents are updated to at least version 1\.9\.0, and you have configured and started a task with containers that use the `awslogs` log driver, your configured containers should be sending their log data to CloudWatch Logs\. You can view and search these logs in the console\.

**To view your CloudWatch Logs data for a container from the Amazon ECS console**

1. Open the Amazon ECS console at [https://console\.aws\.amazon\.com/ecs/](https://console.aws.amazon.com/ecs/)\.
1. On the **Clusters** page, select the cluster that contains the task to view\.
1. On the **Cluster: *cluster\_name*** page, choose **Tasks** and select the task to view\.
1. On the **Task: *task\_id*** page, expand the container view by choosing the arrow to the left of the container name\.
1. In the **Log Configuration** section, choose **View logs in CloudWatch**, which opens the associated log stream in the CloudWatch console\.  
![\[Task definition view of log configuration\]](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/images/view_logs_in_cw.png)

**To view your CloudWatch Logs data in the CloudWatch console**

1. Open the CloudWatch console at [https://console\.aws\.amazon\.com/cloudwatch/](https://console.aws.amazon.com/cloudwatch/)\.
1. In the left navigation pane, choose **Logs**\.
1. Select a log group to view\. You should see the log groups that you created in [Creating a Log Group](#create_awslogs_loggroups)\.  
![\[awslogs console metrics view\]](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/images/awslogs-log-groups.png)
1. Choose a log stream to view\.  
![\[awslogs console metrics view\]](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/images/awslogs-log-stream.png) 

## 7. Creating a Trail to log ECS API calls
Follow the procedure to create a trail that applies to all Regions\. A trail that applies to all Regions delivers log files from all Regions to an S3 bucket\. After you create the trail, CloudTrail automatically starts logging the events that you specified\. 

**Note**  
After you create a trail, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs\. 

**Contents**
+ [Creating a Trail in the Console](#creating-a-trail-in-the-console)


### Creating a Trail in the Console

 You can configure your trail for the following: 
+ Specify if you want the trail to apply to all Regions or a single Region\.
+ Specify an Amazon S3 bucket to receive log files\.
+ For management and data events, specify if you want to log read\-only, write\-only, or all events\.

**To create a CloudTrail trail with the AWS Management Console**

1. Sign in to the AWS Management Console and open the CloudTrail console at [https://console\.aws\.amazon\.com/cloudtrail/](https://console.aws.amazon.com/cloudtrail/)\.

1. Choose the AWS Region where you want the trail to be created\.

1. Choose **Get Started Now**\.
**Tip**  
If you do not see **Get Started Now**, choose **Trails**, and then choose **Create trail**\.

1. On the **Create Trail** page, for **Trail name**, type a name for your trail\. 

1. For **Apply trail to all regions**, choose **Yes** to receive log files from all Regions\. This is the default and recommended setting\. If you choose **No**, the trail logs files only from the Region in which you create the trail\.

1. For **Management events**, do the following\.

   1. For **Read/Write events**, choose if you want your trail to log **All**, **Read\-only**, **Write\-only**, or **None**, and then choose **Save**\. By default, trails log all management events\. 

   1. For **Log AWS KMS events**, choose **Yes** to log AWS Key Management Service \(AWS KMS\) events in your trail\. Choose **No** to filter AWS KMS events out of your trail\. The default setting is **Yes**\.

1. In **Insights events**, for **Log Insights events**, choose **Yes** if you want your trail to log Insights events\. By default, trails don't log Insights events\.  Additional charges apply for logging Insights events\. For CloudTrail pricing, see [AWS CloudTrail Pricing](https://aws.amazon.com/cloudtrail/pricing/)\.

   Insights events are delivered to a different folder named `/CloudTrail-Insight`of the same S3 bucket that is specified in the **Storage location** area of the trail details page\. CloudTrail creates the new prefix for you\. For example, if your current destination S3 bucket is named `S3bucketName/AWSLogs/CloudTrail/`, the S3 bucket name with a new prefix is named `S3bucketName/AWSLogs/CloudTrail-Insight/`\.

1. For **Data events**, you can specify logging data events for Amazon S3 buckets, for AWS Lambda functions, or both\. By default, trails don't log data events\. Additional charges apply for logging data events\. For CloudTrail pricing, see [AWS CloudTrail Pricing](https://aws.amazon.com/cloudtrail/pricing/)\.

   You can select the option to log all S3 buckets and Lambda functions, or you can specify individual buckets or functions\. 

   For Amazon S3 buckets:
   + Choose the **S3** tab\.
   + To specify a bucket, choose **Add S3 bucket**\. Type the S3 bucket name and prefix \(optional\) for which you want to log data events\. For each bucket, specify whether you want to log **Read** events, such as `GetObject`, **Write** events, such as `PutObject`, or both\. 
   + To log data events for all S3 buckets in your AWS account, select **Select all S3 buckets in your account**\. Then choose whether you want to log **Read** events, such as `GetObject`, **Write** events, such as `PutObject`, or both\. This setting takes precedence over individual settings you configure for individual buckets\. For example, if you specify logging **Read** events for all S3 buckets, and then choose to add a specific bucket for data event logging, **Read** is already selected for the bucket you added\. You cannot clear the selection\. You can only configure the option for **Write**\. 
**Note**  
Selecting the **Select all S3 buckets in your account** option enables data event logging for all buckets currently in your AWS account and any buckets you create after you finish creating the trail\. It also enables logging of data event activity performed by any user or role in your AWS account, even if that activity is performed on a bucket that belongs to another AWS account\.  
If the trail applies only to one Region, selecting the **Select all S3 buckets in your account** option enables data event logging for all buckets in the same Region as your trail and any buckets you create later in that Region\. It will not log data events for Amazon S3 buckets in other Regions in your AWS account\.

   For Lambda functions:
   + Choose the **Lambda** tab\.
   + To specify logging individual functions, select them from the list\. 
**Note**  
If you have more than 15,000 Lambda functions in your account, you cannot view or select all functions in the CloudTrail console when creating a trail\. You can still select the option to log all functions, even if they are not displayed\. If you want to log data events for specific functions, you can manually add a function if you know its ARN\. You can also finish creating the trail in the console, and then use the AWS CLI and the put\-event\-selectors command to configure data event logging for specific Lambda functions\. 

   + To log data events for all Lambda functions in your AWS account, select **Log all current and future functions**\. This setting takes precedence over individual settings you configure for individual functions\. All functions are logged, even if all functions are not displayed\.
**Note**  
If you are creating a trail for all Regions, this selection enables data event logging for all functions currently in your AWS account, and any Lambda functions you might create in any Region after you finish creating the trail\. If you are creating a trail for a single Region, this selection enables data event logging for all functions currently in that Region in your AWS account, and any Lambda functions you might create in that Region after you finish creating the trail\. It does not enable data event logging for Lambda functions created in other Regions\.  
Logging data events for all functions also enables logging of data event activity performed by any user or role in your AWS account, even if that activity is performed on a function that belongs to another AWS account\.

1. For **Storage location**, for **Create a new S3 bucket**, choose **Yes** to create a bucket\. When you create a bucket, CloudTrail creates and applies the required bucket policies\.
**Note**  
If you chose **No**, choose an existing S3 bucket\. The bucket policy must grant CloudTrail permission to write to it\. 

1. For **S3 bucket**, type a name for the bucket you want to designate for log file storage\. The name must be globally unique\. 

1. For **Tags**, add one or more custom tags \(key\-value pairs\) to your trail\. Tags can help you identify both your CloudTrail trails and the Amazon S3 buckets that contain CloudTrail log files\. You can then use resource groups for your CloudTrail resources\. For more information, see [AWS Resource Groups](https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html) 

1. To configure advanced settings, see [Configuring Advanced Settings for Your Trail](#advanced-settings-for-your-trail)\. Otherwise, choose **Create**\.

1. The new trail appears on the **Trails** page\. The **Trails** page shows the trails in your account from all Regions\. In about 15 minutes, CloudTrail publishes log files that show the AWS API calls made in your account\. You can see the log files in the S3 bucket that you specified\. It can take up to 36 hours for CloudTrail to deliver the first Insights event, if you have enabled Insights event logging, and unusual activity is detected\.

**Note**  
You can't rename a trail after it has been created\. Instead, you can delete the trail and create a new one\.



## 8. Enable VPC Flow Logs for ECS Cluster VPC EC2 Launch Types Only
Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch.|

**Why?**
You need to use VPC Flow Logs in order send all the logs from Fargate and EC2 Instance type to centralized logging location. These logs are used but the CG Security Engineering Teams to detect malicious activity and threat detection.

### Working with flow logs

You can work with flow logs using the Amazon EC2, Amazon VPC, CloudWatch, and Amazon S3 consoles\.

**Topics**
+ [Controlling the use of flow logs](#controlling-use-of-flow-logs)
+ [Creating a flow log](#create-flow-log)


### Controlling the use of flow logs

By default, IAM users do not have permission to work with flow logs\. You can create an IAM user policy that grants users the permissions to create, describe, and delete flow logs\. For more information, see [Granting IAM Users Required Permissions for Amazon EC2 Resources](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/ec2-api-permissions.html) in the *Amazon EC2 API Reference*\.

The following is an example policy that grants users full permissions to create, describe, and delete flow logs\.

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ec2:DeleteFlowLogs",
        "ec2:CreateFlowLogs",
        "ec2:DescribeFlowLogs"
      ],
      "Resource": "*"
    }
  ]
}
```

Some additional IAM role and permission configuration is required, depending on whether you're publishing to CloudWatch Logs or Amazon S3\.

### Creating a flow log

You can create flow logs for your VPCs, subnets, or network interfaces\. Flow logs can publish data to CloudWatch Logs or Amazon S3\.

For more information, see [Creating a flow log that publishes to CloudWatch Logs](flow-logs-cwl.md#flow-logs-cwl-create-flow-log) and [Creating a flow log that publishes to Amazon S3](flow-logs-s3.md#flow-logs-s3-create-flow-log)\.


## 9. Scan images for Vulnerabilities
Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch.|

**Why?**

All images deployed in the ECS Cluster must be scanned for identiying vulnerabilites. Similar to their virtual machine counterparts, container images can contain binaries and application libraries with vulnerabilities or develop vulnerabilities over time. The best way to safeguard against exploits is by regularly scanning your images with an image scanner. 

**How?**

All the Teams running ECS Clusters should have a Twistlock agent running in the cluster, so that images deployed in the cluster are scanned by Twistlock Scanner (Prisma Cloud). Please contact PDS team if you dont have twistlock agent running in the Cluster.


## 10. Remove special permissions from images
Capital Group:
|Control Statement|Description|
|------|----------------------|
|Need to be updated|Need to be updated|

**Why?**

All the images which are getting deployed in the ECS Cluster should not have elevated, privilages and special permissions

**How?**

The access rights flags setuid and setgid allow running an executable with the permissions of the owner or group of the executable. Remove all binaries with these access rights from your image as these binaries can be used to escalate privileges. Consider removing all shells and utilities like nc and curl that can be used for malicious purposes. You can find the files with setuid and setgid access rights by using the following command.

```
 find / -perm /6000 -type f -exec ls -ld {} \;
```
To remove these special permissions from these files, add the following directive to your container image.

```
RUN find / -xdev -perm /6000 -type f -exec chmod a-s {} \; || true
```


## 11. Run containers as non-root users
Capital Group:
|Control Statement|Description|
|------|----------------------|
||Need to be updated|Need to be updated|

**Why?**

You should run containers as a non-root user. By default, containers run as the root user unless the USER directive is included in your Dockerfile. The default Linux capabilities that are assigned by Docker restrict the actions that can be run as root, but only marginally. For example, a container running as root is still not allowed to access devices.

As part of your CI/CD pipeline you should lint Dockerfiles to look for the USER directive and fail the build if it's missing. For more information, see the following topics:

+ [Dockerfile-lint](https://github.com/projectatomic/dockerfile_lint) is an open-source tool from RedHat that can be used to check if the file conforms to best practices.

+ [Hadolint](https://github.com/hadolint/hadolint) is another tool for building Docker images that conform to best practices.


## 12. Use a read-only root file system
Capital Group:
|Control Statement|Description|
|------|----------------------|
|Need to be updated|Need to be updated|

**Why?**

You should use a read-only root file system. A container's root file system is writable by default. When you configure a container with a RO (read-only) root file system it forces you to explicitly define where data can be persisted. This reduces your attack surface because the container's file system can't be written to unless permissions are specifically granted.


## 13. ECS data in transit must enforce TLS with version 1.2 or higher
Capital Group:
|Control Statement|Description|
|------|----------------------|
|Need to be updated|Need to be updated|

**Why?**

Encrypting network traffic prevents unauthorized users from intercepting and reading data when that data is transmitted across a network. All the data in trasit must be encrypted and enforce a TLS version of 1.2 or more.

**How?**

With Amazon ECS, network encryption can be implemented in any of the following ways.


## 14. Make sure ECS Task network interface does not have public IP address
Capital Group:
|Control Statement|Description|
|------|----------------------|
|Need to be updated|Need to be updated|

**Why?**

## 15. Use always Fargate launch type in ECS Cluster
Capital Group:
|Control Statement|Description|
|------|----------------------|
|Need to be updated|Need to be updated|

**Why?**




## Operational Best Practices


## 1. Utilizing AWS CloudWatch Container Insights

### Setting Up Container Insights on Amazon ECS for Cluster\- and Service\-Level Metrics

You can enable Container Insights on new and existing Amazon ECS clusters\. Container Insights collects metrics at the cluster, task, and service levels\. For existing clusters, you use the AWS CLI\. For new clusters, use either the Amazon ECS console or the AWS CLI\.

If you're using Amazon ECS on an Amazon EC2 instance, and you want to collect network and storage metrics from Container Insights, launch that instance using an AMI that includes Amazon ECS agent version 1\.29\. For information about updating your agent version, see [Updating the Amazon ECS Container Agent](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-update.html)

You can use the AWS CLI to set account\-level permission to enable Container Insights for any new Amazon ECS clusters created in your account\. To do so, enter the following command\.

```
aws ecs put-account-setting --name "containerInsights" --value "enabled"
```

### Setting Up Container Insights on Existing Amazon ECS Clusters

To enable Container Insights on an existing Amazon ECS cluster, enter the following command\. You must be running version 1\.16\.200 or later of the AWS CLI for the following command to work\.

```
aws ecs update-cluster-settings --cluster myCICluster --settings name=containerInsights,value=enabled
```

### Setting Up Container Insights on New Amazon ECS Clusters

There are two ways to enable Container Insights on new Amazon ECS clusters\. You can configure Amazon ECS so that all new clusters are enabled for Container Insights by default\. Otherwise, you can enable a new cluster when you create it\.

### Using the AWS Management Console

You can enable Container Insights on all new clusters by default, or on an individual cluster as you create it\.

**To enable Container Insights on all new clusters by default**

1. Open the Amazon ECS console at [https://console\.aws\.amazon\.com/ecs/](https://console.aws.amazon.com/ecs/)\.

1. In the navigation pane, choose **Account Settings**\.

1. Select the check box at the bottom of the page to enable the Container Insights default\.

If you haven't used the preceding procedure to enable Container Insights on all new clusters by default, use the following steps to create a cluster with Container Insights enabled\.

**To create a cluster with Container Insights enabled**

1. Open the Amazon ECS console at [https://console\.aws\.amazon\.com/ecs/](https://console.aws.amazon.com/ecs/)\.

1. In the navigation pane, choose **Clusters**\.

1. Choose **Create cluster**\.

1. On the next page, do the following:

   1. Name your cluster\.

   1. If you don’t have a VPC already, select the check box to create one\. You can use the default values for the VPC\.

   1. Fill out all other needed information, including instance type\.

   1. Select **Enabled Container Insights**\.

   1. Choose **Create**\.

You can now create task definitions, run tasks, and launch services in the cluster\. For more information, see the following:
+ [Creating a Task Definition](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/create-task-definition.html)
+ [Running Tasks](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_run_task.html)
+ [Creating a Service](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/create-service.html)

### Setting Up Container Insights on New Amazon ECS Clusters Using the AWS CLI

To enable Container Insights on all new clusters by default, enter the following command\.

```
aws ecs put-account-setting --name "containerInsights" --value "enabled"
```

If you didn't use the preceding command to enable Container Insights on all new clusters by default, enter the following command to create a new cluster with Container Insights enabled\. You must be running version 1\.16\.200 or later of the AWS CLI for the following command to work\.

```
aws ecs create-cluster --cluster-name myCICluster --settings "name=containerInsights,value=enabled"
```

### Disabling Container Insights on Amazon ECS Clusters

To disable Container Insights on an existing Amazon ECS cluster, enter the following command\.

```
aws ecs update-cluster-settings --cluster myCICluster --settings name=containerInsights,value=disabled
```
## Respond/Recover


## 2. Running the X\-Ray Daemon
You can run the AWS X\-Ray daemon locally on Linux, MacOS, Windows, or in a Docker container\. Run the daemon to relay trace data to X\-Ray when you are developing and testing your instrumented application\. Download and extract the daemon by using the instructions [here](xray-daemon.md#xray-daemon-downloading)\.

When running locally, the daemon can read credentials from an AWS SDK credentials file \(`.aws/credentials` in your user directory\) or from environment variables\. For more information, see [Giving the Daemon Permission to Send Data to X\-Ray](xray-daemon.md#xray-daemon-permissions)\.

The daemon listens for UDP data on port 2000\. You can change the port and other options by using a configuration file and command line options\. For more information, see [Configuring the AWS X\-Ray Daemon](xray-daemon-configuration.md)\.

### Running the X\-Ray Daemon on Linux

You can run the daemon executable from the command line\. Use the `-o` option to run in local mode, and `-n` to set the region\.

```
~/xray-daemon$ ./xray -o -n us-east-2
```

To run the daemon in the background, use `&`\.

```
~/xray-daemon$ ./xray -o -n us-east-2 &
```

Terminate a daemon process running in the background with `pkill`\.

```
~$ pkill xray
```

### Running the X\-Ray Daemon in a Docker Container

To run the daemon locally in a Docker container, save the following text to a file named `Dockerfile`\. Download the complete [example image](https://hub.docker.com/r/amazon/aws-xray-daemon/) on Docker Hub\.

**Example Dockerfile – Amazon Linux**

```
FROM amazonlinux
RUN yum install -y unzip
RUN curl -o daemon.zip https://s3.dualstack.us-east-2.amazonaws.com/aws-xray-assets.us-east-2/xray-daemon/aws-xray-daemon-linux-3.x.zip
RUN unzip daemon.zip && cp xray /usr/bin/xray
ENTRYPOINT ["/usr/bin/xray", "-t", "0.0.0.0:2000", "-b", "0.0.0.0:2000"]
EXPOSE 2000/udp
EXPOSE 2000/tcp
```

Build the container image with `docker build`\.

```
~/xray-daemon$ docker build -t xray-daemon .
```

Run the image in a container with `docker run`\.

```
~/xray-daemon$ docker run \
      --attach STDOUT \
      -v ~/.aws/:/root/.aws/:ro \
      --net=host \
      -e AWS_REGION=us-east-2 \
      --name xray-daemon \
      -p 2000:2000/udp \
      xray-daemon -o
```

This command uses the following options:
+ `--attach STDOUT` – View output from the daemon in the terminal\.
+ `-v ~/.aws/:/root/.aws/:ro` – Give the container read\-only access to the `.aws` directory to let it read your AWS SDK credentials\.
+ `AWS_REGION=us-east-2` – Set the `AWS_REGION` environment variable to tell the daemon which region to use\.
+ `--net=host` – Attach the container to the `host` network\. Containers on the host network can communicate with each other without publishing ports\.
+ `-p 2000:2000/udp` – Map UDP port 2000 on your machine to the same port on the container\. This is not required for containers on the same network to communicate, but it does let you send segments to the daemon [from the command line](xray-api-sendingdata.md#xray-api-daemon) or from an application not running in Docker\.
+ `--name xray-daemon` – Name the container `xray-daemon` instead of generating a random name\.
+ `-o` \(after the image name\) – Append the `-o` option to the entry point that runs the daemon within the container\. This option tells the daemon to run in local mode to prevent it from trying to read Amazon EC2 instance metadata\.

To stop the daemon, use `docker stop`\. If you make changes to the `Dockerfile` and build a new image, you need to delete the existing container before you can create another one with the same name\. Use `docker rm` to delete the container\.

```
$ docker stop xray-daemon
$ docker rm xray-daemon
```

The Scorekeep sample application shows how to use the X\-Ray daemon in a local Docker container\. See [Instrumenting Amazon ECS Applications](scorekeep-ecs.md) for details\.

### Running the X\-Ray Daemon on Windows

You can run the daemon executable from the command line\. Use the `-o` option to run in local mode, and `-n` to set the region\.

```
> .\xray_windows.exe -o -n us-east-2
```

Use a PowerShell script to create and run a service for the daemon\.

**Example PowerShell Script \- Windows**

```
if ( Get-Service "AWSXRayDaemon" -ErrorAction SilentlyContinue ){
    sc.exe stop AWSXRayDaemon
    sc.exe delete AWSXRayDaemon
}
if ( Get-Item -path aws-xray-daemon -ErrorAction SilentlyContinue ) {
    Remove-Item -Recurse -Force aws-xray-daemon
}

$currentLocation = Get-Location
$zipFileName = "aws-xray-daemon-windows-service-3.x.zip"
$zipPath = "$currentLocation\$zipFileName"
$destPath = "$currentLocation\aws-xray-daemon"
$daemonPath = "$destPath\xray.exe"
$daemonLogPath = "C:\inetpub\wwwroot\xray-daemon.log"
$url = "https://s3.dualstack.us-west-2.amazonaws.com/aws-xray-assets.us-west-2/xray-daemon/aws-xray-daemon-windows-service-3.x.zip"

Invoke-WebRequest -Uri $url -OutFile $zipPath
Add-Type -Assembly "System.IO.Compression.Filesystem"
[io.compression.zipfile]::ExtractToDirectory($zipPath, $destPath)

sc.exe create AWSXRayDaemon binPath= "$daemonPath -f $daemonLogPath"
sc.exe start AWSXRayDaemon
```

### Running the X\-Ray Daemon on OS X

You can run the daemon executable from the command line\. Use the `-o` option to run in local mode, and `-n` to set the region\.

```
~/xray-daemon$ ./xray_mac -o -n us-east-2
```

To run the daemon in the background, use `&`\.

```
~/xray-daemon$ ./xray_mac -o -n us-east-2 &
```

Use `nohup` to prevent the daemon from terminating when the terminal is closed\.

```
~/xray-daemon$ nohup ./xray_mac &
```
### 3. ECS Resources are tagged according to CG standards
**Capital Group:** <br>

|Control Statement|Description|
|------|----------------------|
|N/A| No security control currently defined.|

**What, Why & How?**

Tagging resources in the cloud is an easy way for teams to provide information related to who owns the resource, what the resource is used for, as well as other important information related to the deployment lifecycle of the resource. CG has mandated that all cloud resources are to be tagged with certain important for cross-team use. Although most of the mandatory tags will be added through automation, one should still check to make sure that all newly deployed recources have the appropriate tags attached. please see the documentation below for the latest tagging standards.

[CG Cloud Tagging Strategy](https://confluence.capgroup.com/display/HCEA/Resource+Tagging+standards)
<br><br>



## Endnotes
**Resources**<br>
1. https://docs.aws.amazon.com/ecs/index.html
2. https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/security.html
3. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html
4. https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_on_ECS.html
5. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/vpc-endpoints.html
6. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data-secrets.html
7. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data-parameters.html
8. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html#enable_awslogs
9. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/logging-using-cloudtrail.html
10. https://docs.aws.amazon.com/vpc/latest/userguide/working-with-flow-logs.html
11. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/deploy-container-insights-ECS-cluster.html
12. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cloudwatch_event_stream.html
13. https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-local.html
<br><br>

## Capital Group Glossary
**Data** - Digital pieces of information stored or transmitted for use with an information system from which understandable information is derived. Items that could be considered to be data are: Source code, meta-data, build artifacts, information input and output.

**Information System** - An organized assembly of resources and procedures for the collection, processing, maintenance, use, sharing, dissemination, or disposition of information. All systems, platforms, compute instances including and not limited to physical and virtual client endpoints, physical and virtual servers, software containers, databases, Internet of Things (IoT) devices, network devices, applications (internal and external), Serverless computing instances (i.e. AWS Lambda), vendor provided appliances, and third-party platforms, connected to the Capital Group network or used by Capital Group users or customers.

**Log** - a record of the events occurring within information systems and networks. Logs are composed of log entries; each entry contains information related to a specific event that has occurred within a system or network.

**Information** - communication or representation of knowledge such as facts, data, or opinions in any medium or form, including textual, numerical, graphic, cartographic, narrative, or audiovisual.

**Cloud computing** - A model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.

**Vulnerability**  - Weakness in an information system, system security procedures, internal controls, or implementation that could be exploited or triggered by a threat source. Note: The term weakness is synonymous for deficiency. Weakness may result in security and/or privacy risks.
