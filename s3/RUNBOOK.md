<img src="https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png" alt="AWS" width="250"/>

# AWS DynamoDB - Security Runbook <!-- omit in toc -->
## Capgroup Cybersecurity Control Alignment <!-- omit in toc -->

**Generated By:**  
[Josh Linus (JHZL)](https://cgweb3/profile/JHZL)  
Security Engineering

**Last Update:** *08/10/2021*

Table of Contents
- [Disclaimer](#disclaimer)
- [Overview](#overview)
- [Cloud Security Requirements](#cloud-security-requirements)
  - [1. Enforce least privilege for all S3 users and roles](#1-Enforce-least-privilege-for-all-S3-users-and-roles)
  - [2. Buckets are encrypted using CG CMK](#2-Buckets-are-encrypted-using-CG-CMK)
  - [3. Data in Transit is encrypted using TLS 1.2](#3-Data-in-Transit-is-encrypted-using-TLS-1.2)
  - [4. Block Public Access](#4-Block-Public-Access)
  - [5. S3 Utilizes VPC Endpoints to Prevent Public Access](#5-EBS-Utilizes-VPC-Endpoints-to-Prevent-Public-Access)
  - [6. Enable Server Access Logs](#6-Enable-Server-Access-Logs)  
  - [7. Implement Appropriate Backups](#7-Implement-Appropriate-Backups)
  - [8. CloudTrail logging enabled for S3](#8-CloudTrail-logging-enabled-for-S3)
  - [9. CloudWatch alarms enabled for S3](#9-CloudWatch-alarms-enabled-for-S3)
- [Operational Best Practices](#operational-best-practices)
  - [1. Resource Tags](#1-Resource-Tags)
  - [2. Enable AWS Config](#Enable-AWS-Config)
  - [3. Enable AWS Trusted Advisor](#3-Enable-AWS-Trusted-Advisor)
  - [4. Utilize Lifecycle Management](#4-Utilize-Lifecycle-Management)
- [Endnotes](#endnotes)
- [Capital Group Control Statements](#capital-group-control-statements)
- [Glossary](#glossary)


## Overview
Amazon S3 is a core service offered by AWS that provides object storage. It allows you to store and retrieve any amount of data, at any time, from anywhere on the web. It gives any developer access to highly scalable, reliable, fast, and inexpensive data storage infrastructure; one of the foundational components to any platform.

<img src="/docs/img/s3/s3.png" width="800"><br>

### Use Case Examples:
 - Data Lakes
 - Running Cloud Native Applications
 - Backup and Restore Data
 - Archive Data 
 - Webites

<br><br>

## Cloud Security Requirements
### 1. Enforce least privilege for all S3 users and roles 

**Capital Group:** <br>

|Control Statement|Description|
|------|----------------------|
|Control|Control Definition|
|CS0012298 |Access to change cloud identity access and service control policies is restricted to authorized cloud administrative personnel.|
|CS0012299|Access to change cloud resource-based access policies is restricted to authorized personnel.|
|CS0012195|Multi-Factor Authentication (MFA) is enforced for enterprise network access to systems and data.|


<br>

**Why?** 

CG utlizes the least privilege model when using IAM for services. In accordance with CGs principle of least privilege, only admins will need to have full access. Everyone else (specified users, groups of users, or roles) should only have read access to view findings.

When you grant permissions in AWS S3, you decide who is getting what permissions to which Amazon S3 resources. You enable specific actions that you want to allow on those resources. Therefore you should grant only the permissions that are required to perform a task. Implementing least privilege access is fundamental in reducing security risk and the impact that could result from errors or malicious intent.

**How?** 

AWS IAM policies are JSON documents that are used for setting permissions on users, groups, and roles within AWS services. There are many AWS-managed policies available to pick from, or you can create your own policies using the IAM policy builder, or by just writing the JSON policy. Each user and role should have its own set of permissions.

Manage permissions with IAM User Policies, S3 Bucket Policies, and S3 Access Control Lists (ACLs) to ensure appropriateness of access. IAM Roles should be used to manage temporary credentials for applications and AWS Services that need access to S3.  The assigned Role will supply temporary permissions that applications can use when they make calls to other AWS resources.

IAM provides permissions on which API calls can be made by users to the service as well as who/what can access the service at a bucket and object level. Bucket policies and ACLs can be used separately and also in conjunction with IAM to control access to S3 buckets and objects.


|Permission|Description|Needs Admin Privilege to Use
|------|----------------------|-------|
|READ|Allows grantee to list the objects in the bucket.|❌|
|WRITE|Allows grantee to create new objects in the bucket. For the bucket and object owners of existing objects, also allows deletions and overwrites of those objects.|✔️|
|READ_ACP|Allows grantee to read the bucket ACL|✔️|
|WRITE_ACP|Allows grantee to write the ACL for the applicable bucket|✔️|
|FULL_CONTROL|Allows grantee the READ, WRITE, READ_ACP, and WRITE_ACP permissions on the bucket|✔️|

>**NOTE:**  
>Do **NOT** ever use the **"All Users Group"** predefined group. This allows anyone in the world access to the resource and **MUST NOT  BE USED**. 


### Employ MFA for Sensitive S3 Resources

**Why?** 

S3 supports MFA-protected API access, a feature that can enforce multi-factor authentication (MFA) for access to your Amazon S3 resources. Multi-factor authentication provides an extra level of security that you can apply to your AWS environment. It is a security feature that requires users to prove physical possession of an MFA device by providing a valid MFA code. You can require MFA for any requests to access your Amazon S3 resources.

**How?** 

You can enforce the MFA requirement using the `aws:MultiFactorAuthAge` key in a bucket policy. AWS Identity and Access Management (IAM) users can access Amazon S3 resources by using temporary credentials issued by the AWS Security Token Service (AWS STS). You provide the MFA code at the time of the AWS STS request.

The following bucket policy includes two policy statements. One statement allows the `s3:GetObject` permission on a bucket (`awsexamplebucket1`) to everyone. Another statement further restricts access to the `awsexamplebucket1/examplefolder` folder in the bucket by requiring MFA.  
The `Null` condition in the Condition block evaluates to true if the `aws:MultiFactorAuthAge` key value is null, indicating that the temporary security credentials in the request were created without the MFA key.
```json
{
    "Version": "2012-10-17",
    "Id": "123",
    "Statement": [
      {
        "Sid": "",
        "Effect": "Deny",
        "Principal": "*",
        "Action": "s3:*",
        "Resource": "arn:aws:s3:::awsexamplebucket1/examplefolder/*",
        "Condition": { "Null": { "aws:MultiFactorAuthAge": true } }
      },
      {
        "Sid": "",
        "Effect": "Allow",
        "Principal": "*",
        "Action": ["s3:GetObject"],
        "Resource": "arn:aws:s3:::awsexamplebucket1/*"
      }
    ]
 }
```
For more info on MFA Delete: https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html

<br><br>

### 2. Buckets are encrypted using CG CMK

**Capital Group:** <br>

|Control Statement|Description|
|------|----------------------|
|Control|Control Definition|
|CS0012168|Strong encryption key management controls are in place for cloud provider services to protect data at rest.|

<br>

**Why?** 

CG's Cloud Security standards require that we ensure that the AWS services that hold sensitive, critical or any other data are encrypted to fulfill compliance requirements for data-at-rest encryption. The S3 data encryption and decryption is handled transparently once it has been enabled.

**How?**

To enable default encryption on an Amazon S3 bucket:

1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. In the Buckets list, choose the name of the bucket that you want.
3. Choose Properties.
4. Under Default encryption, choose Edit.
5. To enable or disable server-side encryption, choose Enable or Disable.
6. To enable server-side encryption using an Amazon S3-managed key, under Encryption key type, choose Amazon S3 key (SSE-S3).  
For more information about using Amazon S3 server-side encryption to encrypt your data, see Protecting data using server-side encryption with Amazon S3-managed encryption keys (SSE-S3).
7. To enable server-side encryption using an AWS KMS CMK, follow these steps:
    - Under Encryption key type, choose AWS Key Management Service key (SSE-KMS).
    >**NOTE:**  
    >If you use the AWS KMS option for your default encryption configuration, you are subject to the RPS (requests per second) limits of AWS KMS.
    - Under AWS KMS key choose one of the following:
      - AWS managed key (aws/s3)
      - Choose from your KMS master keys, and choose your KMS master key.
      - Enter KMS master key ARN, and enter your AWS KMS key ARN.    
    > **NOTE:**
    > You can only use KMS CMKs that are enabled in the same AWS Region as the bucket. When you choose Choose from your KMS master keys, the S3 console only lists 100 KMS CMKs per Region. If you have more than 100 CMKs in the same Region, you can only see the first 100 CMKs in the S3 console. To use a KMS CMK that is not listed in the console, choose Custom KMS ARN, and enter the KMS CMK ARN.
    >
    > When you use an AWS KMS CMK for server-side encryption in Amazon S3, you must choose a symmetric CMK. Amazon S3 only supports symmetric CMKs and not asymmetric CMKs.  
8. To use S3 Bucket Keys, under Bucket Key, choose Enable.  
When you configure your bucket to use default encryption with SSE-KMS, you can also enable S3 Bucket Key. S3 Bucket Keys decrease request traffic from Amazon S3 to AWS KMS and lower the cost of encryption.
9. Choose Save changes.

<br><br>

### 3. Data in Transit is encrypted using TLS 1.2

**Capital Group:** <br>

|Control Statement|Description|
|------|----------------------|
|CS0012261|Cloud based data in transit must be encrypted with enterprise approved algorithms.|

<br>

**Why?**   

TLS 1.2 and above is the standard when it comes to network security. CG requirements also need a service to support at least TLS1.2. 

**How?**    

As of March 31, 2021, AWS updated all AWS Federal Information Processing Standard (FIPS) endpoints to a minimum Transport Layer Security (TLS) version TLS 1.2. (TLS 1.0 and 1.1 will be deprecated)

AWS CLI version 2 uses an internal Python script that's compiled to use a minimum of TLS 1.2 when the service it's talking to supports it. No further steps are needed to enforce this minimum. 

More info on enforcing TLS: https://docs.aws.amazon.com/cli/latest/userguide/cli-security-enforcing-tls.html

>**NOTE:**  
>All communications with the S3 API are encrypted using TLS, but file transfers to/from S3 buckets do not require secure transport by default. This can be changed, using S3 bucket policies.
>  
>The following statement uses the Bool condition operator with the `aws:SecureTransport` key to specify that the request must use Secure Transport.
```json
{
  "Version": "2012-10-17",
  "Statement": {
    "Effect": "Allow",
    "Action": "s3:*",
    "Resource": "arn:aws:s3:::awsexamplebucket1/*",
    "Condition": {"Bool": {"aws:SecureTransport": "true"}}
  }
}
```
<br><br>

### 4. Block Public Access

**Capital Group:** <br>

|Control Statement|Description|
|------|----------------------|
|Control|Control Definition|
|[CS0012300](https://capitalgroup.service-now.com/cg_grc?sys_id=80df48c01bac20506a50beef034bcb47&table=sn_compliance_policy_statement&id=cg_grc_action_item_details&view=sp)|Cloud products and services must be deployed on private subnets and public access must be disabled for these services.|

<br>

**Why?** 

CG's Cloud Security standards require that we ensure that the AWS services do not have public access. S3 buckets come with a permission that can change this. By default, new buckets, access points, and objects don't allow public access. However, it is good practice to manually check to make sure.

**How?**

To edit block public access settings for all the S3 buckets in an AWS account:

1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. Choose Account settings for Block Public Access.
3. Choose Edit to change the block public access settings for all the buckets in your AWS account.
4. Choose the settings that you want to change. Make sure the 'public access block' setting is applied and then choose Save changes.
5. When you're asked for confirmation, enter confirm. Then choose Confirm to save your changes.

For more info on blocking public access specific buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/configuring-block-public-access-bucket.html

<br><br>

### 5. S3 Utilizes VPC Endpoints to Prevent Public Access

**Capital Group:** <br>

|Control Statement|Description|
|------|----------------------|
|Control|Control Definition|
|[CS0012300](https://capitalgroup.service-now.com/cg_grc?sys_id=80df48c01bac20506a50beef034bcb47&table=sn_compliance_policy_statement&id=cg_grc_action_item_details&view=sp)|Cloud products and services must be deployed on private subnets and public access must be disabled for these services.|

<br>

**Why?**

VPC Controls provide reasonable assurance that direct access (with provided credentials) is not allowed to the AWS environment from outside the corporate network or AWS backbone.

A VPC endpoint for S3 enables buckets in your VPC to use their private IP addresses to access S3 with no exposure to the public internet. You use endpoint policies to control access to S3. Traffic between your VPC and the AWS service does not leave the Amazon network. Due to the possibility of sensitive data being stored and processed through S3, we need to make sure that this traffic is not transmitted directly over the Public Internet.

When you create a VPC endpoint for S3, any requests to a S3 endpoint within the Region (for example, S3.us-west-2.amazonaws.com) are routed to a private S3 endpoint within the Amazon network. You don't need to modify your applications running on S3 in your VPC. The endpoint name remains the same, but the route to S3 stays entirely within the Amazon network, and does not access the public internet.

**How?**

1. Open the Amazon VPC console.
2.    Using the Region selector in the navigation bar, set the AWS Region to the same Region as your VPC.
3.    From the navigation pane, choose Endpoints.
4.    Choose Create Endpoint.
5.    For Service category, verify that "AWS services" is selected.
6.    For Service Name, select the "s3" service name and "Gateway" type. For example, the service name in the US East (N. Virginia) Region is com.amazonaws.us-east-1.s3.
7.    For VPC, select your VPC.
8.    For Configure route tables, select the route tables based on the associated subnets that you want to be able to access the endpoint from.
9.    For Policy, verify that Full Access is selected.
10.    Choose Create endpoint.
11.    Note the VPC Endpoint ID. You'll need this endpoint ID for a later step.

<br><br>

### 6. Enable Server Access Logs 

**Capital Group:** <br>

|Control Statement|Description|
|------|----------------------|
|Control|Control Definition|
|CS0012142|Backups must adhere to enterprise backup and retention requirements.|

<br>

**Why?** 

Server access logging provides detailed records of the requests that are made to a bucket. Server access logs can assist you in security and access audits, help you learn about your customer base, and understand your Amazon S3 bill.

**How?** 
1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. In the Buckets list, choose the name of the bucket that you want to enable server access logging for.
3. Choose Properties.
4. In the Server access logging section, choose Edit.
5. Under Server access logging, select Enable.
6. For Target bucket, enter the name of the bucket that you want to receive the log record objects.  
The target bucket must be in the same Region as the source bucket and must not have a default retention period configuration.
7. Choose Save changes.
When you enable logging on a bucket, the console both enables logging on the source bucket and adds a grant in the target bucket's access control list (ACL) granting write permission to the Log Delivery group.       
You can view the logs in the target bucket. After you enable server access logging, it might take a few hours before the logs are delivered to the target bucket.

For more info on logging server access, [click here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html)   
For more information on properties, see [Viewing the properties for an S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/view-bucket-properties.html).

<br><br>

### 7. Implement Appropriate Backups

**Capital Group:** <br>

|Control Statement|Description|
|------|----------------------|
|Control|Control Definition|
|CS0012142|Backups must adhere to enterprise backup and retention requirements.|

<br>

**What, Why & How?**

Incorporating backups into your services increases the resilence, and  reliability of the data being used. Backups allow you keep you data safe, and protect you from unintented failures or events. In S3 the two main ways we require you to do this is with 'S3 Versioning' and 'S3 Cross-Region Replication'.

**Versioning** - Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your S3 bucket. You can easily recover from both unintended user actions and application failures. 

1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. In the Buckets list, choose the name of the bucket that you want to enable versioning for.
3. Choose Properties.
4. Under Bucket Versioning, choose Edit.
5. Choose Suspend or Enable, and then choose Save changes.

**Cross-Region Replication** - Although Amazon S3 stores your data across multiple geographically diverse Availability Zones by default, compliance requirements might dictate that you store data at even greater distances. Cross-region replication (CRR) allows you to replicate data between distant AWS Regions to help satisfy these requirements.

The following [guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough1.html) will walk you through how to set up replication and what each permission/setting means: https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough1.html

<br><br>

### 8. CloudTrail logging enabled for S3  

**Capital Group:** <br>

|Control Statement|Description|
|------|----------------------|
|Control|Control Definition|

<br>

**What, Why & How?**  

S3 is integrated with Amazon CloudTrail, a service that provides a record of actions taken by a user, role, or an Amazon service in S3. CloudTrail captures all API calls for S3 as events. Using the information collected by CloudTrail, you can determine the request that was made to S3, the IP address from which the request was made, who made the request, when it was made, and additional details.

- A `default trail` should have been enabled through automation to allow for the continuous delivery of CloudTrail events to an Amazon Simple Storage Service (Amazon S3) bucket, including events for S3 itself. This will enable the forwarding of logs into Splunk for long term archival and reporting.

More info on monitoring and CloudTrail Events: https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging.html

<br><br>

### 9. CloudWatch alarms enabled for S3

**Capital Group:** <br>

|Control Statement|Description|
|------|----------------------|
|Control |Control Definition|

<br>

**What, Why & How?**  
AWS S3 Service allows for the collection of CloudWatch Events, Logs and Alarms. At least one these these tools must be used. Uses include daily storage metrics for buckets, request metrics, and replication metrics.

 - **Amazon CloudWatch Alarms** – Watch a single metric over a time period that you specify, and perform one or more actions based on the value of the metric relative to a given threshold over a number of time periods.
 - **Amazon CloudWatch Logs** – Monitor, store, and access your log files from Amazon CloudTrail or other sources.
 - **Amazon CloudWatch Events** – Match events and route them to one or more target functions or streams to make changes, capture state information, and take corrective action.

 Utilizing these [CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html) tools together can be useful in detecting anomolous activity and patterns in data access within the S3 service. It is recommended that CloudWatch is used to monitor any critical services in use at CG. Please see the [CloudWatch Runbook](https://github.com/open-itg/aws_runbooks/blob/master/cloudwatch/RUNBOOK.md) for further information.

More info on monitoring: https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudwatch-monitoring.html

<br><br> 

## Operational Best Practices
### 1. Resource Tags
**What, Why & How?**  

Identification of your IT assets is a crucial aspect of governance and security. You need to have visibility of all your Amazon S3 resources to assess their security posture and take action on potential areas of weakness.

Tagging resources in the cloud is an easy way for teams to provide information related to who owns the resource, what the resource is used for, as well as other important information related to the deployment lifecycle of the resource. CG has mandated that all cloud resources are to be tagged with for cross-team use. Although most of the mandatory tags will be added through automation, one should still check to make sure that all newly deployed recources have the appropriate tags attached. Please see the documentation below for the latest tagging standards.

<br><br>

### 2. Enable AWS Config
**Why?**

AWS Config allows you to monitor your AWS resources, to assess, audit, and record configurations and changes. It is essentially a database to keep track of the historical metadata for the resources in an account. It allows for Continous monitoring, assessment, change management, and troubleshooting.

**How?**  

First you need to make sure that you have AWS Config set up. Follow the links for more information on AWS Config and how to set them up.

  - AWS Config Setup: https://docs.aws.amazon.com/config/latest/developerguide/gs-console.html  
  - AWS Config Runboook: https://github.com/open-itg/aws_runbooks/blob/master/config/Runbook.md  

Next we need to Enable AWS config and Amazon S3 Montioring:

1. Sign into the AWS Management Console and open the AWS Config console.
2. If this is your first time using AWS Config, select Get started. If you’ve already used AWS Config, select Settings.
3. In the Settings page, under Resource types to record, clear the All resources checkbox. In the Specific types list, select Bucket under S3.
4. Choose the Amazon S3 bucket for storing configuration history and snapshots. We’ll create a new Amazon S3 bucket.  
    - If you prefer to use an existing Amazon S3 bucket in your account, select the Choose a bucket from your account radio button and, using the dropdown, select an existing bucket.
5. Under Amazon SNS topic, check the box next to Stream configuration changes and notifications to an Amazon SNS topic, and then select the radio button to Create a topic.
6. Under AWS Config role, choose Create a role (unless you already have a role you want to use). We’re using the auto-suggested role name.
7. Select Next.
8. Configure Amazon S3 bucket monitoring rules:
    - On the AWS Config rules page, search for S3 and choose the s3-bucket-publice-read-prohibited and s3-bucket-public-write-prohibited rules, then click Next.
    - On the Review page, select Confirm. AWS Config is now analyzing your Amazon S3 buckets, capturing their current configurations, and evaluating the configurations against the rules we selected.
9. If you created a new Amazon SNS topic, open the Amazon SNS Management Console and locate the topic you created:
10.Copy the ARN of the topic (the string that begins with arn:) because you’ll need it in a later step.
11. Select the checkbox next to the topic, and then, under the Actions menu, select Subscribe to topic.
12. Select Email as the protocol, enter your email address, and then select Create subscription.  
After several minutes, you’ll receive an email asking you to confirm your subscription for notifications for this topic. Select the link to confirm the subscription.



<br><br>

### 3. Enable AWS Trusted Advisor
**What, Why & How?** 

Trust Advisor checks your AWS environment and makes recommendations when opportunities exist to save money, improve system availability and performance, or help close security gaps.

Trusted Advisor has the following Amazon S3-related checks:

  - Logging configuration of Amazon S3 buckets.
  - Security checks for Amazon S3 buckets that have open access permissions.
  - Fault tolerance checks for Amazon S3 buckets that don't have versioning enabled, or have versioning suspended.

More info on [Trusted Advisor](https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html) in the *AWS Support Guide*

<br><br>

### 4.  Utilize Lifecycle Management
**Why?**

Managing your storage lifecycle is increasingly important as organizations seek to limit storage investments, especially when it comes to operating expenses like cloud storage. Lifecycle management allow a user's predetermined rules to automatically migrate data objects to other storage options or to schedule unnecessary or expired data to be deleted.

More information on managing storage lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html

**How?**
1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. In the Buckets list, choose the name of the bucket that you want to create a lifecycle rule for.
3. Choose the Management tab, and choose Create lifecycle rule.
4. In Lifecycle rule name, enter a name for your rule.
(The name must be unique within the bucket.)
5. Choose the scope of the lifecycle rule:
    - To apply this lifecycle rule to all objects with a specific prefix or tag, choose Limit the scope to specific prefixes or tags.
      - To limit the scope by prefix, in Prefix, enter the prefix.
      - To limit the scope by tag, choose Add tag, and enter the tag key and value.
    - For more information about object name prefixes, see Creating object key names. For more information about object tags, see Categorizing your storage using tags.
    - To apply this lifecycle rule to all objects in the bucket, choose This rule applies to all objects in the bucket, and choose I acknowledge that this rule applies to all objects in the bucket.
6. Under Lifecycle rule actions, choose the actions that you want your lifecycle rule to perform:
    - Transition current versions of objects between storage classes
    - Transition previous versions of objects between storage classes
    - Expire current versions of objects
    - Permanently delete previous versions of objects
    - Delete expired delete markers or incomplete multipart uploads
Depending on the actions that you choose, different options appear.
7. To transition *current* versions of objects between storage classes, under Transition current versions of objects between storage classes:  
  a. In Storage class transitions, choose the storage class to transition to:
    - Standard-IA
    - Intelligent-Tiering
    - One Zone-IA
    - Glacier
    - Glacier Deep Archive

    b. In Days after object creation, enter the number of days after creation to transition the object.
8. Follow step 7 for *non-current* versions.
9. To expire current versions of objects, under Expire previous versions of objects, in Number of days after object creation, enter the number of days.
>**NOTE**  
>In a non-versioned bucket the expiration action results in Amazon S3 permanently removing the object. For more information about lifecycle actions, see [Elements to describe lifecycle actions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-actions).
10. To permanently delete previous versions of objects, under Permanently delete previous versions of objects, in Number of days after objects become previous versions, enter the number of days.
11. Under Delete expired delete markers or incomplete multipart uploads, choose Delete expired object delete markers and Delete incomplete multipart uploads. Then, enter the number of days after the multipart upload initiation that you want to end and clean up incomplete multipart uploads.
12. Choose Create rule.  
If the rule does not contain any errors, Amazon S3 enables it, and you can see it on the Management tab under Lifecycle rules.

<br><br>

## Endnotes
https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html
https://docs.aws.amazon.com/AmazonS3/latest/userguide/security.html

## Capital Group Control Statements 
1. All Data-at-rest must be encrypted and use a CG BYOK encryption key.
2. All Data-in-transit must be encrypted using certificates using CG Certificate Authority.
3. Keys storied in a Key Management System (KMS) should be created by Capital Group's hardware security module (HSM) and are a minimum of AES-256.
4. AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch.
5. AWS IAM User accounts are only to be created for use by services or products that do not support IAM Roles. Services are not allowed to create local accounts for human use within the service. All human user authentication will take place within CG’s Identity Provider.
6. Any AWS service used by CG should not be directly available to the Internet and the default route is always the CG gateway.
7. Use of AWS IAM accounts are restricted to CG networks.
8. AWS IAM User secrets, including passwords and secret access keys, are to be rotated every 90 days. Accounts created locally within any service must also have their secrets rotated every 90 days.
9. Encryption keys are rotated annually.
10. Administrative access to AWS resources will have MFA enabled

## Glossary
**Data** - Digital pieces of information stored or transmitted for use with an information system from which understandable information is derived. Items considered to be data are: Source code, meta-data, build artifacts, information input and output.  
 
**Information System** - An organized assembly of resources and procedures for the collection, processing, maintenance, use, sharing, dissemination, or disposition of information. All systems, platforms, compute instances including and not limited to physical and virtual client endpoints, physical and virtual servers, software containers, databases, Internet of Things (IoT) devices, network devices, applications (internal and external), Serverless computing instances (i.e. AWS Lambda), vendor provided appliances, and third-party platforms, connected to the Capital Group network or used by Capital Group users or customers.
 
**Log** - a record of the events occurring within information systems and networks. Logs are composed of log entries; each entry contains information related to a specific event that has occurred within a system or network.
 
**Information** - communication or representation of knowledge such as facts, data, or opinions in any medium or form, including textual, numerical, graphic, cartographic, narrative, or audiovisual. 
 
**Cloud Computing** - A model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.
 
**Vulnerability**  - Weakness in an information system, system security procedures, internal controls, or implementation that could be exploited or triggered by a threat source. Note: The term weakness is synonymous for deficiency. Weakness may result in security and/or privacy risks.