<img src="https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png" alt="AWS" width="250"/>

# AWS DynamoDB - Security Runbook <!-- omit in toc -->
## Capgroup Cybersecurity Control Alignment <!-- omit in toc -->

**Generated By:**  
[Josh Linus (JHZL)](https://cgweb3/profile/JHZL)  
Security Engineering

**Last Update:** *08/08/2021*

Table of Contents
- [Disclaimer](#disclaimer)
- [Overview](#overview)
- [Cloud Security Requirements](#cloud-security-requirements)
  - [1. Enforce least privilege for all S3 users and roles](#1-Enforce-least-privilege-for-all-S3-users-and-roles)
  - [2. Buckets are encrypted using CG CMK](#2-Buckets-are-encrypted-using-CG-CMK)
  - [3. Data in Transit is encrypted using TLS 1.2](#3-Data-in-Transit-is-encrypted-using-TLS-1.2)
  - [4. Block Public Access](#4-Block-Public-Access)
  - [5. S3 Utilizes VPC Endpoints to Prevent Public Access](#5-EBS-Utilizes-VPC-Endpoints-to-Prevent-Public-Access)
  - [6. Enable Access Server Logs](#6-Enable-Access-Server-Logs) 
  - [7. Enable S3 Replication](#7-Enable-S3-Replication) 
  - [8. Enable S3 Versioning](#8-Enable-S3-Versioning)
  - [9. CloudTrail logging enabled for S3](#9-CloudTrail-logging-enabled-for-S3)
  - [10. CloudWatch logging enabled for S3](#10-CloudWatch-logging-enabled-for-S3)
- [Operational Best Practices](#Operational-Best-Practices)
  - [1. Resource Tags](#1-Resource-Tags)
  - [2. Enable AWS Config](#Enable-AWS-Config)
  - [3. Enable AWS Trusted Advisor](#3-Enable-AWS-Trusted-Advisor)
  - [4. Utilize Lifecycle Management](#4-Utilize-Lifecycle-Management)
- [Endnotes](#endnotes)
- [Capital Group Control Statements](#capital-group-control-statements)
- [Glossary](#glossary)


## Overview
Amazon S3 is a core service offered by AWS that provides object storage. It allows you to store and retrieve any amount of data, at any time, from anywhere on the web. It gives any developer access to highly scalable, reliable, fast, and inexpensive data storage infrastructure; one of the foundational components to any platform.

<img src="/docs/img/s3/s3.png" width="800"><br>

### Use Case Examples:
 - Data Lakes
 - Running Cloud Native Applications
 - Backup and Restore Data
 - Archive Data 
 - Webites

<br><br>

## Cloud Security Requirements
### 1. Enforce least privilege for all S3 users and roles  

**Why?** Controls provide reasonable assurance that logical access to data is restricted to authorized and appropriate users, and such users are restricted to performing authorized and appropriate actions.

Controls provide reasonable assurance that privileged functions are managed and monitored commensurate with the risk level.

Controls provide reasonable assurance that users, devices, and other assets are authenticated commensurate with the risk level.

**How?** Manage permissions with IAM User Policies, S3 Bucket Policies, and S3 Access Control Lists (ACLs) to ensure appropriateness of access. IAM Roles should be used to manage temporary credentials for applications and AWS Services that need access to S3.  The assigned Role will supply temporary permissions that applications can use when they make calls to other AWS resources.

IAM provides permissions on which API calls can be made by users to the service as well as who/what can access the service at a bucket and object level. Bucket policies and ACLs can be used separately and also in conjunction with IAM to control access to S3 buckets and objects.

The following S3 bucket policy denies permissions to any user to perform any Amazon S3 operations on objects in the specified S3 bucket unless the request originates from the range of IP addresses specified in the condition.

> **Important**  
> This statement identifies the 54.240.143.0/24 as the range of allowed IP addresses.  
> Replace the IP address range in this example with an appropriate value for your use case before using this policy. **Otherwise, you will lose the ability to access your bucket.**

```json
{
  "Version": "2012-10-17",
  "Id": "S3PolicyId1",
  "Statement": [
    {
      "Sid": "IPAllow",
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:*",
      "Resource": [
	 "arn:aws:s3:::awsexamplebucket1",
         "arn:aws:s3:::awsexamplebucket1/*"
      ],
      "Condition": {
	 "NotIpAddress": {"aws:SourceIp": "54.240.143.0/24"}
      }
    }
  ]
}
```

Employ MFA for Sensitive S3 Resources

**Why?** S3 supports MFA-protected API access, a feature that can enforce multi-factor authentication (MFA) for access to your Amazon S3 resources. Multi-factor authentication provides an extra level of security that you can apply to your AWS environment. It is a security feature that requires users to prove physical possession of an MFA device by providing a valid MFA code. For more information, see [Endnote 1](#endnote-1). You can require MFA for any requests to access your Amazon S3 resources.

**How?** You can enforce the MFA requirement using the `aws:MultiFactorAuthAge` key in a bucket policy. AWS Identity and Access Management (IAM) users can access Amazon S3 resources by using temporary credentials issued by the AWS Security Token Service (AWS STS). You provide the MFA code at the time of the AWS STS request.

The following bucket policy includes two policy statements. One statement allows the `s3:GetObject` permission on a bucket (`awsexamplebucket1`) to everyone. Another statement further restricts access to the `awsexamplebucket1/examplefolder` folder in the bucket by requiring MFA.  
The `Null` condition in the Condition block evaluates to true if the `aws:MultiFactorAuthAge` key value is null, indicating that the temporary security credentials in the request were created without the MFA key.
```json
{
    "Version": "2012-10-17",
    "Id": "123",
    "Statement": [
      {
        "Sid": "",
        "Effect": "Deny",
        "Principal": "*",
        "Action": "s3:*",
        "Resource": "arn:aws:s3:::awsexamplebucket1/examplefolder/*",
        "Condition": { "Null": { "aws:MultiFactorAuthAge": true } }
      },
      {
        "Sid": "",
        "Effect": "Allow",
        "Principal": "*",
        "Action": ["s3:GetObject"],
        "Resource": "arn:aws:s3:::awsexamplebucket1/*"
      }
    ]
 }
```

### 2. Buckets are encrypted using CG CMK
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.DS-1|Data-at-rest is protected|
|PR.DS-2|Data-in-transit is protected|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|1|All Data-at-rest must be encrypted and use a CG BYOK encryption key.|
|2|All Data-in-transit must be encrypted using certificates using CG Certificate Authority.|
|3|Keys storied in a Key Management System (KMS) should be created by Capital Group's hardware security module (HSM) and are a minimum of AES-256.|

**Why?** Controls provide reasonable assurance that data is encrypted at-rest and in-transit throughout the lifecycle using a NIST-approved, CG-compliant encryption mechanism.

**How?**
#### Requiring Server-Side Encryption <!-- omit in toc -->
To require server-side encryption of all objects in a particular Amazon S3 bucket, you can use a policy. For example, the following bucket policy denies upload object (`s3:PutObject`) permission to everyone if the request does not include the `x-amz-server-side-encryption` header requesting server-side encryption with SSE-KMS. The policy also specifies that a particular key must be used. 
```json
{
   "Version":"2012-10-17",
   "Id":"PutObjectPolicy",
   "Statement":[{
         "Sid":"DenyUnEncryptedObjectUploads",
         "Effect":"Deny",
         "Principal":"*",
         "Action":"s3:PutObject",
         "Resource":"arn:aws:s3:::awsexamplebucket1/*",
         "Condition":{
            "StringNotEquals":{
               "s3:x-amz-server-side-encryption":"aws:kms"
            },
            "StringEquals":{
               "s3:x-amz-server-side-encryption-aws-kms-key-id":"arn:aws:kms:region:acct-id:key/key-id"
            }
         }
      }
   ]
}
```

#### Using S3 Block Public Access <!-- omit in toc -->
The following `put-public-access-block` example sets a restrictive block public access configuration for the specified bucket.
```
aws s3api put-public-access-block \
    --bucket my-bucket \
    --public-access-block-configuration "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
```
For instructions on setting up Block Public Access on a bucket from the S3 Console, see [Endnote 2](#endnote-2).

The following `put-public-access-block` example toggles all block public access settings to true for the specified account.
```
aws s3control put-public-access-block \
    --account-id 123456789012 \
    --public-access-block-configuration '{"BlockPublicAcls": true, "IgnorePublicAcls": true, "BlockPublicPolicy": true, "RestrictPublicBuckets": true}'
```

Once S3 Block Public Access is set up, add a Service Control Policy to deny `s3:PutAccountPublicAccessBlock`.

#### Enforce Encryption of Data-In-Transit <!-- omit in toc -->
All communications with the S3 API are encrypted using TLS, but file transfers to/from S3 buckets do not require secure transport by default. This can be changed, using S3 bucket policies.

The following statement uses the Bool condition operator with the `aws:SecureTransport` key to specify that the request must use Secure Transport.
```json
{
  "Version": "2012-10-17",
  "Statement": {
    "Effect": "Allow",
    "Action": "s3:*",
    "Resource": "arn:aws:s3:::awsexamplebucket1/*",
    "Condition": {"Bool": {"aws:SecureTransport": "true"}}
  }
}
```

### 4. Enhance Durability of Critical Data
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.IP-4|Backups of information are conducted, maintained, and tested|
|PR.PT-5|Mechanisms (e.g., failsafe, load balancing, hot swap) are implemented to achieve resilience requirements in normal and adverse situations|

**Why?** The AWS global infrastructure is built around Regions and Availability Zones. AWS Regions provide multiple, physically separated and isolated Availability Zones that are connected with low latency, high throughput, and highly redundant networking. These Availability Zones offer you an effective way to design and operate applications and databases. They are more highly available, fault tolerant, and scalable than traditional single data center infrastructures or multi-data center infrastructures. If you specifically need to replicate your data over greater geographic distances, you can use Replication, which enables automatic, asynchronous copying of objects across buckets in different AWS Regions.

Each AWS Region has multiple Availability Zones. You can deploy your applications across multiple Availability Zones in the same Region for fault tolerance and low latency. Availability Zones are connected to each other with fast, private fiber-optic networking, enabling you to easily architect applications that automatically fail over between Availability Zones without interruption.

In addition to the AWS global infrastructure, Amazon S3 offers several features to help support your data resiliency and backup needs.

**How?**
#### Setting Up S3 Replication <!-- omit in toc -->
Create a role for the replication service to use. This role will need a policy attached to it similar to the following:  
```json
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Action":[
            "s3:ListBucket",
            "s3:GetReplicationConfiguration",
            "s3:GetObjectVersionForReplication",
            "s3:GetObjectVersionAcl"
         ],
         "Effect":"Allow",
         "Resource":[
            "arn:aws:s3:::source",
            "arn:aws:s3:::source/*"
         ]
      },
      {
         "Action":[
            "s3:ReplicateObject",
            "s3:ReplicateDelete",
            "s3:ReplicateTags",
            "s3:GetObjectVersionTagging"
         ],
         "Effect":"Allow",
         "Condition":{
            "StringLikeIfExists":{
               "s3:x-amz-server-side-encryption":[
                  "aws:kms",
                  "AES256"
               ],
               "s3:x-amz-server-side-encryption-aws-kms-key-id":[
                  "AWS KMS key IDs(in ARN format) to use for encrypting object replicas"  
               ]
            }
         },
         "Resource":"arn:aws:s3:::destination/*"
      },
      {
         "Action":[
            "kms:Decrypt"
         ],
         "Effect":"Allow",
         "Condition":{
            "StringLike":{
               "kms:ViaService":"s3.us-east-1.amazonaws.com",
               "kms:EncryptionContext:aws:s3:arn":[
                  "arn:aws:s3:::source/*"
               ]
            }
         },
         "Resource":[
            "AWS KMS key IDs(in ARN format) used to encrypt source objects." 
         ]
      },
      {
         "Action":[
            "kms:Encrypt"
         ],
         "Effect":"Allow",
         "Condition":{
            "StringLike":{
               "kms:ViaService":"s3.us-west-2.amazonaws.com",
               "kms:EncryptionContext:aws:s3:arn":[
                  "arn:aws:s3:::destination/*"
               ]
            }
         },
         "Resource":[
            "AWS KMS key IDs(in ARN format) to use for encrypting object replicas" 
         ]
      }
   ]
}
```

Run the following CLI command:  
```
aws s3api put-bucket-replication \
    --bucket my-bucket \
    --replication-configuration file://replication.json
```

This will be the contents of the `replication.json` file referenced in the previous command. It outlines the rules for replication.  
```json
{
    "Role": "arn:aws:iam::123456789012:role/s3-replication-role",
    "Rules": [
        {
            "Status": "Enabled",
            "SourceSelectionCriteria": {
              "SseKmsEncryptedObjects": {
                "Status": "Enabled"
              }
            },
            "Priority": 1,
            "DeleteMarkerReplication": { "Status": "Disabled" },
            "Filter" : { "Prefix": ""},
            "Destination": {
                "Bucket": "arn:aws:s3:::my-bucket-backup",
                "EncryptionConfiguration": {
                  "ReplicaKmsKeyID": "AWS KMS key IDs(in ARN format) to use for encrypting object replicas"
                }
            }
        }
    ]
}
```

Further information on setting up replication can be found in [Endnote 3](#endnote-3).

#### Setting up S3 Versioning <!-- omit in toc -->
Each bucket you create has a versioning subresource associated with it. By default, your bucket is unversioned, and accordingly the versioning subresource stores empty versioning configuration.
```xml
<VersioningConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
</VersioningConfiguration>
```
To enable versioning, you send a request to Amazon S3 with a versioning configuration that includes a status.
```xml
<VersioningConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
  <Status>Enabled</Status> 
</VersioningConfiguration>
```
To suspend versioning, you set the status value to `Suspended`.

#### MFA delete <!-- omit in toc -->
You can optionally add another layer of security by configuring a bucket to enable MFA (multi-factor authentication) Delete, which requires additional authentication for either of the following operations:

- Change the versioning state of your bucket
- Permanently delete an object version

MFA Delete requires two forms of authentication together:

- Your security credentials
- The concatenation of a valid serial number, a space, and the six-digit code displayed on an approved authentication device

MFA Delete thus provides added security in the event, for example, your security credentials are compromised.

To enable or disable MFA Delete, you use the same API that you use to configure versioning on a bucket. Amazon S3 stores the MFA Delete configuration in the same versioning subresource that stores the bucket's versioning status.
```xml
<VersioningConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
  <Status>VersioningState</Status>
  <MfaDelete>MfaDeleteState</MfaDelete>  
</VersioningConfiguration>
```

### 5. Prevent S3 Bucket Sniping
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.IP-6|Data is destroyed according to policy|
|PR.IP-7|Protection processes are improved|
|PR.IP-8|Effectiveness of protection technologies is shared|

**Why?** While it is important to properly destroy S3 data in line with regulatory and internal requirements, it is prudent to maintain ownership of the S3 bucket name (by not deleting the bucket) to avoid someone getting access to a legacy bucket hoping for accidental and/or inadvertent uploads.

**How?** Maintain S3 Buckets by adding an Service Control Policy denying `s3:DeleteBucket` and `s3:DeleteBucketPolicy`.

When an S3 bucket is decommissioned, add a restrictive S3 bucket policy to the bucket which allows only the Account Administrators to perform `s3:*` actions. Remove all other statements from the bucket policy.

### 6. Isolate S3 Bucket Access from the Internet
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.PT-4|Communications and control networks are protected|
|PR.AC-3|Remote access is managed|
|PR.AC-5|Network integrity is protected (e.g., network segregation, network segmentation)|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|6|Any AWS service used by CG should not be directly available to the Internet and the default route is always the CG gateway.|
|7|Use of AWS IAM accounts are restricted to CG networks.|

**Why?** Controls provide reasonable assurance that direct access (with provided credentials) is not allowed to the AWS environment from outside the corporate network or AWS backbone.

**How?** Amazon S3 bucket policies can be used to control access to buckets from specific Amazon VPC endpoints, which will isolate network access to a given S3 bucket from only the specific VPC within the AWS network. 

The following is an example of an Amazon S3 bucket policy that restricts access to a specific bucket, `awsexamplebucket1`, only from the VPC endpoint with the ID `vpce-1a2b3c4d`. The policy denies all access to the bucket if the specified endpoint is not being used. The `aws:SourceVpce` condition is used to specify the endpoint. The `aws:SourceVpce` condition does not require an ARN for the VPC endpoint resource, only the VPC endpoint ID. 

> **Important**
> Before using the following example policy, replace the VPC endpoint ID with an appropriate value for your use case. Otherwise, you won't be able to access your bucket.

This policy disables console access to the specified bucket, because console requests don't originate from the specified VPC endpoint.
```json
{
   "Version": "2012-10-17",
   "Id": "Policy1415115909152",
   "Statement": [
     {
       "Sid": "Access-to-specific-VPCE-only",
       "Principal": "*",
       "Action": "s3:*",
       "Effect": "Deny",
       "Resource": ["arn:aws:s3:::awsexamplebucket1",
                    "arn:aws:s3:::awsexamplebucket1/*"],
       "Condition": {
         "StringNotEquals": {
           "aws:SourceVpce": "vpce-1a2b3c4d"
         }
       }
     }
   ]
}
```

## Detective
### 1. Ensure Amazon S3 Inventory is enabled and monitored regularly
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|ID.AM-1|physical devices and systems within the organization are inventoried|
|ID.AM-2|Software platforms and applications within the organization are inventoried|



**Why?** 
Amazon S3 inventory is one of the tools Amazon S3 provides to help manage your storage\. You can use it to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs\.

**How?** 
Set up Amazon S3 Inventory

#### How do I set up Amazon S3 inventory?<a name="storage-inventory-how-to-set-up"><!-- omit in toc -->

This section describes how to set up an inventory, including details about the inventory source and destination buckets\.

#### Amazon S3 inventory source and destination buckets<a name="storage-inventory-buckets"><!-- omit in toc -->

The bucket that the inventory lists the objects for is called the *source bucket*\. The bucket where the inventory list file is stored is called the *destination bucket*\. 

**Source Bucket**

The inventory lists the objects that are stored in the source bucket\. You can get inventory lists for an entire bucket or filtered by \(object key name\) prefix\.

The source bucket:
+ Contains the objects that are listed in the inventory\.
+ Contains the configuration for the inventory\.

**Destination Bucket**

Amazon S3 inventory list files are written to the destination bucket\. To group all the inventory list files in a common location in the destination bucket, you can specify a destination \(object key name\) prefix in the inventory configuration\.

The destination bucket:
+ Contains the inventory file lists\. 
+ Contains the manifest files that list all the file inventory lists that are stored in the destination bucket\. 
+ Must have a bucket policy to give Amazon S3 permission to verify ownership of the bucket and permission to write files to the bucket\. 
+ Must be in the same AWS Region as the source bucket\.
+ Can be the same as the source bucket\.
+ Can be owned by a different AWS account than the account that owns the source bucket\.

#### Setting up Amazon S3 inventory<a name="storage-inventory-setting-up"><!-- omit in toc -->

Amazon S3 inventory helps you manage your storage by creating lists of the objects in an S3 bucket on a defined schedule\. You can configure multiple inventory lists for a bucket\. The inventory lists are published to CSV, ORC, or Parquet files in a destination bucket\. 

The easiest way to set up an inventory is by using the AWS Management Console, but you can also use the REST API, AWS CLI, or AWS SDKs\. The console performs the first step of the following procedure for you: adding a bucket policy to the destination bucket\.

**To set up Amazon S3 inventory for an S3 bucket**

1. **Add a bucket policy for the destination bucket\.**

   You must create a bucket policy on the destination bucket to grant permissions to Amazon S3 to write objects to the bucket in the defined location\. 

1. **Configure an inventory to list the objects in a source bucket and publish the list to a destination bucket\.**

   When you configure an inventory list for a source bucket, you specify the destination bucket where you want the list to be stored, and whether you want to generate the list daily or weekly\. You can also configure what object metadata to include and whether to list all object versions or only current versions\. 

   You can specify that the inventory list file be encrypted by using an Amazon S3 managed key \(SSE\-S3\) or an AWS Key Management Service \(AWS KMS\) customer managed customer master key \(CMK\)\.  If you plan to use SSE\-KMS encryption, see Step 3\.
   + For information about how to use the console to configure an inventory list, see [How Do I Configure Amazon S3 Inventory?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-inventory.html) in the *Amazon Simple Storage Service Console User Guide*\.
   + To use the Amazon S3 API to configure an inventory list, use the [PUT Bucket inventory configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTInventoryConfig.html) REST API or the equivalent from the AWS CLI or AWS SDKs\. 

1. **To encrypt the inventory list file with SSE\-KMS, grant Amazon S3 permission to use the CMK stored in AWS KMS\.**

   You can configure encryption for the inventory list file by using the AWS Management Console, REST API, AWS CLI, or AWS SDKs\. Whichever way you choose, you must grant Amazon S3 permission to use the AWS KMS customer managed CMK to encrypt the inventory file\. You grant Amazon S3 permission by modifying the key policy for the customer managed CMK that you want to use to encrypt the inventory file\. 

#### Granting Amazon S3 permission to use your AWS KMS CMK for encryption<a name="storage-inventory-kms-key-policy"><!-- omit in toc -->

To grant Amazon S3 permission to encrypt using a customer managed AWS Key Management Service \(AWS KMS\) customer master key \(CMK\), you must use a key policy\. To update your key policy so that you can use an AWS KMS customer managed CMK to encrypt the inventory file, follow these steps\.

**To grant permissions to encrypt using your AWS KMS CMK**

1. Using the AWS account that owns the customer managed CMK, sign into the AWS Management Console\.

1. Open the AWS KMS console at [https://console\.aws\.amazon\.com/kms](https://console.aws.amazon.com/kms)\.

1. To change the AWS Region, use the Region selector in the upper\-right corner of the page\.

1. In the left navigation pane, choose **Customer managed keys**\.

1. Under **Customer managed keys**, choose the key that you want to use to encrypt the inventory file\. CMKs are Region specific and must be in the same Region as the source bucket\.

1. Under **Key policy**, choose **Switch to policy view**\.

1. To update the key policy, choose **Edit**\.

1. Under **Edit key policy**, add the following key policy to the existing key policy\.

   ```
   {
       "Sid": "Allow Amazon S3 use of the CMK",
       "Effect": "Allow",
       "Principal": {
           "Service": "s3.amazonaws.com"
       },
       "Action": [
           "kms:GenerateDataKey"
       ],
       "Resource": "*"
   }
   ```

1. Choose **Save changes**\.

   For more information about creating AWS KMS customer managed CMKs and using key policies, see the following topics in the *AWS Key Management Service Developer Guide*:
   + [Getting Started](https://docs.aws.amazon.com/kms/latest/developerguide/getting-started.html)
   + [Using Key Policies in AWS KMS](https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html)

   You can also use the AWS KMS PUT key policy API [PutKeyPolicy](https://docs.aws.amazon.com/kms/latest/APIReference/API_PutKeyPolicy.html) to copy the key policy to the customer managed CMK that you want to use to encrypt the inventory file\. 

#### What's included in an Amazon S3 inventory?<a name="storage-inventory-contents"><!-- omit in toc -->

An inventory list file contains a list of the objects in the source bucket and metadata for each object\. The inventory lists are stored in the destination bucket as a CSV file compressed with GZIP, as an Apache optimized row columnar \(ORC\) file compressed with ZLIB, or as an Apache Parquet \(Parquet\) file compressed with Snappy\. 

The inventory list contains a list of the objects in an S3 bucket and the following metadata for each listed object: 
+ **Bucket name** – The name of the bucket that the inventory is for\.
+ **Key name** – Object key name \(or key\) that uniquely identifies the object in the bucket\. When using the CSV file format, the key name is URL\-encoded and must be decoded before you can use it\.
+ **Version ID** – Object version ID\. When you enable versioning on a bucket, Amazon S3 assigns a version number to objects that are added to the bucket\.  \(This field is not included if the list is only for the current version of objects\.\)
+ **IsLatest** – Set to `True` if the object is the current version of the object\. \(This field is not included if the list is only for the current version of objects\.\)
+ **Size** – Object size in bytes\.
+ **Last modified date** – Object creation date or the last modified date, whichever is the latest\.
+ **ETag** – The entity tag is a hash of the object\. The ETag reflects changes only to the contents of an object, not its metadata\. The ETag may or may not be an MD5 digest of the object data\. Whether it is depends on how the object was created and how it is encrypted\.
+ **Storage class** – Storage class used for storing the object\. 
+ **Intelligent\-Tiering access tier** – Access tier \(frequent or infrequent\) of the object if stored in Intelligent\-Tiering\. For more information, see [Amazon S3 Intelligent\-Tiering](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-dynamic-data-access)\.
+ **Multipart upload flag** – Set to `True` if the object was uploaded as a multipart upload\. 
+ **Delete marker** – Set to `True`, if the object is a delete marker\.  \(This field is automatically added to your report if you've configured the report to include all versions of your objects\)\.
+ **Replication status** – Set to `PENDING`, `COMPLETED`, `FAILED`, or `REPLICA.` 
+ **Encryption status** – Set to `SSE-S3`, `SSE-C`, `SSE-KMS`, or `NOT-SSE`\. The server\-side encryption status for SSE\-S3, SSE\-KMS, and SSE with customer\-provided keys \(SSE\-C\)\. A status of `NOT-SSE` means that the object is not encrypted with server\-side encryption\. 
+ **S3 Object Lock Retain until date** – The date until which the locked object cannot be deleted\. 
+ **S3 Object Lock Mode** – Set to `Governance` or `Compliance` for objects that are locked\. 
+ **S3 Object Lock Legal hold status ** – Set to `On` if a legal hold has been applied to an object; otherwise it is set to `Off`\. 

We recommend that you create a lifecycle policy that deletes old inventory lists\. 

#### Inventory consistency<a name="storage-inventory-contents-consistency"><!-- omit in toc -->

All of your objects might not appear in each inventory list\. The inventory list provides eventual consistency for PUTs of both new objects and overwrites, and DELETEs\. Inventory lists are a rolling snapshot of bucket items, which are eventually consistent \(that is, the list might not include recently added or deleted objects\)\. 

To validate the state of the object before you take action on the object, we recommend that you perform a `HEAD Object` REST API request to retrieve metadata for the object, or check the object's properties in the Amazon S3 console\. You can also check object metadata with the AWS CLI or the AWS SDKS\. For more information, see [HEAD Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html) in the *Amazon Simple Storage Service API Reference*\.

#### Where are inventory lists located?<a name="storage-inventory-location"><!-- omit in toc -->

When an inventory list is published, the manifest files are published to the following location in the destination bucket\.

```
 destination-prefix/source-bucket/config-ID/YYYY-MM-DDTHH-MMZ/manifest.json
 destination-prefix/source-bucket/config-ID/YYYY-MM-DDTHH-MMZ/manifest.checksum
 destination-prefix/source-bucket/config-ID/hive/dt=YYYY-MM-DD-HH-MM/symlink.txt
```
+ *destination\-prefix* is the \(object key name\) prefix set in the inventory configuration, which can be used to group all the inventory list files in a common location within the destination bucket\.
+ *source\-bucket* is the source bucket that the inventory list is for\. It is added to prevent collisions when multiple inventory reports from different source buckets are sent to the same destination bucket\.
+ *config\-ID* is added to prevent collisions with multiple inventory reports from the same source bucket that are sent to the same destination bucket\. The *config\-ID* comes from the inventory report configuration, and is the name for the report that is defined on setup\.
+ *YYYY\-MM\-DDTHH\-MMZ* is the timestamp that consists of the start time and the date when the inventory report generation begins scanning the bucket; for example, `2016-11-06T21-32Z`\.
+ `manifest.json` is the manifest file\. 
+ `manifest.checksum` is the MD5 of the content of the `manifest.json` file\. 
+ `symlink.txt` is the Apache Hive\-compatible manifest file\. 

The inventory lists are published daily or weekly to the following location in the destination bucket\.

```
      destination-prefix/source-bucket/config-ID/example-file-name.csv.gz
      ...
      destination-prefix/source-bucket/config-ID/example-file-name-1.csv.gz
```
+ *destination\-prefix* is the \(object key name\) prefix set in the inventory configuration\. It can be used to group all the inventory list files in a common location in the destination bucket\.
+ *source\-bucket* is the source bucket that the inventory list is for\. It is added to prevent collisions when multiple inventory reports from different source buckets are sent to the same destination bucket\.
+ *example\-file\-name*`.csv.gz` is one of the CSV inventory files\. ORC inventory names end with the file name extension `.orc`, and Parquet inventory names end with the file name extension `.parquet`\.

#### What is an inventory manifest?<a name="storage-inventory-location-manifest"><!-- omit in toc -->

The manifest files `manifest.json` and `symlink.txt` describe where the inventory files are located\. Whenever a new inventory list is delivered, it is accompanied by a new set of manifest files\. These files may overwrite each other and in versioning enabled buckets will create a new versions of the manifest files\. 

Each manifest contained in the `manifest.json` file provides metadata and other basic information about an inventory\. This information includes the following:
+ Source bucket name
+ Destination bucket name
+ Version of the inventory
+ Creation timestamp in the epoch date format that consists of the start time and the date when the inventory report generation begins scanning the bucket
+ Format and schema of the inventory files
+ Actual list of the inventory files that are in the destination bucket

Whenever a `manifest.json` file is written, it is accompanied by a `manifest.checksum` file that is the MD5 of the content of `manifest.json` file\.

The following is an example of a manifest in a `manifest.json` file for a CSV\-formatted inventory\.

```
{
    "sourceBucket": "example-source-bucket",
    "destinationBucket": "arn:aws:s3:::example-inventory-destination-bucket",
    "version": "2016-11-30",
    "creationTimestamp" : "1514944800000",
    "fileFormat": "CSV",
    "fileSchema": "Bucket, Key, VersionId, IsLatest, IsDeleteMarker, Size, LastModifiedDate, ETag, StorageClass, IsMultipartUploaded, ReplicationStatus, EncryptionStatus, ObjectLockRetainUntilDate, ObjectLockMode, ObjectLockLegalHoldStatus",
    "files": [
        {
            "key": "Inventory/example-source-bucket/2016-11-06T21-32Z/files/939c6d46-85a9-4ba8-87bd-9db705a579ce.csv.gz",
            "size": 2147483647,
            "MD5checksum": "f11166069f1990abeb9c97ace9cdfabc"
        }
    ]
}
```

The following is an example of a manifest in a `manifest.json` file for an ORC\-formatted inventory\.

```
{
    "sourceBucket": "example-source-bucket",
    "destinationBucket": "arn:aws:s3:::example-destination-bucket",
    "version": "2016-11-30",
    "creationTimestamp" : "1514944800000",
    "fileFormat": "ORC",
    "fileSchema": "struct<bucket:string,key:string,version_id:string,is_latest:boolean,is_delete_marker:boolean,size:bigint,last_modified_date:timestamp,e_tag:string,storage_class:string,is_multipart_uploaded:boolean,replication_status:string,encryption_status:string,object_lock_retain_until_date:timestamp,object_lock_mode:string,object_lock_legal_hold_status:string>",
    "files": [
        {
            "key": "inventory/example-source-bucket/data/d794c570-95bb-4271-9128-26023c8b4900.orc",
            "size": 56291,
            "MD5checksum": "5925f4e78e1695c2d020b9f6eexample"
        }
    ]
}
```

The following is an example of a manifest in a `manifest.json` file for a Parquet\-formatted inventory\.

```
{
    "sourceBucket": "example-source-bucket",
    "destinationBucket": "arn:aws:s3:::example-destination-bucket",
    "version": "2016-11-30",
    "creationTimestamp" : "1514944800000",
    "fileFormat": "Parquet",
    "fileSchema": "message s3.inventory { required binary bucket (UTF8); required binary key (UTF8); optional binary version_id (UTF8); optional boolean is_latest; optional boolean is_delete_marker;  optional int64 size;  optional int64 last_modified_date (TIMESTAMP_MILLIS);  optional binary e_tag (UTF8);  optional binary storage_class (UTF8);  optional boolean is_multipart_uploaded;  optional binary replication_status (UTF8);  optional binary encryption_status (UTF8);}"
  "files": [
        {
           "key": "inventory/example-source-bucket/data/d754c470-85bb-4255-9218-47023c8b4910.parquet",
            "size": 56291,
            "MD5checksum": "5825f2e18e1695c2d030b9f6eexample" 
        }
    ]
}
```

The `symlink.txt` file is an Apache Hive\-compatible manifest file that allows Hive to automatically discover inventory files and their associated data files\. The Hive\-compatible manifest works with the Hive\-compatible services Athena and Amazon Redshift Spectrum\. It also works with Hive\-compatible applications, including [Presto](https://prestodb.io/), [Apache Hive](https://hive.apache.org/), [Apache Spark](https://databricks.com/spark/about/), and many others\.

**Important**  
The `symlink.txt` Apache Hive\-compatible manifest file does not currently work with AWS Glue\.  
Reading `symlink.txt` with [Apache Hive](https://hive.apache.org/) and [Apache Spark](https://databricks.com/spark/about/) is not supported for ORC and Parquet\-formatted inventory files\. 

#### How do I know when an inventory is complete?<a name="storage-inventory-notification"><!-- omit in toc -->

You can set up an Amazon S3 event notification to receive notice when the manifest checksum file is created, which indicates that an inventory list has been added to the destination bucket\. The manifest is an up\-to\-date list of all the inventory lists at the destination location\.

Amazon S3 can publish events to an Amazon Simple Notification Service \(Amazon SNS\) topic, an Amazon Simple Queue Service \(Amazon SQS\) queue, or an AWS Lambda function\. For more information, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.

The following notification configuration defines that all `manifest.checksum` files newly added to the destination bucket are processed by the AWS Lambda `cloud-function-list-write`\.

```
<NotificationConfiguration>
  <QueueConfiguration>
      <Id>1</Id>
      <Filter>
          <S3Key>
              <FilterRule>
                  <Name>prefix</Name>
                  <Value>destination-prefix/source-bucket</Value>
              </FilterRule>
              <FilterRule>
                  <Name>suffix</Name>
                  <Value>checksum</Value>
              </FilterRule>
          </S3Key>
     </Filter>
     <Cloudcode>arn:aws:lambda:us-west-2:222233334444:cloud-function-list-write</Cloudcode>
     <Event>s3:ObjectCreated:*</Event>
  </QueueConfiguration>
  </NotificationConfiguration>
```

For more information, see [Using AWS Lambda with Amazon S3](https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html) in the *AWS Lambda Developer Guide*\.

#### Querying inventory with Amazon Athena<a name="storage-inventory-athena-query"><!-- omit in toc -->

You can query Amazon S3 inventory using standard SQL by using Amazon Athena in all Regions where Athena is available\. To check for AWS Region availability, see the [AWS Region Table](https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/)\. 

Athena can query Amazon S3 inventory files in ORC, Parquet, or CSV format\. When you use Athena to query inventory, we recommend that you use ORC\-formatted or Parquet\-formatted inventory files\. ORC and Parquet formats provide faster query performance and lower query costs\. ORC and Parquet are self\-describing type\-aware columnar file formats designed for [Apache Hadoop](http://hadoop.apache.org/)\. The columnar format lets the reader read, decompress, and process only the columns that are required for the current query\. The ORC and Parquet formats for Amazon S3 inventory are available in all AWS Regions\.

**To get started using Athena to query Amazon S3 inventory**

1. Create an Athena table\. For information about creating a table, see [Creating Tables in Amazon Athena](https://docs.aws.amazon.com/athena/latest/ug/creating-tables.html) in the *Amazon Athena User Guide*\.

   The following sample query includes all optional fields in an ORC\-formatted inventory report\. Drop any optional field that you did not choose for your inventory so that the query corresponds to the fields chosen for your inventory\. Also, you must use your bucket name and the location\. The location points to your inventory destination path; for example, `s3://destination-prefix/source-bucket/config-ID/hive/`\.

   ```
   CREATE EXTERNAL TABLE your_table_name(
     `bucket` string,
     key string,
     version_id string,
     is_latest boolean,
     is_delete_marker boolean,
     size bigint,
     last_modified_date timestamp,
     e_tag string,
     storage_class string,
     is_multipart_uploaded boolean,
     replication_status string,
     encryption_status string,
     object_lock_retain_until_date timestamp,
     object_lock_mode string,
     object_lock_legal_hold_status string
     )
     PARTITIONED BY (dt string)
     ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
     STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'
     OUTPUTFORMAT  'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
     LOCATION 's3://destination-prefix/source-bucket/config-ID/hive/';
   ```

    When using Athena to query a Parquet\-formatted inventory report, use the following Parquet SerDe in place of the ORC SerDe in the `ROW FORMAT SERDE` statement\.

   ```
   ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
   ```

    When using Athena to query a CSV\-formatted inventory report, use the following Parquet SerDe in place of the ORC SerDe in the `ROW FORMAT SERDE` statement\.

   ```
   ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
   ```

1. To add new inventory lists to your table, use the following `MSCK REPAIR TABLE` command\.

   ```
   MSCK REPAIR TABLE your-table-name;
   ```

1. After performing the first two steps, you can run ad hoc queries on your inventory, as shown in the following examples\. 

   ```
   # Get list of latest inventory report dates available
   SELECT DISTINCT dt FROM your-table-name ORDER BY 1 DESC limit 10;
             
   # Get encryption status for a provided report date.
   SELECT encryption_status, count(*) FROM your-table-name WHERE dt = 'YYYY-MM-DD-HH-MM' GROUP BY encryption_status;
             
   # Get encryption status for report dates in the provided range.
   SELECT dt, encryption_status, count(*) FROM your-table-name 
   WHERE dt > 'YYYY-MM-DD-HH-MM' AND dt < 'YYYY-MM-DD-HH-MM' GROUP BY dt, encryption_status;
   ```

For more information about using Athena, see [Amazon Athena User Guide](https://docs.aws.amazon.com/athena/latest/ug/)\.

#### Amazon S3 inventory REST APIs<a name="storage-inventory-related-resources"><!-- omit in toc -->

The following are the REST operations used for Amazon S3 inventory\.
+  [ DELETE Bucket Inventory ](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEInventoryConfiguration.html) 
+  [ GET Bucket Inventory](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETInventoryConfig.html) 
+  [ List Bucket Inventory](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketListInventoryConfigs.html) 
+  [ PUT Bucket Inventory](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTInventoryConfig.html) 
  

### 2. Enable Amazon S3 server access logging 
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|DE.AE-3|Event data are aggregated and correlated from multiple sources and sensors|
|DE.AE-4|Impact of events is determined|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch|

**Why?** 
Server access logging provides detailed records of the requests that are made to a bucket\. Server access logs can assist you in security and access audits, help you learn about your customer base, and understand your Amazon S3 bill\.

**How?** 
#### Enable Amazon S3 server access logging<a name="ServerLogs"><!-- omit in toc -->

Server access logging provides detailed records for the requests that are made to a bucket\. Server access logs are useful for many applications\. For example, access log information can be useful in security and access audits\. It can also help you learn about your customer base and understand your Amazon S3 bill\.

**Note**  
Server access logs don't record information about wrong\-region redirect errors for Regions that launched after March 20, 2019\. Wrong\-region redirect errors occur when a request for an object or bucket is made outside the Region in which the bucket exists\. 

#### How to enable server access logging<a name="server-access-logging-overview"><!-- omit in toc -->

To track requests for access to your bucket, you can enable server access logging\. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and an error code, if relevant\. 

There is no extra charge for enabling server access logging on an Amazon S3 bucket, and you are not charged when the logs are PUT to your bucket\. However, any log files that the system delivers to your bucket accrue the usual charges for storage\. You can delete these log files at any time\. Subsequent reads and other requests to these log files are charged normally, as for any other object, including data transfer charges\.

By default, logging is disabled\. When logging is enabled, logs are saved to a bucket in the same AWS Region as the source bucket\. 

To enable logging: 

1. Turn on logging on the Amazon S3 bucket that you want to monitor\. We refer to this bucket as the *source bucket*\. 

1. Grant the Amazon S3 Log Delivery group write permission on the bucket where you want the access logs saved\. We refer to this bucket as the *target bucket*\. 

**Note**  
In Amazon S3 you can grant permission to deliver access logs through bucket access control lists \(ACLs\), but not through bucket policy\.
Adding *deny* conditions to a bucket policy might prevent Amazon S3 from delivering access logs\.
[Default bucket encryption](bucket-encryption.html) on the target bucket *can only be used* if **AES256 \(SSE\-S3\)** is selected\. SSE\-KMS encryption is not supported\. 
S3 Object Lock cannot be enabled on the target bucket\.

To enable log delivery:

1. Provide the name of the target bucket where you want Amazon S3 to save the access logs as objects\. Both the source and target buckets must be in the same AWS Region and owned by the same account\. 

   You can have logs delivered to any bucket that you own that is in the same Region as the source bucket, including the source bucket itself\. But for simpler log management, we recommend that you save access logs in a different bucket\. 

   When your source bucket and target bucket are the same bucket, additional logs are created for the logs that are written to the bucket\. This might not be ideal because it could result in a small increase in your storage billing\. In addition, the extra logs about logs might make it harder to find the log that you are looking for\. If you choose to save access logs in the source bucket, we recommend that you specify a prefix for all log object keys so that the object names begin with a common string and the log objects are easier to identify\. 

   [Key prefixes](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#keyprefix) are also useful to distinguish between source buckets when multiple buckets log to the same target bucket\.

1. \(Optional\) Assign a prefix to all Amazon S3 log object keys\. The prefix makes it simpler for you to locate the log objects\. For example, if you specify the prefix value `logs/`, each log object that Amazon S3 creates begins with the `logs/` prefix in its key\.

   ```
   logs/2013-11-01-21-32-16-E568B2907131C0C0
   ```

   The key prefix can also help when you delete the logs\. For example, you can set a lifecycle configuration rule for Amazon S3 to delete objects with a specific key prefix\. 

1. \(Optional\) Set permissions so that others can access the generated logs\. By default, only the bucket owner always has full access to the log objects\. 




Amazon S3 uses the following object key format for the log objects it uploads in the target bucket:

```
TargetPrefixYYYY-mm-DD-HH-MM-SS-UniqueString/
```

In the key, `YYYY`, `mm`, `DD`, `HH`, `MM`, and `SS` are the digits of the year, month, day, hour, minute, and seconds \(respectively\) when the log file was delivered\. These dates and times are in Coordinated Universal Time \(UTC\)\. 

A log file delivered at a specific time can contain records written at any point before that time\. There is no way to know whether all log records for a certain time interval have been delivered or not\. 

The `UniqueString` component of the key is there to prevent overwriting of files\. It has no meaning, and log processing software should ignore it\. 

The trailing slash */* is required to denote the end of the prefix\.


Amazon S3 periodically collects access log records, consolidates the records in log files, and then uploads log files to your target bucket as log objects\. If you enable logging on multiple source buckets that identify the same target bucket, the target bucket will have access logs for all those source buckets\. However, each log object reports access log records for a specific source bucket\. 

Amazon S3 uses a special log delivery account, called the *Log Delivery* group, to write access logs\. These writes are subject to the usual access control restrictions\. You must grant the Log Delivery group write permission on the target bucket by adding a grant entry in the bucket's access control list \(ACL\)\. If you use the Amazon S3 console to enable logging on a bucket, the console both enables logging on the source bucket and updates the ACL on the target bucket to grant write permission to the Log Delivery group\.

#### Best effort server log delivery<a name="LogDeliveryBestEffort"><!-- omit in toc -->

Server access log records are delivered on a best effort basis\. Most requests for a bucket that is properly configured for logging result in a delivered log record\. Most log records are delivered within a few hours of the time that they are recorded, but they can be delivered more frequently\. 

The completeness and timeliness of server logging is not guaranteed\. The log record for a particular request might be delivered long after the request was actually processed, or *it might not be delivered at all*\. The purpose of server logs is to give you an idea of the nature of traffic against your bucket\. It is rare to lose log records, but server logging is not meant to be a complete accounting of all requests\. 

It follows from the best\-effort nature of the server logging feature that the usage reports available at the AWS portal \(Billing and Cost Management reports on the [AWS Management Console](https://console.aws.amazon.com/)\) might include one or more access requests that do not appear in a delivered server log\. 

#### Bucket logging status changes take effect over time<a name="BucketLoggingStatusChanges"><!-- omit in toc -->

Changes to the logging status of a bucket take time to actually affect the delivery of log files\. For example, if you enable logging for a bucket, some requests made in the following hour might be logged, while others might not\. If you change the target bucket for logging from bucket A to bucket B, some logs for the next hour might continue to be delivered to bucket A, while others might be delivered to the new target bucket B\. In all cases, the new settings eventually take effect without any further action on your part\. 

## Operational Best Practices
### 1. Resource Tags
**What, Why & How?**  

Identification of your IT assets is a crucial aspect of governance and security. You need to have visibility of all your Amazon S3 resources to assess their security posture and take action on potential areas of weakness.

Tagging resources in the cloud is an easy way for teams to provide information related to who owns the resource, what the resource is used for, as well as other important information related to the deployment lifecycle of the resource. CG has mandated that all cloud resources are to be tagged with for cross-team use. Although most of the mandatory tags will be added through automation, one should still check to make sure that all newly deployed recources have the appropriate tags attached. Please see the documentation below for the latest tagging standards.

<br><br>

### 2. Enable AWS Config
**Why?**

AWS Config allows you to monitor your AWS resources, to assess, audit, and record configurations and changes. It is essentially a database to keep track of the historical metadata for the resources in an account. It allows for Continous monitoring, assessment, change management, and troubleshooting.

**How?**  

First you need to make sure that you have AWS Config set up. Follow the links for more information on AWS Config and how to set them up.

  - AWS Config Setup: https://docs.aws.amazon.com/config/latest/developerguide/gs-console.html  
  - AWS Config Runboook: https://github.com/open-itg/aws_runbooks/blob/master/config/Runbook.md  

Next we need to Enable AWS config and Amazon S3 Montioring:

1. Sign into the AWS Management Console and open the AWS Config console.
2. If this is your first time using AWS Config, select Get started. If you’ve already used AWS Config, select Settings.
3. In the Settings page, under Resource types to record, clear the All resources checkbox. In the Specific types list, select Bucket under S3.
4. Choose the Amazon S3 bucket for storing configuration history and snapshots. We’ll create a new Amazon S3 bucket.  
    - If you prefer to use an existing Amazon S3 bucket in your account, select the Choose a bucket from your account radio button and, using the dropdown, select an existing bucket.
5. Under Amazon SNS topic, check the box next to Stream configuration changes and notifications to an Amazon SNS topic, and then select the radio button to Create a topic.
6. Under AWS Config role, choose Create a role (unless you already have a role you want to use). We’re using the auto-suggested role name.
7. Select Next.
8. Configure Amazon S3 bucket monitoring rules:
    - On the AWS Config rules page, search for S3 and choose the s3-bucket-publice-read-prohibited and s3-bucket-public-write-prohibited rules, then click Next.
    - On the Review page, select Confirm. AWS Config is now analyzing your Amazon S3 buckets, capturing their current configurations, and evaluating the configurations against the rules we selected.
9. If you created a new Amazon SNS topic, open the Amazon SNS Management Console and locate the topic you created:
10.Copy the ARN of the topic (the string that begins with arn:) because you’ll need it in a later step.
11. Select the checkbox next to the topic, and then, under the Actions menu, select Subscribe to topic.
12. Select Email as the protocol, enter your email address, and then select Create subscription.  
After several minutes, you’ll receive an email asking you to confirm your subscription for notifications for this topic. Select the link to confirm the subscription.



<br><br>

### 3. Enable AWS Trusted Advisor
**What, Why & How?** 

Trust Advisor checks your AWS environment and makes recommendations when opportunities exist to save money, improve system availability and performance, or help close security gaps.

Trusted Advisor has the following Amazon S3-related checks:

  - Logging configuration of Amazon S3 buckets.
  - Security checks for Amazon S3 buckets that have open access permissions.
  - Fault tolerance checks for Amazon S3 buckets that don't have versioning enabled, or have versioning suspended.

More info on [Trusted Advisor](https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html) in the *AWS Support Guide*

<br><br>

### 4.  Utilize Lifecycle Management
**Why?**

Managing your storage lifecycle is increasingly important as organizations seek to limit storage investments, especially when it comes to operating expenses like cloud storage. Lifecycle management allow a user's predetermined rules to automatically migrate data objects to other storage options or to schedule unnecessary or expired data to be deleted.

More information on managing storage lifecycle: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html

**How?**
1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.
2. In the Buckets list, choose the name of the bucket that you want to create a lifecycle rule for.
3. Choose the Management tab, and choose Create lifecycle rule.
4. In Lifecycle rule name, enter a name for your rule.
(The name must be unique within the bucket.)
5. Choose the scope of the lifecycle rule:
    - To apply this lifecycle rule to all objects with a specific prefix or tag, choose Limit the scope to specific prefixes or tags.
      - To limit the scope by prefix, in Prefix, enter the prefix.
      - To limit the scope by tag, choose Add tag, and enter the tag key and value.
    - For more information about object name prefixes, see Creating object key names. For more information about object tags, see Categorizing your storage using tags.
    - To apply this lifecycle rule to all objects in the bucket, choose This rule applies to all objects in the bucket, and choose I acknowledge that this rule applies to all objects in the bucket.
6. Under Lifecycle rule actions, choose the actions that you want your lifecycle rule to perform:
    - Transition current versions of objects between storage classes
    - Transition previous versions of objects between storage classes
    - Expire current versions of objects
    - Permanently delete previous versions of objects
    - Delete expired delete markers or incomplete multipart uploads
Depending on the actions that you choose, different options appear.
7. To transition *current* versions of objects between storage classes, under Transition current versions of objects between storage classes:  
  a. In Storage class transitions, choose the storage class to transition to:
    - Standard-IA
    - Intelligent-Tiering
    - One Zone-IA
    - Glacier
    - Glacier Deep Archive

    b. In Days after object creation, enter the number of days after creation to transition the object.
8. Follow step 7 for *non-current* versions.
9. To expire current versions of objects, under Expire previous versions of objects, in Number of days after object creation, enter the number of days.
>**NOTE**  
>In a non-versioned bucket the expiration action results in Amazon S3 permanently removing the object. For more information about lifecycle actions, see [Elements to describe lifecycle actions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-actions).
10. To permanently delete previous versions of objects, under Permanently delete previous versions of objects, in Number of days after objects become previous versions, enter the number of days.
11. Under Delete expired delete markers or incomplete multipart uploads, choose Delete expired object delete markers and Delete incomplete multipart uploads. Then, enter the number of days after the multipart upload initiation that you want to end and clean up incomplete multipart uploads.
12. Choose Create rule.  
If the rule does not contain any errors, Amazon S3 enables it, and you can see it on the Management tab under Lifecycle rules.

<br><br>

## Endnotes


## Capital Group Control Statements 
1. All Data-at-rest must be encrypted and use a CG BYOK encryption key.
2. All Data-in-transit must be encrypted using certificates using CG Certificate Authority.
3. Keys storied in a Key Management System (KMS) should be created by Capital Group's hardware security module (HSM) and are a minimum of AES-256.
4. AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch.
5. AWS IAM User accounts are only to be created for use by services or products that do not support IAM Roles. Services are not allowed to create local accounts for human use within the service. All human user authentication will take place within CG’s Identity Provider.
6. Any AWS service used by CG should not be directly available to the Internet and the default route is always the CG gateway.
7. Use of AWS IAM accounts are restricted to CG networks.
8. AWS IAM User secrets, including passwords and secret access keys, are to be rotated every 90 days. Accounts created locally within any service must also have their secrets rotated every 90 days.
9. Encryption keys are rotated annually.
10. Administrative access to AWS resources will have MFA enabled

## Glossary
**Data** - Digital pieces of information stored or transmitted for use with an information system from which understandable information is derived. Items considered to be data are: Source code, meta-data, build artifacts, information input and output.  
 
**Information System** - An organized assembly of resources and procedures for the collection, processing, maintenance, use, sharing, dissemination, or disposition of information. All systems, platforms, compute instances including and not limited to physical and virtual client endpoints, physical and virtual servers, software containers, databases, Internet of Things (IoT) devices, network devices, applications (internal and external), Serverless computing instances (i.e. AWS Lambda), vendor provided appliances, and third-party platforms, connected to the Capital Group network or used by Capital Group users or customers.
 
**Log** - a record of the events occurring within information systems and networks. Logs are composed of log entries; each entry contains information related to a specific event that has occurred within a system or network.
 
**Information** - communication or representation of knowledge such as facts, data, or opinions in any medium or form, including textual, numerical, graphic, cartographic, narrative, or audiovisual. 
 
**Cloud Computing** - A model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.
 
**Vulnerability**  - Weakness in an information system, system security procedures, internal controls, or implementation that could be exploited or triggered by a threat source. Note: The term weakness is synonymous for deficiency. Weakness may result in security and/or privacy risks.