<img src="https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png" alt="AWS" width="250"/>

# AWS DynamoDB - Security Runbook <!-- omit in toc -->
## Capgroup Cybersecurity Control Alignment <!-- omit in toc -->

**Generated By:**  
[Josh Linus (JHZL)](https://cgweb3/profile/JHZL)  
Security Engineering

**Last Update:** *08/08/2021*

<!---
Table of Contents
- [Disclaimer](#disclaimer)
- [Overview](#overview)
- [Cloud Security Requirements](#cloud-security-requirements)
  - [1. Enforce least privilege for all S3 users and roles](#1-Enforce-least-privilege-for-all-S3-users-and-roles)
  - [?. Buckets are encrypted using CG CMK] (#?-Buckets-are-encrypted-using-CG-CMK)
  - [?. Data in Transit is encrypted using TLS 1.2](#?-Data-in-Transit-is-encrypted-using-TLS-1.2)
  - [?. Block Public Access](#?-Block-Public-Access)
  - [?. S3 Utilizes VPC Endpoints to Prevent Public Access](#?-EBS-Utilizes-VPC-Endpoints-to-Prevent-Public-Access)
  - [?. Enable Access Server Logs](#?-Enable-Access-Server-Logs) 
  - [?. Enable S3 Replication](#?-Enable-S3-Replication) 
  - [?. Enable S3 Versioning](#?-Enable-S3-Versioning)
  - [?. CloudTrail logging enabled for S3](#?-CloudTrail-logging-enabled-for-S3)
  - [?. CloudWatch logging enabled for S3](#?-CloudWatch-logging-enabled-for-S3)
- [Operational Best Practices](#Operational-Best-Practices)
  - [?. Resource Tags](#?-Resource-Tags)
  - [?. Enable AWS Config] (#?-Enable-AWS-Config)
  - [?. Enable AWS Trusted Advisor](#?-Enable-AWS-Trusted-Advisor)
  - [?. Utilize Lifecycle Management](#?-Utilize-Lifecycle-Management)
- [Endnotes](#endnotes)
- [Capital Group Control Statements](#capital-group-control-statements)
- [Glossary](#glossary)
-->

## Table of Contents <!-- omit in toc -->
- [Overview](#overview)
- [Preventative Controls](#preventative-controls)
  - [1. Manage Strict Access Controls to Protect Data from Unauthorized Access](#1-manage-strict-access-controls-to-protect-data-from-unauthorized-access)
  - [2. Employ MFA for Sensitive S3 Resources](#2-employ-mfa-for-sensitive-s3-resources)
  - [3. Enforce Data Protection Measures](#3-enforce-data-protection-measures)
  - [4. Enhance Durability of Critical Data](#4-enhance-durability-of-critical-data)
  - [5. Prevent S3 Bucket Sniping](#5-prevent-s3-bucket-sniping)
  - [6. Isolate S3 Bucket Access from the Internet](#6-isolate-s3-bucket-access-from-the-internet)
- [Detective](#detective)
  - [1. Ensure Amazon S3 Inventory is enabled and monitored regularly](#1-ensure-amazon-s3-inventory-is-enabled-and-monitored-regularly)
  - [2. Enable Amazon S3 server access logging](#2-enable-amazon-s3-server-access-logging)
  - [3. Implement monitoring for Amazon S3 utilizing Amazon CloudWatch metrics](#3-implement-monitoring-for-amazon-s3-utilizing-amazon-cloudwatch-metrics)
  - [4. Utilize AWS CLoudtrail to log Amazon S3 API calls](#4-utilize-aws-cloudtrail-to-log-amazon-s3-api-calls)
  - [5. Enable AWS Config rules for Amazon S3](#5-enable-aws-config-rules-for-amazon-s3)
  - [6. Use AWS CloudTrail to Identify Amazon S3 request](#6-use-aws-cloudtrail-to-identify-amazon-s3-request)
- [Respond/Recover](#respondrecover)
  - [1. Utilize Amazon SNS topics or SQS queue](#1-utilize-amazon-sns-topics-or-sqs-queue)
- [Endnotes](#endnotes)
- [Capital Group Control Statements](#capital-group-control-statements)
- [Glossary](#glossary)

## Overview
Amazon S3 is a core service offered by AWS that provides object storage. It allows you to store and retrieve any amount of data, at any time, from anywhere on the web. It gives any developer access to highly scalable, reliable, fast, and inexpensive data storage infrastructure; one of the foundational components to any platform.

<img src="/docs/img/s3/s3.png" width="800"><br>

### Use Case Examples:
 - Data Lakes
 - Running Cloud Native Applications
 - Backup and Restore Data
 - Archive Data 
 - Webites

<br><br>

## Cloud Security Requirements
### 1. Enforce least privilege for all S3 users and roles
**Why?** Controls provide reasonable assurance that logical access to data is restricted to authorized and appropriate users, and such users are restricted to performing authorized and appropriate actions.

Controls provide reasonable assurance that privileged functions are managed and monitored commensurate with the risk level.

Controls provide reasonable assurance that users, devices, and other assets are authenticated commensurate with the risk level.

**How?** Manage permissions with IAM User Policies, S3 Bucket Policies, and S3 Access Control Lists (ACLs) to ensure appropriateness of access. IAM Roles should be used to manage temporary credentials for applications and AWS Services that need access to S3.  The assigned Role will supply temporary permissions that applications can use when they make calls to other AWS resources.

IAM provides permissions on which API calls can be made by users to the service as well as who/what can access the service at a bucket and object level. Bucket policies and ACLs can be used separately and also in conjunction with IAM to control access to S3 buckets and objects.

The following S3 bucket policy denies permissions to any user to perform any Amazon S3 operations on objects in the specified S3 bucket unless the request originates from the range of IP addresses specified in the condition.

> **Important**  
> This statement identifies the 54.240.143.0/24 as the range of allowed IP addresses.  
> Replace the IP address range in this example with an appropriate value for your use case before using this policy. **Otherwise, you will lose the ability to access your bucket.**

```json
{
  "Version": "2012-10-17",
  "Id": "S3PolicyId1",
  "Statement": [
    {
      "Sid": "IPAllow",
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:*",
      "Resource": [
	 "arn:aws:s3:::awsexamplebucket1",
         "arn:aws:s3:::awsexamplebucket1/*"
      ],
      "Condition": {
	 "NotIpAddress": {"aws:SourceIp": "54.240.143.0/24"}
      }
    }
  ]
}
```

### 2. Employ MFA for Sensitive S3 Resources
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.AC-4|Access permissions and authorizations are managed, incorporating the principles of least privilege and separation of duties|
|PR.AC-6|Identities are proofed and bound to credentials and asserted in interactions|
|PR.AC-7|Users, devices, and other assets are authenticated (e.g., single-factor, multi-factor) commensurate with the risk of the transaction (e.g., individuals’ security and privacy risks and other organizational risks)|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|10|Administrative access to AWS resources will have MFA enabled|

**Why?** S3 supports MFA-protected API access, a feature that can enforce multi-factor authentication (MFA) for access to your Amazon S3 resources. Multi-factor authentication provides an extra level of security that you can apply to your AWS environment. It is a security feature that requires users to prove physical possession of an MFA device by providing a valid MFA code. For more information, see [Endnote 1](#endnote-1). You can require MFA for any requests to access your Amazon S3 resources.

**How?** You can enforce the MFA requirement using the `aws:MultiFactorAuthAge` key in a bucket policy. AWS Identity and Access Management (IAM) users can access Amazon S3 resources by using temporary credentials issued by the AWS Security Token Service (AWS STS). You provide the MFA code at the time of the AWS STS request.

The following bucket policy includes two policy statements. One statement allows the `s3:GetObject` permission on a bucket (`awsexamplebucket1`) to everyone. Another statement further restricts access to the `awsexamplebucket1/examplefolder` folder in the bucket by requiring MFA.  
The `Null` condition in the Condition block evaluates to true if the `aws:MultiFactorAuthAge` key value is null, indicating that the temporary security credentials in the request were created without the MFA key.
```json
{
    "Version": "2012-10-17",
    "Id": "123",
    "Statement": [
      {
        "Sid": "",
        "Effect": "Deny",
        "Principal": "*",
        "Action": "s3:*",
        "Resource": "arn:aws:s3:::awsexamplebucket1/examplefolder/*",
        "Condition": { "Null": { "aws:MultiFactorAuthAge": true } }
      },
      {
        "Sid": "",
        "Effect": "Allow",
        "Principal": "*",
        "Action": ["s3:GetObject"],
        "Resource": "arn:aws:s3:::awsexamplebucket1/*"
      }
    ]
 }
```

### 3. Enforce Data Protection Measures
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.DS-1|Data-at-rest is protected|
|PR.DS-2|Data-in-transit is protected|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|1|All Data-at-rest must be encrypted and use a CG BYOK encryption key.|
|2|All Data-in-transit must be encrypted using certificates using CG Certificate Authority.|
|3|Keys storied in a Key Management System (KMS) should be created by Capital Group's hardware security module (HSM) and are a minimum of AES-256.|

**Why?** Controls provide reasonable assurance that data is encrypted at-rest and in-transit throughout the lifecycle using a NIST-approved, CG-compliant encryption mechanism.

**How?**
#### Requiring Server-Side Encryption <!-- omit in toc -->
To require server-side encryption of all objects in a particular Amazon S3 bucket, you can use a policy. For example, the following bucket policy denies upload object (`s3:PutObject`) permission to everyone if the request does not include the `x-amz-server-side-encryption` header requesting server-side encryption with SSE-KMS. The policy also specifies that a particular key must be used. 
```json
{
   "Version":"2012-10-17",
   "Id":"PutObjectPolicy",
   "Statement":[{
         "Sid":"DenyUnEncryptedObjectUploads",
         "Effect":"Deny",
         "Principal":"*",
         "Action":"s3:PutObject",
         "Resource":"arn:aws:s3:::awsexamplebucket1/*",
         "Condition":{
            "StringNotEquals":{
               "s3:x-amz-server-side-encryption":"aws:kms"
            },
            "StringEquals":{
               "s3:x-amz-server-side-encryption-aws-kms-key-id":"arn:aws:kms:region:acct-id:key/key-id"
            }
         }
      }
   ]
}
```

#### Using S3 Block Public Access <!-- omit in toc -->
The following `put-public-access-block` example sets a restrictive block public access configuration for the specified bucket.
```
aws s3api put-public-access-block \
    --bucket my-bucket \
    --public-access-block-configuration "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
```
For instructions on setting up Block Public Access on a bucket from the S3 Console, see [Endnote 2](#endnote-2).

The following `put-public-access-block` example toggles all block public access settings to true for the specified account.
```
aws s3control put-public-access-block \
    --account-id 123456789012 \
    --public-access-block-configuration '{"BlockPublicAcls": true, "IgnorePublicAcls": true, "BlockPublicPolicy": true, "RestrictPublicBuckets": true}'
```

Once S3 Block Public Access is set up, add a Service Control Policy to deny `s3:PutAccountPublicAccessBlock`.

#### Enforce Encryption of Data-In-Transit <!-- omit in toc -->
All communications with the S3 API are encrypted using TLS, but file transfers to/from S3 buckets do not require secure transport by default. This can be changed, using S3 bucket policies.

The following statement uses the Bool condition operator with the `aws:SecureTransport` key to specify that the request must use Secure Transport.
```json
{
  "Version": "2012-10-17",
  "Statement": {
    "Effect": "Allow",
    "Action": "s3:*",
    "Resource": "arn:aws:s3:::awsexamplebucket1/*",
    "Condition": {"Bool": {"aws:SecureTransport": "true"}}
  }
}
```

### 4. Enhance Durability of Critical Data
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.IP-4|Backups of information are conducted, maintained, and tested|
|PR.PT-5|Mechanisms (e.g., failsafe, load balancing, hot swap) are implemented to achieve resilience requirements in normal and adverse situations|

**Why?** The AWS global infrastructure is built around Regions and Availability Zones. AWS Regions provide multiple, physically separated and isolated Availability Zones that are connected with low latency, high throughput, and highly redundant networking. These Availability Zones offer you an effective way to design and operate applications and databases. They are more highly available, fault tolerant, and scalable than traditional single data center infrastructures or multi-data center infrastructures. If you specifically need to replicate your data over greater geographic distances, you can use Replication, which enables automatic, asynchronous copying of objects across buckets in different AWS Regions.

Each AWS Region has multiple Availability Zones. You can deploy your applications across multiple Availability Zones in the same Region for fault tolerance and low latency. Availability Zones are connected to each other with fast, private fiber-optic networking, enabling you to easily architect applications that automatically fail over between Availability Zones without interruption.

In addition to the AWS global infrastructure, Amazon S3 offers several features to help support your data resiliency and backup needs.

**How?**
#### Setting Up S3 Replication <!-- omit in toc -->
Create a role for the replication service to use. This role will need a policy attached to it similar to the following:  
```json
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Action":[
            "s3:ListBucket",
            "s3:GetReplicationConfiguration",
            "s3:GetObjectVersionForReplication",
            "s3:GetObjectVersionAcl"
         ],
         "Effect":"Allow",
         "Resource":[
            "arn:aws:s3:::source",
            "arn:aws:s3:::source/*"
         ]
      },
      {
         "Action":[
            "s3:ReplicateObject",
            "s3:ReplicateDelete",
            "s3:ReplicateTags",
            "s3:GetObjectVersionTagging"
         ],
         "Effect":"Allow",
         "Condition":{
            "StringLikeIfExists":{
               "s3:x-amz-server-side-encryption":[
                  "aws:kms",
                  "AES256"
               ],
               "s3:x-amz-server-side-encryption-aws-kms-key-id":[
                  "AWS KMS key IDs(in ARN format) to use for encrypting object replicas"  
               ]
            }
         },
         "Resource":"arn:aws:s3:::destination/*"
      },
      {
         "Action":[
            "kms:Decrypt"
         ],
         "Effect":"Allow",
         "Condition":{
            "StringLike":{
               "kms:ViaService":"s3.us-east-1.amazonaws.com",
               "kms:EncryptionContext:aws:s3:arn":[
                  "arn:aws:s3:::source/*"
               ]
            }
         },
         "Resource":[
            "AWS KMS key IDs(in ARN format) used to encrypt source objects." 
         ]
      },
      {
         "Action":[
            "kms:Encrypt"
         ],
         "Effect":"Allow",
         "Condition":{
            "StringLike":{
               "kms:ViaService":"s3.us-west-2.amazonaws.com",
               "kms:EncryptionContext:aws:s3:arn":[
                  "arn:aws:s3:::destination/*"
               ]
            }
         },
         "Resource":[
            "AWS KMS key IDs(in ARN format) to use for encrypting object replicas" 
         ]
      }
   ]
}
```

Run the following CLI command:  
```
aws s3api put-bucket-replication \
    --bucket my-bucket \
    --replication-configuration file://replication.json
```

This will be the contents of the `replication.json` file referenced in the previous command. It outlines the rules for replication.  
```json
{
    "Role": "arn:aws:iam::123456789012:role/s3-replication-role",
    "Rules": [
        {
            "Status": "Enabled",
            "SourceSelectionCriteria": {
              "SseKmsEncryptedObjects": {
                "Status": "Enabled"
              }
            },
            "Priority": 1,
            "DeleteMarkerReplication": { "Status": "Disabled" },
            "Filter" : { "Prefix": ""},
            "Destination": {
                "Bucket": "arn:aws:s3:::my-bucket-backup",
                "EncryptionConfiguration": {
                  "ReplicaKmsKeyID": "AWS KMS key IDs(in ARN format) to use for encrypting object replicas"
                }
            }
        }
    ]
}
```

Further information on setting up replication can be found in [Endnote 3](#endnote-3).

#### Setting up S3 Versioning <!-- omit in toc -->
Each bucket you create has a versioning subresource associated with it. By default, your bucket is unversioned, and accordingly the versioning subresource stores empty versioning configuration.
```xml
<VersioningConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
</VersioningConfiguration>
```
To enable versioning, you send a request to Amazon S3 with a versioning configuration that includes a status.
```xml
<VersioningConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
  <Status>Enabled</Status> 
</VersioningConfiguration>
```
To suspend versioning, you set the status value to `Suspended`.

#### MFA delete <!-- omit in toc -->
You can optionally add another layer of security by configuring a bucket to enable MFA (multi-factor authentication) Delete, which requires additional authentication for either of the following operations:

- Change the versioning state of your bucket
- Permanently delete an object version

MFA Delete requires two forms of authentication together:

- Your security credentials
- The concatenation of a valid serial number, a space, and the six-digit code displayed on an approved authentication device

MFA Delete thus provides added security in the event, for example, your security credentials are compromised.

To enable or disable MFA Delete, you use the same API that you use to configure versioning on a bucket. Amazon S3 stores the MFA Delete configuration in the same versioning subresource that stores the bucket's versioning status.
```xml
<VersioningConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
  <Status>VersioningState</Status>
  <MfaDelete>MfaDeleteState</MfaDelete>  
</VersioningConfiguration>
```

### 5. Prevent S3 Bucket Sniping
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.IP-6|Data is destroyed according to policy|
|PR.IP-7|Protection processes are improved|
|PR.IP-8|Effectiveness of protection technologies is shared|

**Why?** While it is important to properly destroy S3 data in line with regulatory and internal requirements, it is prudent to maintain ownership of the S3 bucket name (by not deleting the bucket) to avoid someone getting access to a legacy bucket hoping for accidental and/or inadvertent uploads.

**How?** Maintain S3 Buckets by adding an Service Control Policy denying `s3:DeleteBucket` and `s3:DeleteBucketPolicy`.

When an S3 bucket is decommissioned, add a restrictive S3 bucket policy to the bucket which allows only the Account Administrators to perform `s3:*` actions. Remove all other statements from the bucket policy.

### 6. Isolate S3 Bucket Access from the Internet
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.PT-4|Communications and control networks are protected|
|PR.AC-3|Remote access is managed|
|PR.AC-5|Network integrity is protected (e.g., network segregation, network segmentation)|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|6|Any AWS service used by CG should not be directly available to the Internet and the default route is always the CG gateway.|
|7|Use of AWS IAM accounts are restricted to CG networks.|

**Why?** Controls provide reasonable assurance that direct access (with provided credentials) is not allowed to the AWS environment from outside the corporate network or AWS backbone.

**How?** Amazon S3 bucket policies can be used to control access to buckets from specific Amazon VPC endpoints, which will isolate network access to a given S3 bucket from only the specific VPC within the AWS network. 

The following is an example of an Amazon S3 bucket policy that restricts access to a specific bucket, `awsexamplebucket1`, only from the VPC endpoint with the ID `vpce-1a2b3c4d`. The policy denies all access to the bucket if the specified endpoint is not being used. The `aws:SourceVpce` condition is used to specify the endpoint. The `aws:SourceVpce` condition does not require an ARN for the VPC endpoint resource, only the VPC endpoint ID. 

> **Important**
> Before using the following example policy, replace the VPC endpoint ID with an appropriate value for your use case. Otherwise, you won't be able to access your bucket.

This policy disables console access to the specified bucket, because console requests don't originate from the specified VPC endpoint.
```json
{
   "Version": "2012-10-17",
   "Id": "Policy1415115909152",
   "Statement": [
     {
       "Sid": "Access-to-specific-VPCE-only",
       "Principal": "*",
       "Action": "s3:*",
       "Effect": "Deny",
       "Resource": ["arn:aws:s3:::awsexamplebucket1",
                    "arn:aws:s3:::awsexamplebucket1/*"],
       "Condition": {
         "StringNotEquals": {
           "aws:SourceVpce": "vpce-1a2b3c4d"
         }
       }
     }
   ]
}
```

## Detective
### 1. Ensure Amazon S3 Inventory is enabled and monitored regularly
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|ID.AM-1|physical devices and systems within the organization are inventoried|
|ID.AM-2|Software platforms and applications within the organization are inventoried|



**Why?** 
Amazon S3 inventory is one of the tools Amazon S3 provides to help manage your storage\. You can use it to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs\.

**How?** 
Set up Amazon S3 Inventory

#### How do I set up Amazon S3 inventory?<a name="storage-inventory-how-to-set-up"><!-- omit in toc -->

This section describes how to set up an inventory, including details about the inventory source and destination buckets\.

#### Amazon S3 inventory source and destination buckets<a name="storage-inventory-buckets"><!-- omit in toc -->

The bucket that the inventory lists the objects for is called the *source bucket*\. The bucket where the inventory list file is stored is called the *destination bucket*\. 

**Source Bucket**

The inventory lists the objects that are stored in the source bucket\. You can get inventory lists for an entire bucket or filtered by \(object key name\) prefix\.

The source bucket:
+ Contains the objects that are listed in the inventory\.
+ Contains the configuration for the inventory\.

**Destination Bucket**

Amazon S3 inventory list files are written to the destination bucket\. To group all the inventory list files in a common location in the destination bucket, you can specify a destination \(object key name\) prefix in the inventory configuration\.

The destination bucket:
+ Contains the inventory file lists\. 
+ Contains the manifest files that list all the file inventory lists that are stored in the destination bucket\. 
+ Must have a bucket policy to give Amazon S3 permission to verify ownership of the bucket and permission to write files to the bucket\. 
+ Must be in the same AWS Region as the source bucket\.
+ Can be the same as the source bucket\.
+ Can be owned by a different AWS account than the account that owns the source bucket\.

#### Setting up Amazon S3 inventory<a name="storage-inventory-setting-up"><!-- omit in toc -->

Amazon S3 inventory helps you manage your storage by creating lists of the objects in an S3 bucket on a defined schedule\. You can configure multiple inventory lists for a bucket\. The inventory lists are published to CSV, ORC, or Parquet files in a destination bucket\. 

The easiest way to set up an inventory is by using the AWS Management Console, but you can also use the REST API, AWS CLI, or AWS SDKs\. The console performs the first step of the following procedure for you: adding a bucket policy to the destination bucket\.

**To set up Amazon S3 inventory for an S3 bucket**

1. **Add a bucket policy for the destination bucket\.**

   You must create a bucket policy on the destination bucket to grant permissions to Amazon S3 to write objects to the bucket in the defined location\. 

1. **Configure an inventory to list the objects in a source bucket and publish the list to a destination bucket\.**

   When you configure an inventory list for a source bucket, you specify the destination bucket where you want the list to be stored, and whether you want to generate the list daily or weekly\. You can also configure what object metadata to include and whether to list all object versions or only current versions\. 

   You can specify that the inventory list file be encrypted by using an Amazon S3 managed key \(SSE\-S3\) or an AWS Key Management Service \(AWS KMS\) customer managed customer master key \(CMK\)\.  If you plan to use SSE\-KMS encryption, see Step 3\.
   + For information about how to use the console to configure an inventory list, see [How Do I Configure Amazon S3 Inventory?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-inventory.html) in the *Amazon Simple Storage Service Console User Guide*\.
   + To use the Amazon S3 API to configure an inventory list, use the [PUT Bucket inventory configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTInventoryConfig.html) REST API or the equivalent from the AWS CLI or AWS SDKs\. 

1. **To encrypt the inventory list file with SSE\-KMS, grant Amazon S3 permission to use the CMK stored in AWS KMS\.**

   You can configure encryption for the inventory list file by using the AWS Management Console, REST API, AWS CLI, or AWS SDKs\. Whichever way you choose, you must grant Amazon S3 permission to use the AWS KMS customer managed CMK to encrypt the inventory file\. You grant Amazon S3 permission by modifying the key policy for the customer managed CMK that you want to use to encrypt the inventory file\. 

#### Granting Amazon S3 permission to use your AWS KMS CMK for encryption<a name="storage-inventory-kms-key-policy"><!-- omit in toc -->

To grant Amazon S3 permission to encrypt using a customer managed AWS Key Management Service \(AWS KMS\) customer master key \(CMK\), you must use a key policy\. To update your key policy so that you can use an AWS KMS customer managed CMK to encrypt the inventory file, follow these steps\.

**To grant permissions to encrypt using your AWS KMS CMK**

1. Using the AWS account that owns the customer managed CMK, sign into the AWS Management Console\.

1. Open the AWS KMS console at [https://console\.aws\.amazon\.com/kms](https://console.aws.amazon.com/kms)\.

1. To change the AWS Region, use the Region selector in the upper\-right corner of the page\.

1. In the left navigation pane, choose **Customer managed keys**\.

1. Under **Customer managed keys**, choose the key that you want to use to encrypt the inventory file\. CMKs are Region specific and must be in the same Region as the source bucket\.

1. Under **Key policy**, choose **Switch to policy view**\.

1. To update the key policy, choose **Edit**\.

1. Under **Edit key policy**, add the following key policy to the existing key policy\.

   ```
   {
       "Sid": "Allow Amazon S3 use of the CMK",
       "Effect": "Allow",
       "Principal": {
           "Service": "s3.amazonaws.com"
       },
       "Action": [
           "kms:GenerateDataKey"
       ],
       "Resource": "*"
   }
   ```

1. Choose **Save changes**\.

   For more information about creating AWS KMS customer managed CMKs and using key policies, see the following topics in the *AWS Key Management Service Developer Guide*:
   + [Getting Started](https://docs.aws.amazon.com/kms/latest/developerguide/getting-started.html)
   + [Using Key Policies in AWS KMS](https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html)

   You can also use the AWS KMS PUT key policy API [PutKeyPolicy](https://docs.aws.amazon.com/kms/latest/APIReference/API_PutKeyPolicy.html) to copy the key policy to the customer managed CMK that you want to use to encrypt the inventory file\. 

#### What's included in an Amazon S3 inventory?<a name="storage-inventory-contents"><!-- omit in toc -->

An inventory list file contains a list of the objects in the source bucket and metadata for each object\. The inventory lists are stored in the destination bucket as a CSV file compressed with GZIP, as an Apache optimized row columnar \(ORC\) file compressed with ZLIB, or as an Apache Parquet \(Parquet\) file compressed with Snappy\. 

The inventory list contains a list of the objects in an S3 bucket and the following metadata for each listed object: 
+ **Bucket name** – The name of the bucket that the inventory is for\.
+ **Key name** – Object key name \(or key\) that uniquely identifies the object in the bucket\. When using the CSV file format, the key name is URL\-encoded and must be decoded before you can use it\.
+ **Version ID** – Object version ID\. When you enable versioning on a bucket, Amazon S3 assigns a version number to objects that are added to the bucket\.  \(This field is not included if the list is only for the current version of objects\.\)
+ **IsLatest** – Set to `True` if the object is the current version of the object\. \(This field is not included if the list is only for the current version of objects\.\)
+ **Size** – Object size in bytes\.
+ **Last modified date** – Object creation date or the last modified date, whichever is the latest\.
+ **ETag** – The entity tag is a hash of the object\. The ETag reflects changes only to the contents of an object, not its metadata\. The ETag may or may not be an MD5 digest of the object data\. Whether it is depends on how the object was created and how it is encrypted\.
+ **Storage class** – Storage class used for storing the object\. 
+ **Intelligent\-Tiering access tier** – Access tier \(frequent or infrequent\) of the object if stored in Intelligent\-Tiering\. For more information, see [Amazon S3 Intelligent\-Tiering](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-dynamic-data-access)\.
+ **Multipart upload flag** – Set to `True` if the object was uploaded as a multipart upload\. 
+ **Delete marker** – Set to `True`, if the object is a delete marker\.  \(This field is automatically added to your report if you've configured the report to include all versions of your objects\)\.
+ **Replication status** – Set to `PENDING`, `COMPLETED`, `FAILED`, or `REPLICA.` 
+ **Encryption status** – Set to `SSE-S3`, `SSE-C`, `SSE-KMS`, or `NOT-SSE`\. The server\-side encryption status for SSE\-S3, SSE\-KMS, and SSE with customer\-provided keys \(SSE\-C\)\. A status of `NOT-SSE` means that the object is not encrypted with server\-side encryption\. 
+ **S3 Object Lock Retain until date** – The date until which the locked object cannot be deleted\. 
+ **S3 Object Lock Mode** – Set to `Governance` or `Compliance` for objects that are locked\. 
+ **S3 Object Lock Legal hold status ** – Set to `On` if a legal hold has been applied to an object; otherwise it is set to `Off`\. 

We recommend that you create a lifecycle policy that deletes old inventory lists\. 

#### Inventory consistency<a name="storage-inventory-contents-consistency"><!-- omit in toc -->

All of your objects might not appear in each inventory list\. The inventory list provides eventual consistency for PUTs of both new objects and overwrites, and DELETEs\. Inventory lists are a rolling snapshot of bucket items, which are eventually consistent \(that is, the list might not include recently added or deleted objects\)\. 

To validate the state of the object before you take action on the object, we recommend that you perform a `HEAD Object` REST API request to retrieve metadata for the object, or check the object's properties in the Amazon S3 console\. You can also check object metadata with the AWS CLI or the AWS SDKS\. For more information, see [HEAD Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html) in the *Amazon Simple Storage Service API Reference*\.

#### Where are inventory lists located?<a name="storage-inventory-location"><!-- omit in toc -->

When an inventory list is published, the manifest files are published to the following location in the destination bucket\.

```
 destination-prefix/source-bucket/config-ID/YYYY-MM-DDTHH-MMZ/manifest.json
 destination-prefix/source-bucket/config-ID/YYYY-MM-DDTHH-MMZ/manifest.checksum
 destination-prefix/source-bucket/config-ID/hive/dt=YYYY-MM-DD-HH-MM/symlink.txt
```
+ *destination\-prefix* is the \(object key name\) prefix set in the inventory configuration, which can be used to group all the inventory list files in a common location within the destination bucket\.
+ *source\-bucket* is the source bucket that the inventory list is for\. It is added to prevent collisions when multiple inventory reports from different source buckets are sent to the same destination bucket\.
+ *config\-ID* is added to prevent collisions with multiple inventory reports from the same source bucket that are sent to the same destination bucket\. The *config\-ID* comes from the inventory report configuration, and is the name for the report that is defined on setup\.
+ *YYYY\-MM\-DDTHH\-MMZ* is the timestamp that consists of the start time and the date when the inventory report generation begins scanning the bucket; for example, `2016-11-06T21-32Z`\.
+ `manifest.json` is the manifest file\. 
+ `manifest.checksum` is the MD5 of the content of the `manifest.json` file\. 
+ `symlink.txt` is the Apache Hive\-compatible manifest file\. 

The inventory lists are published daily or weekly to the following location in the destination bucket\.

```
      destination-prefix/source-bucket/config-ID/example-file-name.csv.gz
      ...
      destination-prefix/source-bucket/config-ID/example-file-name-1.csv.gz
```
+ *destination\-prefix* is the \(object key name\) prefix set in the inventory configuration\. It can be used to group all the inventory list files in a common location in the destination bucket\.
+ *source\-bucket* is the source bucket that the inventory list is for\. It is added to prevent collisions when multiple inventory reports from different source buckets are sent to the same destination bucket\.
+ *example\-file\-name*`.csv.gz` is one of the CSV inventory files\. ORC inventory names end with the file name extension `.orc`, and Parquet inventory names end with the file name extension `.parquet`\.

#### What is an inventory manifest?<a name="storage-inventory-location-manifest"><!-- omit in toc -->

The manifest files `manifest.json` and `symlink.txt` describe where the inventory files are located\. Whenever a new inventory list is delivered, it is accompanied by a new set of manifest files\. These files may overwrite each other and in versioning enabled buckets will create a new versions of the manifest files\. 

Each manifest contained in the `manifest.json` file provides metadata and other basic information about an inventory\. This information includes the following:
+ Source bucket name
+ Destination bucket name
+ Version of the inventory
+ Creation timestamp in the epoch date format that consists of the start time and the date when the inventory report generation begins scanning the bucket
+ Format and schema of the inventory files
+ Actual list of the inventory files that are in the destination bucket

Whenever a `manifest.json` file is written, it is accompanied by a `manifest.checksum` file that is the MD5 of the content of `manifest.json` file\.

The following is an example of a manifest in a `manifest.json` file for a CSV\-formatted inventory\.

```
{
    "sourceBucket": "example-source-bucket",
    "destinationBucket": "arn:aws:s3:::example-inventory-destination-bucket",
    "version": "2016-11-30",
    "creationTimestamp" : "1514944800000",
    "fileFormat": "CSV",
    "fileSchema": "Bucket, Key, VersionId, IsLatest, IsDeleteMarker, Size, LastModifiedDate, ETag, StorageClass, IsMultipartUploaded, ReplicationStatus, EncryptionStatus, ObjectLockRetainUntilDate, ObjectLockMode, ObjectLockLegalHoldStatus",
    "files": [
        {
            "key": "Inventory/example-source-bucket/2016-11-06T21-32Z/files/939c6d46-85a9-4ba8-87bd-9db705a579ce.csv.gz",
            "size": 2147483647,
            "MD5checksum": "f11166069f1990abeb9c97ace9cdfabc"
        }
    ]
}
```

The following is an example of a manifest in a `manifest.json` file for an ORC\-formatted inventory\.

```
{
    "sourceBucket": "example-source-bucket",
    "destinationBucket": "arn:aws:s3:::example-destination-bucket",
    "version": "2016-11-30",
    "creationTimestamp" : "1514944800000",
    "fileFormat": "ORC",
    "fileSchema": "struct<bucket:string,key:string,version_id:string,is_latest:boolean,is_delete_marker:boolean,size:bigint,last_modified_date:timestamp,e_tag:string,storage_class:string,is_multipart_uploaded:boolean,replication_status:string,encryption_status:string,object_lock_retain_until_date:timestamp,object_lock_mode:string,object_lock_legal_hold_status:string>",
    "files": [
        {
            "key": "inventory/example-source-bucket/data/d794c570-95bb-4271-9128-26023c8b4900.orc",
            "size": 56291,
            "MD5checksum": "5925f4e78e1695c2d020b9f6eexample"
        }
    ]
}
```

The following is an example of a manifest in a `manifest.json` file for a Parquet\-formatted inventory\.

```
{
    "sourceBucket": "example-source-bucket",
    "destinationBucket": "arn:aws:s3:::example-destination-bucket",
    "version": "2016-11-30",
    "creationTimestamp" : "1514944800000",
    "fileFormat": "Parquet",
    "fileSchema": "message s3.inventory { required binary bucket (UTF8); required binary key (UTF8); optional binary version_id (UTF8); optional boolean is_latest; optional boolean is_delete_marker;  optional int64 size;  optional int64 last_modified_date (TIMESTAMP_MILLIS);  optional binary e_tag (UTF8);  optional binary storage_class (UTF8);  optional boolean is_multipart_uploaded;  optional binary replication_status (UTF8);  optional binary encryption_status (UTF8);}"
  "files": [
        {
           "key": "inventory/example-source-bucket/data/d754c470-85bb-4255-9218-47023c8b4910.parquet",
            "size": 56291,
            "MD5checksum": "5825f2e18e1695c2d030b9f6eexample" 
        }
    ]
}
```

The `symlink.txt` file is an Apache Hive\-compatible manifest file that allows Hive to automatically discover inventory files and their associated data files\. The Hive\-compatible manifest works with the Hive\-compatible services Athena and Amazon Redshift Spectrum\. It also works with Hive\-compatible applications, including [Presto](https://prestodb.io/), [Apache Hive](https://hive.apache.org/), [Apache Spark](https://databricks.com/spark/about/), and many others\.

**Important**  
The `symlink.txt` Apache Hive\-compatible manifest file does not currently work with AWS Glue\.  
Reading `symlink.txt` with [Apache Hive](https://hive.apache.org/) and [Apache Spark](https://databricks.com/spark/about/) is not supported for ORC and Parquet\-formatted inventory files\. 

#### How do I know when an inventory is complete?<a name="storage-inventory-notification"><!-- omit in toc -->

You can set up an Amazon S3 event notification to receive notice when the manifest checksum file is created, which indicates that an inventory list has been added to the destination bucket\. The manifest is an up\-to\-date list of all the inventory lists at the destination location\.

Amazon S3 can publish events to an Amazon Simple Notification Service \(Amazon SNS\) topic, an Amazon Simple Queue Service \(Amazon SQS\) queue, or an AWS Lambda function\. For more information, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.

The following notification configuration defines that all `manifest.checksum` files newly added to the destination bucket are processed by the AWS Lambda `cloud-function-list-write`\.

```
<NotificationConfiguration>
  <QueueConfiguration>
      <Id>1</Id>
      <Filter>
          <S3Key>
              <FilterRule>
                  <Name>prefix</Name>
                  <Value>destination-prefix/source-bucket</Value>
              </FilterRule>
              <FilterRule>
                  <Name>suffix</Name>
                  <Value>checksum</Value>
              </FilterRule>
          </S3Key>
     </Filter>
     <Cloudcode>arn:aws:lambda:us-west-2:222233334444:cloud-function-list-write</Cloudcode>
     <Event>s3:ObjectCreated:*</Event>
  </QueueConfiguration>
  </NotificationConfiguration>
```

For more information, see [Using AWS Lambda with Amazon S3](https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html) in the *AWS Lambda Developer Guide*\.

#### Querying inventory with Amazon Athena<a name="storage-inventory-athena-query"><!-- omit in toc -->

You can query Amazon S3 inventory using standard SQL by using Amazon Athena in all Regions where Athena is available\. To check for AWS Region availability, see the [AWS Region Table](https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/)\. 

Athena can query Amazon S3 inventory files in ORC, Parquet, or CSV format\. When you use Athena to query inventory, we recommend that you use ORC\-formatted or Parquet\-formatted inventory files\. ORC and Parquet formats provide faster query performance and lower query costs\. ORC and Parquet are self\-describing type\-aware columnar file formats designed for [Apache Hadoop](http://hadoop.apache.org/)\. The columnar format lets the reader read, decompress, and process only the columns that are required for the current query\. The ORC and Parquet formats for Amazon S3 inventory are available in all AWS Regions\.

**To get started using Athena to query Amazon S3 inventory**

1. Create an Athena table\. For information about creating a table, see [Creating Tables in Amazon Athena](https://docs.aws.amazon.com/athena/latest/ug/creating-tables.html) in the *Amazon Athena User Guide*\.

   The following sample query includes all optional fields in an ORC\-formatted inventory report\. Drop any optional field that you did not choose for your inventory so that the query corresponds to the fields chosen for your inventory\. Also, you must use your bucket name and the location\. The location points to your inventory destination path; for example, `s3://destination-prefix/source-bucket/config-ID/hive/`\.

   ```
   CREATE EXTERNAL TABLE your_table_name(
     `bucket` string,
     key string,
     version_id string,
     is_latest boolean,
     is_delete_marker boolean,
     size bigint,
     last_modified_date timestamp,
     e_tag string,
     storage_class string,
     is_multipart_uploaded boolean,
     replication_status string,
     encryption_status string,
     object_lock_retain_until_date timestamp,
     object_lock_mode string,
     object_lock_legal_hold_status string
     )
     PARTITIONED BY (dt string)
     ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
     STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'
     OUTPUTFORMAT  'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
     LOCATION 's3://destination-prefix/source-bucket/config-ID/hive/';
   ```

    When using Athena to query a Parquet\-formatted inventory report, use the following Parquet SerDe in place of the ORC SerDe in the `ROW FORMAT SERDE` statement\.

   ```
   ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
   ```

    When using Athena to query a CSV\-formatted inventory report, use the following Parquet SerDe in place of the ORC SerDe in the `ROW FORMAT SERDE` statement\.

   ```
   ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
   ```

1. To add new inventory lists to your table, use the following `MSCK REPAIR TABLE` command\.

   ```
   MSCK REPAIR TABLE your-table-name;
   ```

1. After performing the first two steps, you can run ad hoc queries on your inventory, as shown in the following examples\. 

   ```
   # Get list of latest inventory report dates available
   SELECT DISTINCT dt FROM your-table-name ORDER BY 1 DESC limit 10;
             
   # Get encryption status for a provided report date.
   SELECT encryption_status, count(*) FROM your-table-name WHERE dt = 'YYYY-MM-DD-HH-MM' GROUP BY encryption_status;
             
   # Get encryption status for report dates in the provided range.
   SELECT dt, encryption_status, count(*) FROM your-table-name 
   WHERE dt > 'YYYY-MM-DD-HH-MM' AND dt < 'YYYY-MM-DD-HH-MM' GROUP BY dt, encryption_status;
   ```

For more information about using Athena, see [Amazon Athena User Guide](https://docs.aws.amazon.com/athena/latest/ug/)\.

#### Amazon S3 inventory REST APIs<a name="storage-inventory-related-resources"><!-- omit in toc -->

The following are the REST operations used for Amazon S3 inventory\.
+  [ DELETE Bucket Inventory ](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEInventoryConfiguration.html) 
+  [ GET Bucket Inventory](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETInventoryConfig.html) 
+  [ List Bucket Inventory](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketListInventoryConfigs.html) 
+  [ PUT Bucket Inventory](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTInventoryConfig.html) 
  

### 2. Enable Amazon S3 server access logging 
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|DE.AE-3|Event data are aggregated and correlated from multiple sources and sensors|
|DE.AE-4|Impact of events is determined|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch|

**Why?** 
Server access logging provides detailed records of the requests that are made to a bucket\. Server access logs can assist you in security and access audits, help you learn about your customer base, and understand your Amazon S3 bill\.

**How?** 
#### Enable Amazon S3 server access logging<a name="ServerLogs"><!-- omit in toc -->

Server access logging provides detailed records for the requests that are made to a bucket\. Server access logs are useful for many applications\. For example, access log information can be useful in security and access audits\. It can also help you learn about your customer base and understand your Amazon S3 bill\.

**Note**  
Server access logs don't record information about wrong\-region redirect errors for Regions that launched after March 20, 2019\. Wrong\-region redirect errors occur when a request for an object or bucket is made outside the Region in which the bucket exists\. 

#### How to enable server access logging<a name="server-access-logging-overview"><!-- omit in toc -->

To track requests for access to your bucket, you can enable server access logging\. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and an error code, if relevant\. 

There is no extra charge for enabling server access logging on an Amazon S3 bucket, and you are not charged when the logs are PUT to your bucket\. However, any log files that the system delivers to your bucket accrue the usual charges for storage\. You can delete these log files at any time\. Subsequent reads and other requests to these log files are charged normally, as for any other object, including data transfer charges\.

By default, logging is disabled\. When logging is enabled, logs are saved to a bucket in the same AWS Region as the source bucket\. 

To enable logging: 

1. Turn on logging on the Amazon S3 bucket that you want to monitor\. We refer to this bucket as the *source bucket*\. 

1. Grant the Amazon S3 Log Delivery group write permission on the bucket where you want the access logs saved\. We refer to this bucket as the *target bucket*\. 

**Note**  
In Amazon S3 you can grant permission to deliver access logs through bucket access control lists \(ACLs\), but not through bucket policy\.
Adding *deny* conditions to a bucket policy might prevent Amazon S3 from delivering access logs\.
[Default bucket encryption](bucket-encryption.html) on the target bucket *can only be used* if **AES256 \(SSE\-S3\)** is selected\. SSE\-KMS encryption is not supported\. 
S3 Object Lock cannot be enabled on the target bucket\.

To enable log delivery:

1. Provide the name of the target bucket where you want Amazon S3 to save the access logs as objects\. Both the source and target buckets must be in the same AWS Region and owned by the same account\. 

   You can have logs delivered to any bucket that you own that is in the same Region as the source bucket, including the source bucket itself\. But for simpler log management, we recommend that you save access logs in a different bucket\. 

   When your source bucket and target bucket are the same bucket, additional logs are created for the logs that are written to the bucket\. This might not be ideal because it could result in a small increase in your storage billing\. In addition, the extra logs about logs might make it harder to find the log that you are looking for\. If you choose to save access logs in the source bucket, we recommend that you specify a prefix for all log object keys so that the object names begin with a common string and the log objects are easier to identify\. 

   [Key prefixes](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#keyprefix) are also useful to distinguish between source buckets when multiple buckets log to the same target bucket\.

1. \(Optional\) Assign a prefix to all Amazon S3 log object keys\. The prefix makes it simpler for you to locate the log objects\. For example, if you specify the prefix value `logs/`, each log object that Amazon S3 creates begins with the `logs/` prefix in its key\.

   ```
   logs/2013-11-01-21-32-16-E568B2907131C0C0
   ```

   The key prefix can also help when you delete the logs\. For example, you can set a lifecycle configuration rule for Amazon S3 to delete objects with a specific key prefix\. 

1. \(Optional\) Set permissions so that others can access the generated logs\. By default, only the bucket owner always has full access to the log objects\. 


#### <a name="additional-logging-considerations"><!-- omit in toc -->

#### Log object key format<a name="server-log-keyname-format"><!-- omit in toc -->

Amazon S3 uses the following object key format for the log objects it uploads in the target bucket:

```
TargetPrefixYYYY-mm-DD-HH-MM-SS-UniqueString/
```

In the key, `YYYY`, `mm`, `DD`, `HH`, `MM`, and `SS` are the digits of the year, month, day, hour, minute, and seconds \(respectively\) when the log file was delivered\. These dates and times are in Coordinated Universal Time \(UTC\)\. 

A log file delivered at a specific time can contain records written at any point before that time\. There is no way to know whether all log records for a certain time interval have been delivered or not\. 

The `UniqueString` component of the key is there to prevent overwriting of files\. It has no meaning, and log processing software should ignore it\. 

The trailing slash */* is required to denote the end of the prefix\.

#### How are logs delivered?<a name="how-logs-delivered"><!-- omit in toc -->

Amazon S3 periodically collects access log records, consolidates the records in log files, and then uploads log files to your target bucket as log objects\. If you enable logging on multiple source buckets that identify the same target bucket, the target bucket will have access logs for all those source buckets\. However, each log object reports access log records for a specific source bucket\. 

Amazon S3 uses a special log delivery account, called the *Log Delivery* group, to write access logs\. These writes are subject to the usual access control restrictions\. You must grant the Log Delivery group write permission on the target bucket by adding a grant entry in the bucket's access control list \(ACL\)\. If you use the Amazon S3 console to enable logging on a bucket, the console both enables logging on the source bucket and updates the ACL on the target bucket to grant write permission to the Log Delivery group\.

#### Best effort server log delivery<a name="LogDeliveryBestEffort"><!-- omit in toc -->

Server access log records are delivered on a best effort basis\. Most requests for a bucket that is properly configured for logging result in a delivered log record\. Most log records are delivered within a few hours of the time that they are recorded, but they can be delivered more frequently\. 

The completeness and timeliness of server logging is not guaranteed\. The log record for a particular request might be delivered long after the request was actually processed, or *it might not be delivered at all*\. The purpose of server logs is to give you an idea of the nature of traffic against your bucket\. It is rare to lose log records, but server logging is not meant to be a complete accounting of all requests\. 

It follows from the best\-effort nature of the server logging feature that the usage reports available at the AWS portal \(Billing and Cost Management reports on the [AWS Management Console](https://console.aws.amazon.com/)\) might include one or more access requests that do not appear in a delivered server log\. 

#### Bucket logging status changes take effect over time<a name="BucketLoggingStatusChanges"><!-- omit in toc -->

Changes to the logging status of a bucket take time to actually affect the delivery of log files\. For example, if you enable logging for a bucket, some requests made in the following hour might be logged, while others might not\. If you change the target bucket for logging from bucket A to bucket B, some logs for the next hour might continue to be delivered to bucket A, while others might be delivered to the new target bucket B\. In all cases, the new settings eventually take effect without any further action on your part\. 

### 3. Implement monitoring for Amazon S3 utilizing Amazon CloudWatch metrics
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|DE.AE-2|Detected events are analyzed to understand attack targets and methods|
|DE.AE-3|Event data are aggregated and correlated from multiple sources and sensors|
|DE.AE-4|Impact of events is determined|
|DE.AE-5|Incident alert thresholds are established|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch|

**Why?** 
Amazon CloudWatch metrics for Amazon S3 can help you understand and improve the performance of applications that use Amazon S3\.

**How?** 
#### Monitoring metrics with Amazon CloudWatch<a name="cloudwatch-monitoring"><!-- omit in toc -->

 There are several ways that you can use CloudWatch with Amazon S3\.
+ **Daily storage metrics for buckets** ‐ Monitor bucket storage using CloudWatch, which collects and processes storage data from Amazon S3 into readable, daily metrics\. These storage metrics for Amazon S3 are reported once per day and are provided to all customers at no additional cost\.
+ **Request metrics** ‐ Monitor Amazon S3 requests to quickly identify and act on operational issues\. The metrics are available at 1\-minute intervals after some latency to process\. These CloudWatch metrics are billed at the same rate as the Amazon CloudWatch custom metrics\. 

  When enabled, request metrics are reported for all object operations\. By default, these 1\-minute metrics are available at the Amazon S3 bucket level\. You can also define a filter for the metrics collected using a shared prefix or object tag\. This allows you to align metrics filters to specific business applications, workflows, or internal organizations\.
+ **Replication metrics** ‐ Monitor the total number of S3 API operations that are pending replication, the total size of objects pending replication, and the maximum replication time to the destination Region\. Only replication rules that have S3 Replication Time Control \(S3 RTC\) enabled will publish replication metrics\. 

  Unlike request metrics, you can't filter replication metrics by prefixes and tags\. However, you can set up a replication rule based on prefixes and tags\. Your replication metrics will then monitor replication for the prefixes and tags that you specified\. 

All CloudWatch statistics are retained for a period of 15 months so that you can access historical information and gain a better perspective on how your web application or service is performing\. For more information, see [What Is Amazon CloudWatch?](https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/WhatIsCloudWatch.html) in the *Amazon CloudWatch User Guide*\.

#### Metrics and dimensions<a name="metrics-dimensions"><!-- omit in toc -->

The storage metrics and dimensions that Amazon S3 sends to CloudWatch are listed below\.

#### Amazon S3 CloudWatch Daily Storage Metrics for Buckets<a name="s3-cloudwatch-metrics"><!-- omit in toc -->

The `AWS/S3` namespace includes the following daily storage metrics for buckets\.


| Metric | Description | 
| --- | --- | 
| BucketSizeBytes |  The amount of data in bytes stored in a bucket in the STANDARD storage class, INTELLIGENT\_TIERING storage class, Standard \- Infrequent Access \(STANDARD\_IA\) storage class, OneZone \- Infrequent Access \(ONEZONE\_IA\), Reduced Redundancy Storage \(RRS\) class, Deep Archive Storage \(S3 Glacier Deep Archive\) class or, Glacier \(GLACIER\) storage class\. This value is calculated by summing the size of all objects in the bucket \(both current and noncurrent objects\), including the size of all parts for all incomplete multipart uploads to the bucket\.  Valid storage type filters: `StandardStorage`, `IntelligentTieringStorage`, `StandardIAStorage`, `StandardIASizeOverhead`, `StandardIAObjectOverhead`, `OneZoneIAStorage`, `OneZoneIASizeOverhead`, `ReducedRedundancyStorage`, `GlacierStorage`, `GlacierStagingStorage`, `GlacierObjectOverhead`, `GlacierS3ObjectOverhead`, `DeepArchiveStorage`, `DeepArchiveObjectOverhead`, `DeepArchiveS3ObjectOverhead` and, `DeepArchiveStagingStorage` \(see the `StorageType` dimension\)  Units: Bytes Valid statistics: Average  | 
| NumberOfObjects |  The total number of objects stored in a bucket for all storage classes except for the GLACIER storage class\. This value is calculated by counting all objects in the bucket \(both current and noncurrent objects\) and the total number of parts for all incomplete multipart uploads to the bucket\. Valid storage type filters: `AllStorageTypes` \(see the `StorageType` dimension\) Units: Count Valid statistics: Average  | 

#### Amazon S3 CloudWatch Request Metrics<a name="s3-request-cloudwatch-metrics"><!-- omit in toc -->

The `AWS/S3` namespace includes the following request metrics\.


| Metric | Description | 
| --- | --- | 
| AllRequests |  The total number of HTTP requests made to an Amazon S3 bucket, regardless of type\. If you're using a metrics configuration with a filter, then this metric only returns the HTTP requests made to the objects in the bucket that meet the filter's requirements\. Units: Count Valid statistics: Sum  | 
| GetRequests |  The number of HTTP GET requests made for objects in an Amazon S3 bucket\. This doesn't include list operations\. Units: Count Valid statistics: Sum  Paginated list\-oriented requests, like [List Multipart Uploads](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListMPUpload.html), [List Parts](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html), [Get Bucket Object versions](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETVersion.html), and others, are not included in this metric\.   | 
| PutRequests |  The number of HTTP PUT requests made for objects in an Amazon S3 bucket\. Units: Count Valid statistics: Sum  | 
| DeleteRequests |  The number of HTTP DELETE requests made for objects in an Amazon S3 bucket\. This also includes [Delete Multiple Objects](https://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html) requests\. This metric shows the number of requests, not the number of objects deleted\. Units: Count Valid statistics: Sum  | 
| HeadRequests |  The number of HTTP HEAD requests made to an Amazon S3 bucket\. Units: Count Valid statistics: Sum  | 
| PostRequests |  The number of HTTP POST requests made to an Amazon S3 bucket\. Units: Count Valid statistics: Sum  [Delete Multiple Objects](https://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html) and [SELECT Object Content](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html) requests are not included in this metric\.    | 
| SelectRequests |  The number of Amazon S3 [SELECT Object Content](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html) requests made for objects in an Amazon S3 bucket\.  Units: Count Valid statistics: Sum  | 
| SelectScannedBytes |  The number of bytes of data scanned with Amazon S3 [SELECT Object Content](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html) requests in an Amazon S3 bucket\.   Units: Bytes  Valid statistics: Average \(bytes per request\), Sum \(bytes per period\), Sample Count, Min, Max \(same as p100\), any percentile between p0\.0 and p99\.9  | 
| SelectReturnedBytes |  The number of bytes of data returned with Amazon S3 [SELECT Object Content](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html) requests in an Amazon S3 bucket\.   Units: Bytes  Valid statistics: Average \(bytes per request\), Sum \(bytes per period\), Sample Count, Min, Max \(same as p100\), any percentile between p0\.0 and p99\.9  | 
| ListRequests |  The number of HTTP requests that list the contents of a bucket\. Units: Count Valid statistics: Sum  | 
| BytesDownloaded |  The number of bytes downloaded for requests made to an Amazon S3 bucket, where the response includes a body\. Units: Bytes Valid statistics: Average \(bytes per request\), Sum \(bytes per period\), Sample Count, Min, Max \(same as p100\), any percentile between p0\.0 and p99\.9  | 
| BytesUploaded |  The number of bytes uploaded that contain a request body, made to an Amazon S3 bucket\. Units: Bytes Valid statistics: Average \(bytes per request\), Sum \(bytes per period\), Sample Count, Min, Max \(same as p100\), any percentile between p0\.0 and p99\.9  | 
| 4xxErrors |  The number of HTTP 4xx client error status code requests made to an Amazon S3 bucket with a value of either 0 or 1\. The `average` statistic shows the error rate, and the `sum` statistic shows the count of that type of error, during each period\. Units: Count Valid statistics: Average \(reports per request\), Sum \(reports per period\), Min, Max, Sample Count  | 
| 5xxErrors |  The number of HTTP 5xx server error status code requests made to an Amazon S3 bucket with a value of either 0 or 1\. The `average` statistic shows the error rate, and the `sum` statistic shows the count of that type of error, during each period\. Units: Counts Valid statistics: Average \(reports per request\), Sum \(reports per period\), Min, Max, Sample Count  | 
| FirstByteLatency |  The per\-request time from the complete request being received by an Amazon S3 bucket to when the response starts to be returned\. Units: Milliseconds Valid statistics: Average, Sum, Min, Max\(same as p100\), Sample Count, any percentile between p0\.0 and p100  | 
| TotalRequestLatency |  The elapsed per\-request time from the first byte received to the last byte sent to an Amazon S3 bucket\. This includes the time taken to receive the request body and send the response body, which is not included in `FirstByteLatency`\. Units: Milliseconds Valid statistics: Average, Sum, Min, Max\(same as p100\), Sample Count, any percentile between p0\.0 and p100  | 

#### Amazon S3 CloudWatch replication metrics<a name="s3-cloudwatch-replication-metrics"><!-- omit in toc -->

Monitor the total number of S3 API operations that are pending replication, the total size of objects pending replication, and the maximum replication time to the destination Region\. Only replication rules that have S3 Replication Time Control \(S3 RTC\) enabled can publish replication metrics\.

Unlike CloudWatch request metrics, you can't filter replication metrics by prefixes or tags\. However, you can set up a replication rule based on prefixes and tags\. Your replication metrics then monitor replication for the prefixes and tags that you specified\. 
**Note**  
You can enable alarms for your replication metrics on Amazon CloudWatch\. When you set up alarms for your replication metrics, set the **Missing data treatment** field to **Treat missing data as ignore \(maintain the alarm state\)**\.


| Metric | Description | 
| --- | --- | 
| ReplicationLatency |  The maximum number of seconds by which the replication destination Region is behind the source Region for a given replication rule\.  Units: Seconds Valid statistics: Max  | 
| BytesPendingReplication |  The total number of bytes of objects pending replication for a given replication rule\. Units: Bytes Valid statistics: Max  | 
| OperationsPendingReplication |  The number of operations pending replication for a given replication rule\. Units: Counts Valid statistics: Max  | 

#### Amazon S3 CloudWatch Dimensions<a name="s3-cloudwatch-dimensions"><!-- omit in toc -->

The following dimensions are used to filter Amazon S3 metrics\.


|  Dimension  |  Description  | 
| --- | --- | 
|  BucketName  |  This dimension filters the data you request for the identified bucket only\.  | 
|  StorageType  |  This dimension filters the data that you have stored in a bucket by the following types of storage:  [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/cloudwatch-monitoring.html)  | 
| FilterId | This dimension filters metrics configurations that you specify for request metrics on a bucket, for example, a prefix or a tag\. You specify a filter id when you create a metrics configuration\. For more information, see [Metrics Configurations for Buckets](https://docs.aws.amazon.com/AmazonS3/latest/dev/metrics-configurations.html)\. | 

#### Accessing CloudWatch metrics<a name="cloudwatch-monitoring-accessing"><!-- omit in toc -->

 You can use the following procedures to view the storage metrics for Amazon S3\. To get the Amazon S3 metrics involved, you must set a start and end timestamp\. For metrics for any given 24\-hour period, set the time period to 86400 seconds, the number of seconds in a day\. Also, remember to set the `BucketName` and `StorageType` dimensions\.

For example, if you use the AWS CLI to get the average of a specific bucket's size in bytes, you could use the following command\.

```
aws cloudwatch get-metric-statistics --metric-name BucketSizeBytes --namespace AWS/S3 --start-time 2016-10-19T00:00:00Z --end-time 2016-10-20T00:00:00Z --statistics Average --unit Bytes --region us-west-2 --dimensions Name=BucketName,Value=ExampleBucket Name=StorageType,Value=StandardStorage --period 86400 --output json
```

This example produces the following output\.

```
{
    "Datapoints": [
        {
            "Timestamp": "2016-10-19T00:00:00Z", 
            "Average": 1025328.0, 
            "Unit": "Bytes"
        }
    ], 
    "Label": "BucketSizeBytes"
}
```

**To view metrics using the CloudWatch console**

1. Open the CloudWatch console at [https://console\.aws\.amazon\.com/cloudwatch/](https://console.aws.amazon.com/cloudwatch/)\.

1. In the navigation pane, choose **Metrics**\. 

1. Choose the **S3** namespace\.

1. \(Optional\) To view a metric, enter the metric name in the search box\.

1. \(Optional\) To filter by the **StorageType** dimension, enter the name of the storage class in the search box\.

**To view a list of valid metrics stored for your AWS account using the AWS CLI**
+ At a command prompt, use the following command\.

  ```
  1. aws cloudwatch list-metrics --namespace "AWS/S3"
  ```

#### Related resources<a name="cloudwatch-monitoring-related-resources"><!-- omit in toc -->
+ [Amazon CloudWatch Logs API Reference](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/)
+ [Amazon CloudWatch User Guide](https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/)
+ [list\-metrics](https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/list-metrics.html) action in the *AWS CLI Command Reference*\.
+ [get\-metric\-statistics](https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/get-metric-statistics.html) action in the *AWS CLI Command Reference*\.

#### Metrics configurations for buckets<a name="metrics-configurations"><!-- omit in toc -->

With Amazon CloudWatch request metrics for Amazon S3, you can receive 1\-minute CloudWatch metrics, set CloudWatch alarms, and access CloudWatch dashboards to view near\-real\-time operations and performance of your Amazon S3 storage\. For applications that depend on cloud storage, these metrics let you quickly identify and act on operational issues\. When enabled, these 1\-minute metrics are available at the Amazon S3 bucket\-level, by default\.

If you want to get the CloudWatch request metrics for the objects in a bucket, you must create a metrics configuration for the bucket\. You can also define a filter for the metrics collected using a shared prefix or object tags\. This allows you to align metrics filters to specific business applications, workflows, or internal organizations\.


Keep the following in mind when using metrics configurations:
+ You can have a maximum of 1,000 metrics configurations per bucket\.
+ You can choose which objects in a bucket to include in metrics configurations by using filters\. Filtering on a shared prefix or object tag allows you to align metrics filters to specific business applications, workflows, or internal organizations\. To request metrics for the entire bucket, create a metrics configuration without a filter\.
+ Metrics configurations are necessary only to enable request metrics\. Bucket\-level daily storage metrics are always turned on, and are provided at no additional cost\. Currently, it's not possible to get daily storage metrics for a filtered subset of objects\.
+ Each metrics configuration enables the full set of [available request metrics](cloudwatch-monitoring.md#s3-request-cloudwatch-metrics)\. Operation\-specific metrics \(such as `PostRequests`\) are reported only if there are requests of that type for your bucket or your filter\.
+ Request metrics are reported for object\-level operations\. They are also reported for operations that list bucket contents, like [GET Bucket \(List Objects\)](https://docs.aws.amazon.com/AmazonS3/latest/API/v2-RESTBucketGET.html), [GET Bucket Object Versions](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETVersion.html), and [List Multipart Uploads](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListMPUpload.html), but they are not reported for other operations on buckets\.
+ Request metrics support filtering by prefixes, but storage metrics do not\.

#### Best\-effort CloudWatch metrics delivery<a name="metrics-configurations-delivery"><!-- omit in toc -->

 CloudWatch metrics are delivered on a best\-effort basis\. Most requests for an Amazon S3 object that have request metrics result in a data point being sent to CloudWatch\.

The completeness and timeliness of metrics is not guaranteed\. The data point for a particular request might be returned with a timestamp that is later than when the request was actually processed\. Or the data point for a minute might be delayed before being available through CloudWatch, or it might not be delivered at all\. CloudWatch request metrics give you an idea of the nature of traffic against your bucket in near\-real time\. It is not meant to be a complete accounting of all requests\.

It follows from the best\-effort nature of this feature that the reports available at the [Billing & Cost Management Dashboard](https://console.aws.amazon.com/billing/home?#/) might include one or more access requests that do not appear in the bucket metrics\.

#### Filtering metrics configurations<a name="metrics-configurations-filter"><!-- omit in toc -->

When working with CloudWatch metric configurations, you have the option of filtering the configuration into groups of related objects within a single bucket\. You can filter objects in a bucket for inclusion in a metrics configuration based on one or more of the following elements:
+ **Object key name prefix** – Although the Amazon S3 data model is a flat structure, you can infer hierarchy by using a [prefix](ListingKeysHierarchy.md)\. The Amazon S3 console supports these prefixes with the concept of folders\. If you filter by prefix, objects that have the same prefix are included in the metrics configuration\.
+ **Tag** – You can add tags, which are key\-value name pairs, to objects\. Tags help you find and organize objects easily\. You can also use tags as a filter for metrics configurations\.

If you specify a filter, only requests that operate on single objects can match the filter and be included in the reported metrics\. Requests like [Delete Multiple Objects](https://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html) and `List` requests don't return any metrics for configurations with filters\.

To request more complex filtering, choose two or more elements\. Only objects that have all of those elements are included in the metrics configuration\. If you don't set filters, all of the objects in the bucket are included in the metrics configuration\.

#### How to add metrics configurations<a name="add-metrics-configurations"><!-- omit in toc -->

You can add metrics configurations to a bucket through the Amazon S3 console, with the AWS CLI, or with the Amazon S3 REST API\. For information about how to do this in the AWS Management Console, see the [How Do I Configure Request Metrics for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics.html) in the *Amazon Simple Storage Service Console User Guide*\.

**To add metrics configurations using the AWS CLI**

1. Install and set up the AWS CLI\. For instructions, see [Getting Set Up with the AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html) in the *AWS Command Line Interface User Guide*\.

1. Open a terminal\.

1. Run the following command to add a metrics configuration\.

   ```
   aws s3api put-bucket-metrics-configuration --endpoint https://s3.us-west-2.amazonaws.com --bucket bucket-name --id metrics-config-id --metrics-configuration '{"Id":"metrics-config-id","Filter":{"Prefix":"prefix1"}}'
   ```

1. To verify that the configuration was added, execute the following command\.

   ```
   aws s3api get-bucket-metrics-configuration --endpoint https://s3.us-west-2.amazonaws.com --bucket bucket-name --id metrics-config-id
   ```

   This returns the following response\.

   ```
   {
       "MetricsConfiguration": {
           "Filter": {
               "Prefix": "prefix1"
           },
           "Id": "metrics-config-id"
       }
   }
   ```

You can also add metrics configurations programmatically with the Amazon S3 REST API\. For more information, see the following topics in the *Amazon Simple Storage Service API Reference*:
+ [PUT Bucket Metric Configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTMetricConfiguration.html)
+ [GET Bucket Metric Configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETMetricConfiguration.html)
+ [List Bucket Metric Configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTListBucketMetricsConfiguration.html)
+ [DELETE Bucket Metric Configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTDeleteBucketMetricsConfiguration.html)

### 4. Utilize AWS CLoudtrail to log Amazon S3 API calls
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|-----------|------------------------|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|DE.AE-3|Event data are aggregated and correlated from multiple sources and sensors|
|DE.AE-2|Detected events are analyzed to understand attack targets and methods|
|DE.CM-7|Monitoring for unauthorized personnel, connections, devices, and software is performed|
|DE.CM-6|External service provider activity is monitored to detect potential cybersecurity events|
|DE.CM-3|Personnel activity is monitored to detect potential cybersecurity events|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch|


**Why?** 
AWS CloudTrail Provides a record of actions taken by a user, role or AWS service in Amazon S3\. 

**How?** 
#### Logging Amazon S3 API calls using AWS CloudTrail<a name="cloudtrail-logging"><!-- omit in toc -->

Amazon S3 is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon S3\. CloudTrail captures a subset of API calls for Amazon S3 as events, including calls from the Amazon S3 console and from code calls to the Amazon S3 APIs\. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon S3\. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in **Event history**\. Using the information collected by CloudTrail, you can determine the request that was made to Amazon S3, the IP address from which the request was made, who made the request, when it was made, and additional details\. 

To learn more about CloudTrail, including how to configure and enable it, see the [AWS CloudTrail User Guide](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/)\.

#### Amazon S3 information in CloudTrail<a name="cloudtrail-logging-s3-info"><!-- omit in toc -->

CloudTrail is enabled on your AWS account when you create the account\. When supported event activity occurs in Amazon S3, that activity is recorded in a CloudTrail event along with other AWS service events in **Event history**\. You can view, search, and download recent events in your AWS account\. For more information, see [Viewing Events with CloudTrail Event History](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html)\. 

For an ongoing record of events in your AWS account, including events for Amazon S3, create a trail\. A trail enables CloudTrail to deliver log files to an Amazon S3 bucket\. By default, when you create a trail in the console, the trail applies to all Regions\. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify\. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs\. For more information, see the following: 
+ [Overview for Creating a Trail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-and-update-a-trail.html)
+ [CloudTrail Supported Services and Integrations](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-aws-service-specific-topics.html#cloudtrail-aws-service-specific-topics-integrations)
+ [Configuring Amazon SNS Notifications for CloudTrail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/getting_notifications_top_level.html)
+ [Receiving CloudTrail Log Files from Multiple Regions](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html) and [Receiving CloudTrail Log Files from Multiple Accounts](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-receive-logs-from-multiple-accounts.html)

Every event or log entry contains information about who generated the request\. The identity information helps you determine the following: 
+ Whether the request was made with root or IAM user credentials\.
+ Whether the request was made with temporary security credentials for a role or federated user\.
+ Whether the request was made by another AWS service\.

For more information, see the [CloudTrail userIdentity Element](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-user-identity.html)\.

You can store your log files in your bucket for as long as you want, but you can also define Amazon S3 lifecycle rules to archive or delete log files automatically\. By default, your log files are encrypted by using Amazon S3 server\-side encryption \(SSE\)\.

#### Amazon S3 bucket\-level actions tracked by CloudTrail logging<a name="cloudtrail-bucket-level-tracking"><!-- omit in toc -->

By default, CloudTrail logs bucket\-level actions\. Amazon S3 records are written together with other AWS service records in a log file\. CloudTrail determines when to create and write to a new file based on a time period and file size\. 

The tables in this section list the Amazon S3 bucket\-level actions that are supported for logging by CloudTrail\.


**Amazon S3 bucket\-level actions tracked by CloudTrail logging**  

| REST API name | API event name used in CloudTrail log | 
| --- | --- | 
|  [DELETE Bucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETE.html)  | DeleteBucket | 
|  [DELETE Bucket cors](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEcors.html)  | DeleteBucketCors | 
|  [DELETE Bucket encryption](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEencryption.html)  | DeleteBucketEncryption | 
|  [DELETE Bucket lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETElifecycle.html)  | DeleteBucketLifecycle | 
|  [DELETE Bucket policy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEpolicy.html)  | DeleteBucketPolicy | 
|  [DELETE Bucket replication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEreplication.html)  | DeleteBucketReplication  | 
|  [DELETE Bucket tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEtagging.html)  | DeleteBucketTagging | 
|  [DELETE Bucket website](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEwebsite.html)  | DeleteBucketWebsite | 
|  [GET Bucket acl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETacl.html)  | GetBucketAcl | 
|  [GET Bucket cors](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETcors.html)  | GetBucketCors | 
|  [GET Bucket encryption](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETencryption.html)  | GetBucketEncryption | 
|  [GET Bucket lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlifecycle.html)  | GetBucketLifecycle  | 
|  [GET Bucket location](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlocation.html)  | GetBucketLocation  | 
|  [GET Bucket logging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlogging.html)  | GetBucketLogging  | 
|  [GET Bucket notification](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETnotification.html)  | GetBucketNotification | 
|  [GET Bucket policy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETpolicy.html)  | GetBucketPolicy | 
|  [GET Bucket replication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETreplication.html)  | GetBucketReplication | 
|  [GET Bucket request payment](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTrequestPaymentGET.html)  | GetBucketRequestPayment | 
|  [GET Bucket tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETtagging.html)  | GetBucketTagging  | 
|  [GET Bucket versioning](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETversioningStatus.html)  | GetBucketVersioning  | 
|  [GET Bucket website](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETwebsite.html)  | GetBucketWebsite  | 
|  [GET Service \(List all buckets\)](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTServiceGET.html)  | ListBuckets  | 
|  [PUT Bucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html)  | CreateBucket  | 
|  [PUT Bucket acl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTacl.html)  | PutBucketAcl  | 
|  [PUT Bucket cors](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTcors.html)  | PutBucketCors  | 
|  [PUT Bucket encryption](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTencryption.html)  | PutBucketEncryption | 
|  [PUT Bucket lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlifecycle.html)  | PutBucketLifecycle  | 
|  [PUT Bucket logging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlogging.html)  | PutBucketLogging | 
|  [PUT Bucket notification](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTnotification.html)  | PutBucketNotification  | 
|  [PUT Bucket policy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTpolicy.html)  | PutBucketPolicy  | 
|  [PUT Bucket replication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTreplication.html)  | PutBucketReplication  | 
|  [PUT Bucket request payment](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTrequestPaymentPUT.html)  | PutBucketRequestPayment | 
|  [PUT Bucket tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTtagging.html)  | PutBucketTagging  | 
|  [PUT Bucket versioning](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTVersioningStatus.html)  | PutBucketVersioning | 
|  [PUT Bucket website](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTwebsite.html)  | PutBucketWebsite  | 

In addition to these API operations, you can also use the [OPTIONS object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTOPTIONSobject.html) object\-level action\. This action is treated like a bucket\-level action in CloudTrail logging because the action checks the cors configuration of a bucket\.

#### Amazon S3 object\-level actions tracked by CloudTrail logging<a name="cloudtrail-object-level-tracking"><!-- omit in toc -->

You can also get CloudTrail logs for object\-level Amazon S3 actions\. To do this, specify the Amazon S3 object for your trail\. When an object\-level action occurs in your account, CloudTrail evaluates your trail settings\. If the event matches the object that you specified in a trail, the event is logged\. For more information, see [How Do I Enable Object\-Level Logging for an S3 Bucket with AWS CloudTrail Data Events?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html) in the *Amazon Simple Storage Service Console User Guide* and [Logging Data Events for Trails](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html) in the *AWS CloudTrail User Guide*\. 

The following table lists the object\-level actions that CloudTrail can log:


| REST API name | API event name used in CloudTrail log | 
| --- | --- | 
|  [Abort Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadAbort.html)  | AbortMultipartUpload | 
|  [Complete Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadComplete.html)  | CompleteMultipartUpload | 
|  [Delete Multiple Objects](https://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html)  | DeleteObjects | 
|  [DELETE Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html)  | DeleteObject | 
|  [GET Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html)  | GetObject | 
|  [GET Object ACL](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETacl.html)  | GetObjectAcl | 
|  [GET Object tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETtagging.html)  | GetObjectTagging | 
|  [GET Object torrent](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETtorrent.html)  | GetObjectTorrent | 
|  [HEAD Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html)  | HeadObject | 
|  [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)  | CreateMultipartUpload | 
|  [List Parts](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html)  | ListParts | 
|  [POST Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html)  | PostObject | 
|  [POST Object restore](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html)  | RestoreObject | 
|  [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)  | PutObject | 
|  [PUT Object acl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUTacl.html)  | PutObjectAcl | 
|  [PUT Object tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUTtagging.html)  | PutObjectTagging | 
|  [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html)  | CopyObject | 
|  [SELECT Object Content](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html)  | SelectObjectContent | 
|  [Upload Part](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html)  | UploadPart | 
|  [Upload Part \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html)  | UploadPartCopy | 

In addition to these operations, you can use the following bucket\-level operations to get CloudTrail logs as object\-level Amazon S3 actions under certain conditions:
+ [GET Bucket \(List Objects\) Version 2](https://docs.aws.amazon.com/AmazonS3/latest/API/v2-RESTBucketGET.html) – Select a prefix specified in the trail\.
+ [GET Bucket Object versions](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETVersion.html) – Select a prefix specified in the trail\.
+ [HEAD Bucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketHEAD.html) – Specify a bucket and an empty prefix\.
+ [Delete Multiple Objects](https://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html) – Specify a bucket and an empty prefix\. 
**Note**  
CloudTrail does not log key names for the keys that are deleted using the Delete Multiple Objects operation\.

#### Object\-level actions in cross\-account scenarios<a name="cloudtrail-object-level-crossaccount"><!-- omit in toc -->

The following are special use cases involving the object\-level API calls in cross\-account scenarios and how CloudTrail logs are reported\. CloudTrail always delivers logs to the requester \(who made the API call\)\. When setting up cross\-account access, consider the examples in this section\.

**Note**  
The examples assume that CloudTrail logs are appropriately configured\. 

##### Example 1: CloudTrail delivers access logs to the bucket owner<a name="cloudtrail-crossaccount-example1"><!-- omit in toc -->

CloudTrail delivers access logs to the bucket owner only if the bucket owner has permissions for the same object API\. Consider the following cross\-account scenario:
+ Account\-A owns the bucket\.
+ Account\-B \(the requester\) tries to access an object in that bucket\.

CloudTrail always delivers object\-level API access logs to the requester\. In addition, CloudTrail also delivers the same logs to the bucket owner only if the bucket owner has permissions for the same API actions on that object\. 

**Note**  
If the bucket owner is also the object owner, the bucket owner gets the object access logs\. Otherwise, the bucket owner must get permissions, through the object ACL, for the same object API to get the same object\-access API logs\.

##### Example 2: CloudTrail does not proliferate email addresses used in setting object ACLs<a name="cloudtrail-crossaccount-example2"><!-- omit in toc -->
Consider the following cross\-account scenario:
+ Account\-A owns the bucket\.
+  Account\-B \(the requester\) sends a request to set an object ACL grant using an email address\. 

The request gets the logs along with the email information\. However, the bucket owner—if they are eligible to receive logs, as in example 1—gets the CloudTrail log reporting the event\. However, the bucket owner doesn't get the ACL configuration information, specifically the grantee email and the grant\. The only information that the log tells the bucket owner is that an ACL API call was made by Account\-B\.

#### CloudTrail tracking with Amazon S3 SOAP API calls<a name="cloudtrail-s3-soap"><!-- omit in toc -->

CloudTrail tracks Amazon S3 SOAP API calls\. Amazon S3 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. 

**Important**  
Newer Amazon S3 features are not supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\.


**Amazon S3 SOAP actions tracked by CloudTrail logging**  

| SOAP API name | API event name used in CloudTrail log | 
| --- | --- | 
|  [ListAllMyBuckets](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPListAllMyBuckets.html)  | ListBuckets | 
|  [CreateBucket](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPCreateBucket.html)  | CreateBucket | 
|  [DeleteBucket](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPDeleteBucket.html)  | DeleteBucket | 
|  [GetBucketAccessControlPolicy](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPGetBucketAccessControlPolicy.html)  | GetBucketAcl | 
|  [SetBucketAccessControlPolicy](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPSetBucketAccessControlPolicy.html)  | PutBucketAcl | 
|  [GetBucketLoggingStatus](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPGetBucketLoggingStatus.html)  | GetBucketLogging | 
|  [SetBucketLoggingStatus](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPSetBucketLoggingStatus.html)  | PutBucketLogging | 

#### Using CloudTrail logs with Amazon S3 server access logs and CloudWatch Logs<a name="cloudtrail-logging-vs-server-logs"><!-- omit in toc -->

AWS CloudTrail logs provide a record of actions taken by a user, role, or an AWS service in Amazon S3, while Amazon S3 server access logs provide detailed records for the requests that are made to an S3 bucket\. F

You can use AWS CloudTrail logs together with server access logs for Amazon S3\. CloudTrail logs provide you with detailed API tracking for Amazon S3 bucket\-level and object\-level operations\. Server access logs for Amazon S3 provide you visibility into object\-level operations on your data in Amazon S3\. 

You can also use CloudTrail logs together with CloudWatch for Amazon S3\. CloudTrail integration with CloudWatch Logs delivers S3 bucket\-level API activity captured by CloudTrail to a CloudWatch log stream in the CloudWatch log group that you specify\. You can create CloudWatch alarms for monitoring specific API activity and receive email notifications when the specific API activity occurs\. For more information about CloudWatch alarms for monitoring specific API activity, see the [AWS CloudTrail User Guide](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/)\. 

#### Example: Amazon S3 log file entries<a name="cloudtrail-logging-understanding-s3-entries"><!-- omit in toc -->

 A trail is a configuration that enables delivery of events as log files to an Amazon S3 bucket that you specify\. CloudTrail log files contain one or more log entries\. An event represents a single request from any source\. It includes information about the requested action, the date and time of the action, request parameters, and so on\. CloudTrail log files are not an ordered stack trace of the public API calls, so they do not appear in any specific order\.

The following example shows a CloudTrail log entry that demonstrates the [GET Service](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTServiceGET.html), [PUT Bucket acl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTacl.html), and [GET Bucket versioning](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETversioningStatus.html) actions\.

```
{
    "Records": [
    {
        "eventVersion": "1.03",
        "userIdentity": {
            "type": "IAMUser",
            "principalId": "111122223333",
            "arn": "arn:aws:iam::111122223333:user/myUserName",
            "accountId": "111122223333",
            "accessKeyId": "AKIAIOSFODNN7EXAMPLE",
            "userName": "myUserName"
        },
        "eventTime": "2019-02-01T03:18:19Z",
        "eventSource": "s3.amazonaws.com",
        "eventName": "ListBuckets",
        "awsRegion": "us-west-2",
        "sourceIPAddress": "127.0.0.1",
        "userAgent": "[]",
        "requestParameters": {
            "host": [
                "s3.us-west-2.amazonaws.com"
            ]
        },
        "responseElements": null,
        "additionalEventData": {
            "SignatureVersion": "SigV2",
            "AuthenticationMethod": "QueryString"
    },
        "requestID": "47B8E8D397DCE7A6",
        "eventID": "cdc4b7ed-e171-4cef-975a-ad829d4123e8",
        "eventType": "AwsApiCall",
        "recipientAccountId": "111122223333"
    },
    {
       "eventVersion": "1.03",
       "userIdentity": {
            "type": "IAMUser",
            "principalId": "111122223333",
            "arn": "arn:aws:iam::111122223333:user/myUserName",
            "accountId": "111122223333",
            "accessKeyId": "AKIAIOSFODNN7EXAMPLE",
            "userName": "myUserName"
        },
      "eventTime": "2019-02-01T03:22:33Z",
      "eventSource": "s3.amazonaws.com",
      "eventName": "PutBucketAcl",
      "awsRegion": "us-west-2",
      "sourceIPAddress": "",
      "userAgent": "[]",
      "requestParameters": {
          "bucketName": "",
          "AccessControlPolicy": {
              "AccessControlList": {
                  "Grant": {
                      "Grantee": {
                          "xsi:type": "CanonicalUser",
                          "xmlns:xsi": "http://www.w3.org/2001/XMLSchema-instance",
                          "ID": "d25639fbe9c19cd30a4c0f43fbf00e2d3f96400a9aa8dabfbbebe1906Example"
                       },
                      "Permission": "FULL_CONTROL"
                   }
              },
              "xmlns": "http://s3.amazonaws.com/doc/2006-03-01/",
              "Owner": {
                  "ID": "d25639fbe9c19cd30a4c0f43fbf00e2d3f96400a9aa8dabfbbebe1906Example"
              }
          }
          "host": [
              "s3.us-west-2.amazonaws.com"
          ],
          "acl": [
              ""
          ]
      },
      "responseElements": null,
      "additionalEventData": {
          "SignatureVersion": "SigV4",
          "CipherSuite": "ECDHE-RSA-AES128-SHA",
          "AuthenticationMethod": "AuthHeader"
      },
      "requestID": "BD8798EACDD16751",
      "eventID": "607b9532-1423-41c7-b048-ec2641693c47",
      "eventType": "AwsApiCall",
      "recipientAccountId": "111122223333"
    },
    {
      "eventVersion": "1.03",
      "userIdentity": {
          "type": "IAMUser",
          "principalId": "111122223333",
          "arn": "arn:aws:iam::111122223333:user/myUserName",
          "accountId": "111122223333",
          "accessKeyId": "AKIAIOSFODNN7EXAMPLE",
          "userName": "myUserName"
        },
      "eventTime": "2019-02-01T03:26:37Z",
      "eventSource": "s3.amazonaws.com",
      "eventName": "GetBucketVersioning",
      "awsRegion": "us-west-2",
      "sourceIPAddress": "",
      "userAgent": "[]",
      "requestParameters": {
          "host": [
              "s3.us-west-2.amazonaws.com"
          ],
          "bucketName": "AWSDOC-EXAMPLE-BUCKET1",
          "versioning": [
              ""
          ]
      },
      "responseElements": null,
      "additionalEventData": {
          "SignatureVersion": "SigV4",
          "CipherSuite": "ECDHE-RSA-AES128-SHA",
          "AuthenticationMethod": "AuthHeader",
    },
      "requestID": "07D681279BD94AED",
      "eventID": "f2b287f3-0df1-4961-a2f4-c4bdfed47657",
      "eventType": "AwsApiCall",
      "recipientAccountId": "111122223333"
    }
  ]
}
```

#### Related resources<a name="cloudtrail-logging-related-resources"><!-- omit in toc -->
+ [AWS CloudTrail User Guide](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/)
+ [CloudTrail Event Reference](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference.html)

### 5. Enable AWS Config rules for Amazon S3
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|DE.AE-4|Impact of events is determined|
|DE.AE-5|Incident alert thresholds are established|


**Why?** 

Monitoring AWS Config to assess and evaluate the configurations of S3 provides continuous information on the environment, and allows you to automate the evaluation of recorded configurations against desired configurations.

**How?** 
#### Enable AWS Config and monitor and take appropriate acion for specific AWS config rules <!-- omit in toc -->

AWS Config enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config monitors resource configurations, allowing you to evaluate the recorded configurations against the desired secure configurations. Using AWS Config, you can review changes in configurations and relationships between AWS resources, investigate detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This can help you simplify compliance auditing, security analysis, change management, and operational troubleshooting\. 

#### s3\-bucket\-logging\-enabled<a name="s3-bucket-logging-enabled"><!-- omit in toc -->

Checks whether logging is enabled for your S3 buckets\.

**Identifier:** S3\_BUCKET\_LOGGING\_ENABLED

**Trigger type:** Configuration changes

**Parameters:**

 targetBucket   
 Target S3 bucket for storing server access logs\. 

 targetPrefix   
 Prefix of the target S3 bucket for storing server access logs\. 

 #### s3\-bucket\-blacklisted\-actions\-prohibited<a name="s3-bucket-blacklisted-actions-prohibited"><!-- omit in toc -->

Checks that the Amazon Simple Storage Service bucket policy does not allow blacklisted bucket\-level and object\-level actions on resources in the bucket for principals from other AWS accounts\. For example, the rule checks that the Amazon S3 bucket policy does not allow another AWS account to perform any s3:GetBucket\* actions and s3:DeleteObject on any object in the bucket\. The rule is NON\_COMPLIANT if any blacklisted actions are allowed by the Amazon S3 bucket policy\.

**Identifier:** S3\_BUCKET\_BLACKLISTED\_ACTIONS\_PROHIBITED

**Trigger type:** Configuration changes

**Parameters:**

 blacklistedactionpatterns  
Comma\-separated list of blacklisted action patterns, for example, s3:GetBucket\* and s3:DeleteObject\.

#### s3\-bucket\-policy\-not\-more\-permissive<a name="s3-bucket-policy-not-more-permissive"><!-- omit in toc -->

Verifies that your Amazon Simple Storage Service bucket policies do not allow other inter\-account permissions than the control Amazon S3 bucket policy that you provide\. 

**Note**  
If you provide an invalid parameter value, you will see the following error: Value for controlPolicy parameter must be an Amazon S3 bucket policy\. 

**Identifier:** S3\_BUCKET\_POLICY\_NOT\_MORE\_PERMISSIVE

**Trigger type:** Configuration changes

**Parameters:**

 controlPolicy  
Amazon S3 bucket policy that defines an upper bound on the permissions of your S3 buckets\. The policy can be a maximum of 1024 characters long\.

An example of a control policy is as follows\.

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Principal": {
        "AWS": "11112222333"
      },
      "Effect": "Allow",
      "Action": "s3:GetObject",
      "Resource": "*"
    },
    {
      "Principal": {
        "AWS": "44445556666"
      },
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": "*"
    }
  ]
}
```

The first Allow statement specifies that the AWS account ID `111122223333` can retrieve objects \(`s3:GetObject`\) on any resource \(\*\)\. The second Allow statement specifies that the AWS account ID `44445556666` can perform any s3 action `(s3:*)` on any resource \(\*\)\.

Examples of **NON\_COMPLIANT** bucket policies with the above control policy as an input parameter for the rule are as follows\. 

The following bucket policy is NON\_COMPLIANT because the bucket policy allows permissions for the IAM user, Alice, in the AWS account ID `888899998888`\. These permissions are implicitly denied by the control policy\.

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Principal": {
        "AWS": [
          "arn:aws:iam::888899998888:user/Alice"
        ]
      },
      "Effect": "Allow",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::example-bucket/*"
    }
  ]
}
```

The following bucket policy is NON\_COMPLIANT because the bucket policy allows the AWS account ID `11112222333` permissions to perform `s3:PutBucketPolicy` that is implicitly denied by the control policy\.

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Principal": {
        "AWS": [
          "11112222333"
        ]
      },
      "Effect": "Allow",
      "Action": "s3:PutBucketPolicy",
      "Resource": "arn:aws:s3:::example-bucket"
    }
  ]
}
```

Examples of **COMPLIANT** bucket policies are as follows\.

The following bucket policy is COMPLIANT because the control policy allows principals from the AWS account ID `11112222333` to perform `s3:GetObject` on any object\.

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Principal": {
        "AWS" : [
          "arn:aws:iam::11112222333:user/Bob"
        ]
      },
      "Effect": "Allow",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::example-bucket/photos/*"
    }
  ]
}
```

The following bucket policy is COMPLIANT because the control policy allows a principal with the AWS account ID `444455556666` to perform any S3 action\.

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Principal": {
        "AWS": [
          "44445556666"
        ]
      },
      "Effect": "Allow",
      "Action": "s3:*Configuration",
      "Resource": "arn:aws:s3:::example-bucket"
    }
  ]
}
```

#### s3\-bucket\-public\-read\-prohibited<a name="s3-bucket-public-read-prohibited"><!-- omit in toc -->

Checks that your Amazon S3 buckets do not allow public read access\. The rule checks the Block Public Access settings, the bucket policy, and the bucket access control list \(ACL\)\.

 The rule is compliant when both of the following are true:
+ The Block Public Access setting restricts public policies or the bucket policy does not allow public read access\.
+ The Block Public Access setting restricts public ACLs or the bucket ACL does not allow public read access\.

The rule is noncompliant when:
+ If the Block Public Access setting does not restrict public policies, AWS Config evaluates whether the policy allows public read access\. If the policy allows public read access, the rule is noncompliant\.
+ If the Block Public Access setting does not restrict public bucket ACLs, AWS Config evaluates whether the bucket ACL allows public read access\. If the bucket ACL allows public read access, the rule is noncompliant\.

**Identifier:** S3\_BUCKET\_PUBLIC\_READ\_PROHIBITED

**Trigger type:** Configuration changes

**Parameters:**

None  

#### s3\-bucket\-public\-write\-prohibited<a name="s3-bucket-public-write-prohibited"><!-- omit in toc -->

Checks that your Amazon S3 buckets do not allow public write access\. The rule checks the Block Public Access settings, the bucket policy, and the bucket access control list \(ACL\)\.

 The rule is compliant when both of the following are true:
+ The Block Public Access setting restricts public policies or the bucket policy does not allow public write access\.
+ The Block Public Access setting restricts public ACLs or the bucket ACL does not allow public write access\.

The rule is noncompliant when:
+ If the Block Public Access setting does not restrict public policies, AWS Config evaluates whether the policy allows public write access\. If the policy allows public write access, the rule is noncompliant\.
+ If the Block Public Access setting does not restrict public bucket ACLs, AWS Config evaluates whether the bucket ACL allows public write access\. If the bucket ACL allows public write access, the rule is noncompliant\.

**Identifier:** S3\_BUCKET\_PUBLIC\_WRITE\_PROHIBITED

**Trigger type:** Configuration changes

**Parameters:**

None  

#### s3\-bucket\-replication\-enabled<a name="s3-bucket-replication-enabled"><!-- omit in toc -->

Checks whether S3 buckets have cross\-region replication enabled\.

**Identifier:** S3\_BUCKET\_REPLICATION\_ENABLED

**Trigger type:** Configuration changes

**Parameters:**

 None  

#### s3\-bucket\-server\-side\-encryption\-enabled<a name="s3-bucket-server-side-encryption-enabled"><!-- omit in toc -->

Checks that your Amazon S3 bucket either has Amazon S3 default encryption enabled or that the S3 bucket policy explicitly denies `put-object` requests without server side encryption\.  

**Identifier:** S3\_BUCKET\_SERVER\_SIDE\_ENCRYPTION\_ENABLED

**Trigger type:** Configuration changes

**Parameters:**

 None  

#### s3\-bucket\-ssl\-requests\-only<a name="s3-bucket-ssl-requests-only"><!-- omit in toc -->

Checks whether S3 buckets have policies that require requests to use Secure Socket Layer \(SSL\)\.

**Identifier:**S3\_BUCKET\_SSL\_REQUESTS\_ONLY

**Trigger type:** Configuration changes

**Parameters:**

 None   

An example of a bucket policy that is **COMPLIANT** with the SSL AWS Config rule is as follows:

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [ 
          "123412341234" 
        ]
      },
      "Action": "s3:Get*",
      "Resource": "arn:aws:s3:::example-bucket/*"
    },
    {
      "Effect": "Deny",
      "Principal": "*",
      "Action": "*",
      "Resource": "arn:aws:s3:::example-bucket/*",
      "Condition": {
        "Bool": {
          "aws:SecureTransport": "false"
        }
      }
    }
  ]
}
```

An example of a bucket policy that is **NON\_COMPLIANT** with the SSL AWS Config rule is as follows:

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [ 
          "123412341234" 
        ]
      },
      "Action": "s3:Get*",
      "Resource": "arn:aws:s3:::example-bucket/*"
    },
    {
      "Effect": "Deny",
      "Principal": "*",
      "Action": "*",
      "Resource": "arn:aws:s3:::example-bucket/private/*",
      "Condition": {
        "Bool": {
          "aws:SecureTransport": "false"
        }
      }
    }
  ]
}
```

#### s3\-bucket\-versioning\-enabled<a name="s3-bucket-versioning-enabled"><!-- omit in toc -->

Checks whether versioning is enabled for your S3 buckets\. Optionally, the rule checks if MFA delete is enabled for your S3 buckets\.

**Identifier:** S3\_BUCKET\_VERSIONING\_ENABLED

**Trigger type:** Configuration changes

**Parameters:**

 isMfaDeleteEnabled   
 MFA delete is enabled for your S3 buckets\.

#### Setting Up AWS Config with the Console<a name="gs-console"><!-- omit in toc -->

You can use the AWS Management Console to get started with AWS Config to do the following: 
+ Specify the resource types you want AWS Config to record\.
+ Set up Amazon SNS to notify you of configuration changes\.
+ Specify an Amazon S3 bucket to receive configuration information\.
+ Add AWS Config managed rules to evaluate the resource types\.

If you are using AWS Config for the first time or configuring AWS Config for a new region, you can choose managed rules to evaluate resource configurations\. For regions that support AWS Config and AWS Config Rules, see [AWS Config Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#awsconfig_region) in the *Amazon Web Services General Reference*\.

**To set up AWS Config with the console**

1. Sign in to the AWS Management Console and open the AWS Config console at [https://console\.aws\.amazon\.com/config/](https://console.aws.amazon.com/config/)\.

1. If this is the first time you are opening the AWS Config console or you are setting up AWS Config in a new region\.

1. Choose **Get Started Now**\.

1. On the **Settings** page, for **Resource types to record**, specify the AWS resource types you want AWS Config to record:
   + **All resources** – AWS Config records all supported resources with the following options:
     + **Record all resources supported in this region** – AWS Config records configuration changes for every supported type of regional resource\. When AWS Config adds support for a new resource type, AWS Config automatically starts recording resources of that type\.
     + **Include global resources** – AWS Config includes supported types of global resources with the resources that it records \(for example, IAM resources\)\. When AWS Config adds support for a new global resource type, AWS Config automatically starts recording resources of that type\.
   + **Specific types** – AWS Config records configuration changes for only the AWS resource types that you specify\.



2. For **Amazon S3 Bucket**, choose the Amazon S3 bucket to which AWS Config sends configuration history and configuration snapshot files:
   + **Create a new bucket** – For **Bucket Name**, type a name for your Amazon S3 bucket\. 

     The name that you type must be unique across all existing bucket names in Amazon S3\. One way to help ensure uniqueness is to include a prefix; for example, the name of your organization\. You can't change the bucket name after it is created\. For more information, see [Bucket Restrictions and Limitations](https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html) in the *Amazon Simple Storage Service Developer Guide*\. 
   + **Choose a bucket from your account** – For **Bucket Name**, choose your preferred bucket\.
   + **Choose a bucket from another account** – For **Bucket Name**, type the bucket name\.

     If you choose a bucket from another account, that bucket must have policies that grant access permissions to AWS Config\. 

3. For **Amazon SNS Topic**, choose whether AWS Config streams information by selecting the **Stream configuration changes and notifications to an Amazon SNS topic**\. AWS Config sends notifications such as configuration history delivery, configuration snapshot delivery, and compliance\. 

4. If you chose to have AWS Config stream to an Amazon SNS topic, choose the target topic:
   + **Create a new topic** – For **Topic Name**, type a name for your SNS topic\.
   + **Choose a topic from your account** – For **Topic Name**, select your preferred topic\.
   + **Choose a topic from another account** – For **Topic ARN**, type the Amazon Resource Name \(ARN\) of the topic\. If you choose a topic from another account, the topic must have policies that grant access permissions to AWS Config\. 
**Note**  
The Amazon SNS topic must exist in the same region as the region in which you set up AWS Config\.

1. For **AWS Config role**, choose the IAM role that grants AWS Config permission to record configuration information and send this information to Amazon S3 and Amazon SNS:
   + **Create a role** – AWS Config creates a role that has the required permissions\. For **Role name**, you can customize the name that AWS Config creates\.
   + **Choose a role from your account** – For **Role name**, choose an IAM role in your account\. AWS Config will attach the required policies\.

**Note**  
Check the box if you want to use the IAM role as it\. AWS Config will not attach policies to the role\.

1. If you are setting up AWS Config in a region that supports rules, choose **Next**\. 

   Otherwise, choose **Save**\. AWS Config displays the **Resource inventory** page\.



### 6. Use AWS CloudTrail to Identify Amazon S3 request
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|
|DE.AE-3|Event data are aggregated and correlated from multiple sources and sensors|
|DE.AE-2|Detected events are analyzed to understand attack targets and methods|
|DE.CM-7|Monitoring for unauthorized personnel, connections, devices, and software is performed|
|DE.CM-6|External service provider activity is monitored to detect potential cybersecurity events|
|DE.CM-3|Personnel activity is monitored to detect potential cybersecurity events|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch|

**Why?** 
AWS CloudTrail Provides a record of actions taken by a user, role or AWS service in Amazon S3\. 

**How?** 
#### Use AWS CloudTrail to identify Amazon S3 requests<a name="cloudtrail-request-identification"><!-- omit in toc -->

Amazon S3 lets you identify requests using an AWS CloudTrail event log\. AWS CloudTrail is the preferred way of identifying Amazon S3 requests, versus using Amazon S3 server access logs

#### How CloudTrail captures requests made to Amazon S3<a name="cloudtrail-logging-s3-requests"><!-- omit in toc -->

By default, CloudTrail logs S3 bucket\-level API calls that were made in the last 90 days, but not log requests made to objects\. Bucket\-level calls include events like `CreateBucket`, `DeleteBucket`, `PutBucketLifeCycle`, `PutBucketPolicy`, etc\. You can see bucket\-level events on the CloudTrail console\. However, you can't view data events \(Amazon S3 object\-level calls\) there—you must parse or query CloudTrail logs for them\. 

For information about what Amazon S3 API calls are captured by CloudTrail, see [Amazon S3 information in CloudTrail](cloudtrail-logging.md#cloudtrail-logging-s3-info)\. 

#### Enabling CloudTrail event logging for S3 buckets and objects<a name="enable-cloudtrail-logging-for-s3"><!-- omit in toc -->

CloudTrail data events allow you to get information about bucket and object\-level requests\. To enable CloudTrail data events for a specific bucket, see [How Do I Enable Object\-Level Logging for an S3 Bucket with AWS CloudTrail Data Events?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html) in the *Amazon Simple Storage Service Console User Guide*\.

To enable CloudTrail data events for all your buckets or for a list of specific buckets, you must [create a trail manually in CloudTrail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html)\. 

**Note**  
The default setting for CloudTrail is to find only management events\. Check to ensure that you have the data events enabled for your account\.
 With an S3 bucket that is generating a high workload, you could quickly generate thousands of logs in a short amount of time\. Be mindful of how long you choose to enable CloudTrail data events for a busy bucket\. 

 CloudTrail stores Amazon S3 data event logs in an S3 bucket of your choosing\. You should consider using a bucket in a separate AWS account to better organize events from multiple buckets you might own into a central place for easier querying and analysis\. AWS Organizations makes it easy to create an AWS account that is linked the account owning the bucket you are monitoring\. For more information, see [What Is AWS Organizations?](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html) in the *AWS Organizations User Guide*

When you create a trail in CloudTrail, in the data events section, you can select the **Select all S3 buckets in your account** check box to log all object level events\. 

**Note**  
It's a best practice to [create an Amazon S3 lifecycle policy](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html) for your AWS CloudTrail data event bucket\. Configure the lifecycle policy to periodically remove log files after the period of time you believe you need to audit them\. Doing so reduces the amount of data that Athena analyzes for each query\.
For information about logging format, see [Logging Amazon S3 API Calls by Using AWS CloudTrail](https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html)\.
For examples of how to query CloudTrail logs, see [Analyze Security, Compliance, and Operational Activity Using AWS CloudTrail and Amazon Athena](http://aws.amazon.com/blogs/big-data/aws-cloudtrail-and-amazon-athena-dive-deep-to-analyze-security-compliance-and-operational-activity/)\. 

#### Identifying requests made to Amazon S3 in a CloudTrail log<a name="identify-S3-requests-using-in-CTlog"><!-- omit in toc -->

Events logged by CloudTrail are stored as compressed, GZipped JSON objects in your S3 bucket\. To efficiently find requests, you should use a service like Amazon Athena to index and query the CloudTrail logs\. For more information about CloudTrail and Athena, see [Querying AWS CloudTrail Logs](https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html)\.

#### Using Athena with CloudTrail logs<a name="using-athena"><!-- omit in toc -->

After you set up CloudTrail to deliver events to a bucket, you should start to see objects go to your destination bucket on the Amazon S3 console\. These are formatted as follows: `s3://<myawsexamplebucket1>/AWSLogs/<111122223333>/CloudTrail/<Region>/<yyyy>/<mm>/<dd>` 

**Example — use Athena to query CloudTrail event logs for specific requests**  

Locate your CloudTrail event logs:

`s3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/us-east-2/2019/04/14`

With CloudTrail event logs, you can now create an Athena database and table to query them as follows:

1. Open the Athena console at [https://console\.aws\.amazon\.com/athena/](https://console.aws.amazon.com/athena/home)\.

1. Change the AWS Region to be the same as your CloudTrail destination S3 bucket\.

1. In the query window, create an Athena database for your CloudTrail events\.

   ```
   CREATE DATABASE s3_cloudtrail_events_db
   ```

1. Use the following query to create a table for all of your CloudTrail events in the bucket\. Be sure to change the bucket name from *<CloudTrail\_myawsexamplebucket1>* to your bucket's name\. Also provide the *AWS\_account\_ID* CloudTrail that is used in your bucket\. 

   ```
   CREATE EXTERNAL TABLE s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table(
           eventversion STRING,
           useridentity STRUCT<
               type:STRING,
               principalid:STRING,
               arn:STRING,
               accountid:STRING,
               invokedby:STRING,
               accesskeyid:STRING,
               userName:STRING,
               sessioncontext:STRUCT<
                   attributes:STRUCT<
                           mfaauthenticated:STRING,
                           creationdate:STRING>,
                           sessionissuer:STRUCT<  
                           type:STRING,
                           principalId:STRING,
                           arn:STRING, 
                           accountId:STRING,
                           userName:STRING>
                   >
                >,
           eventtime STRING,
           eventsource STRING,
           eventname STRING,
           awsregion STRING,
           sourceipaddress STRING,
           useragent STRING,
           errorcode STRING,
           errormessage STRING,
           requestparameters STRING,
           responseelements STRING,
           additionaleventdata STRING,
           requestid STRING,
           eventid STRING,
           resources ARRAY<STRUCT<
               ARN:STRING,
               accountId:STRING,
               type:STRING>>,
           eventtype STRING,
           apiversion STRING,
           readonly STRING,
           recipientaccountid STRING,
           serviceeventdetails STRING,
           sharedeventid STRING,
           vpcendpointid STRING
       )
       ROW FORMAT SERDE 'com.amazon.emr.hive.serde.CloudTrailSerde'
       STORED AS INPUTFORMAT 'com.amazon.emr.cloudtrail.CloudTrailInputFormat'
       OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
       LOCATION 's3://<myawsexamplebucket1>/AWSLogs/<111122223333>/';
   ```

1. Test Athena to ensure that the query works\.

   ```
   SELECT * FROM s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table
   WHERE eventsource='s3.amazonaws.com'
   LIMIT 2;
   ```

#### Using AWS CloudTrail to identify Amazon S3 signature version 2 requests<a name="cloudtrail-identification-sigv2-requests"><!-- omit in toc -->

Amazon S3 lets you identify what API signature version was used to sign a request using an AWS CloudTrail event log\. This capability is important because support for Signature Version 2 will be turned off \(deprecated\)\. After that, Amazon S3 will no longer accept requests that use Signature Version 2, and all requests must use *Signature Version 4* signing\. 

We *strongly* recommend that you use CloudTrail to help determine whether any of your workflows are using Signature Version 2 signing\. Remediate them by upgrading your libraries and code to use Signature Version 4 instead to prevent any impact to your business\. 

For more information, see [Announcement: AWS CloudTrail for Amazon S3 adds new fields for enhanced security auditing](https://forums.aws.amazon.com/ann.jspa?annID=6551) in the AWS Discussion Forums\.

**Note**  
CloudTrail events for Amazon S3 include the signature version in the request details under the key name of '`additionalEventData`'\. To find the signature version on requests made for objects in Amazon S3 like GETs, PUTs, and DELETEs, you must enable CloudTrail data events because it is turned off by default\. 

AWS CloudTrail is the preferred method for identifying Signature Version 2 requests, if you are using Amazon S3 server access logs, see [ Using Amazon S3 access logs to identify signature version 2 requests ](using-s3-access-logs-to-identify-requests.md#using-s3-access-logs-to-identify-sigv2-requests)

#### Athena query examples for identifying Amazon S3 signature version 2 requests<a name="ct-examples-identify-sigv2-requests"><!-- omit in toc -->

**Example — select all events that are signature version 2, and print only EventTime, S3 action, Request\_Parameters, Region, SourceIP, and UserAgent**  
In the following Athena query, replace *<s3\_cloudtrail\_events\_db\.cloudtrail\_myawsexamplebucket1\_table>* with your Athena details and increase or remove the limit as needed\.   

```
SELECT EventTime, EventName as S3_Action, requestParameters as Request_Parameters, awsregion as AWS_Region, sourceipaddress as Source_IP, useragent as User_Agent
FROM s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table
WHERE eventsource='s3.amazonaws.com'
AND json_extract_scalar(additionalEventData, '$.SignatureVersion')='SigV2'
LIMIT 10;
```

**Example — select all requesters that are sending signature version 2 traffic**  

```
SELECT useridentity.arn, Count(requestid) as RequestCount
FROM s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table
WHERE eventsource='s3.amazonaws.com'
    and json_extract_scalar(additionalEventData, '$.SignatureVersion')='SigV2'
Group by useridentity.arn
```

#### Partitioning signature version 2 data<a name="partitioning-sigv2-data"><!-- omit in toc -->

If you have a large amount of data that you need to query, you can reduce the costs and runtime of Athena by creating a partitioned table\. 

To do this, create a new table with partitions as follows\.

```
   CREATE EXTERNAL TABLE s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table_partitioned(
        eventversion STRING,
        userIdentity STRUCT<
            type:STRING,
            principalid:STRING,
            arn:STRING,
            accountid:STRING,
            invokedby:STRING,
            accesskeyid:STRING,
            userName:STRING,
         sessioncontext:STRUCT<
                    attributes:STRUCT< 
                    mfaauthenticated:STRING,
                    creationdate:STRING>,
                    sessionIssuer:STRUCT<
                    type:STRING,
                    principalId:STRING,
                    arn:STRING,
                    accountId:STRING,
                    userName:STRING>
                >
             >,
        eventTime STRING,
        eventSource STRING,
        eventName STRING,
        awsRegion STRING,
        sourceIpAddress STRING,
        userAgent STRING,
        errorCode STRING,
        errorMessage STRING,
        requestParameters STRING,
        responseElements STRING,
        additionalEventData STRING,
        requestId STRING,
        eventId STRING,
        resources ARRAY<STRUCT<ARN:STRING,accountId: STRING,type:STRING>>, 
        eventType STRING,
        apiVersion STRING,
        readOnly STRING,
        recipientAccountId STRING,
        serviceEventDetails STRING,
        sharedEventID STRING,
        vpcEndpointId STRING
    )   
    PARTITIONED BY (region string, year string, month string, day string)
    ROW FORMAT SERDE 'com.amazon.emr.hive.serde.CloudTrailSerde'
    STORED AS INPUTFORMAT 'com.amazon.emr.cloudtrail.CloudTrailInputFormat'
    OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
    LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/';
```

Then, create the partitions individually\. You can't get results from dates that you have not created\. 

```
ALTER TABLE s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table_partitioned ADD
    PARTITION (region= 'us-east-1', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/us-east-1/2019/02/19/'
    PARTITION (region= 'us-west-1', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/us-west-1/2019/02/19/'
    PARTITION (region= 'us-west-2', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/us-west-2/2019/02/19/'
    PARTITION (region= 'ap-southeast-1', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/ap-southeast-1/2019/02/19/'
    PARTITION (region= 'ap-southeast-2', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/ap-southeast-2/2019/02/19/'
    PARTITION (region= 'ap-northeast-1', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/ap-northeast-1/2019/02/19/'
    PARTITION (region= 'eu-west-1', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/eu-west-1/2019/02/19/'
    PARTITION (region= 'sa-east-1', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/sa-east-1/2019/02/19/';
```

You can then make the request based on these partitions, and you don't need to load the full bucket\. 

```
SELECT useridentity.arn,
Count(requestid) AS RequestCount
FROM s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table_partitioned
WHERE eventsource='s3.amazonaws.com'
AND json_extract_scalar(additionalEventData, '$.SignatureVersion')='SigV2'
AND region='us-east-1'
AND year='2019'
AND month='02'
AND day='19'
Group by useridentity.arn
```

#### Using AWS CloudTrail to identify access to Amazon S3 objects<a name="cloudtrail-identification-object-access"><!-- omit in toc -->

You can use your AWS CloudTrail event log to identify Amazon S3 object access requests for data events such as GetObject, DeleteObject, and PutObject, and discover further information about those requests\.

The following example shows how to get all PUT object requests for Amazon S3 from the AWS CloudTrail event log\. 

#### Athena query example for identifying Amazon S3 object access requests<a name="ct-examples-identify-object-access-requests"><!-- omit in toc -->

In the following Athena query examples, replace *<s3\_cloudtrail\_events\_db\.cloudtrail\_myawsexamplebucket1\_table>* with your Athena details, and modify the date range as needed\. 

**Example — select all events that have PUT object access requests, and print only EventTime, EventSource, SourceIP, UserAgent, BucketName, object, and UserARN**  

```
SELECT
  eventTime, 
  eventName, 
  eventSource, 
  sourceIpAddress, 
  userAgent, 
  json_extract_scalar(requestParameters, '$.bucketName') as bucketName, 
  json_extract_scalar(requestParameters, '$.key') as object,
  userIdentity.arn as userArn
FROM
  s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket_table
WHERE
  eventName = 'PutObject'
  AND eventTime BETWEEN '2019-07-05T00:00:00Z' and '2019-07-06T00:00:00Z'
```

**Example — select all events that have GET object access requests, and print only EventTime, EventSource, SourceIP, UserAgent, BucketName, object, and UserARN**  

```
SELECT
  eventTime, 
  eventName, 
  eventSource, 
  sourceIpAddress, 
  userAgent, 
  json_extract_scalar(requestParameters, '$.bucketName') as bucketName, 
  json_extract_scalar(requestParameters, '$.key') as object,
  userIdentity.arn as userArn
FROM
  s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket_table
WHERE
  eventName = 'GetObject'
  AND eventTime BETWEEN '2019-07-05T00:00:00Z' and '2019-07-06T00:00:00Z'
```

**Example — select all anonymous requester events to a bucket in a certain period and print only EventTime, EventSource, SourceIP, UserAgent, BucketName, UserIdentity, and UserARN**  

```
SELECT
  eventTime, 
  eventName, 
  eventSource, 
  sourceIpAddress, 
  userAgent, 
  json_extract_scalar(requestParameters, '$.bucketName') as bucketName, 
  userIdentity.arn as userArn,
  userIdentity.principalId 
FROM
  s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket_table
WHERE
  userIdentity.principalId='ANONYMOUS_PRINCIPAL'
  AND eventTime BETWEEN '2019-07-05T00:00:00Z' and '2019-07-06T00:00:00Z'
```

**Note**  
These query examples may also be useful for security monitoring\. You can review the results for `PutObject` or `GetObject` calls from unexpected or unauthorized IP addresses/requesters and for identifying any anonymous requests to your buckets\.
 This query only retrieves information from the time at which logging was enabled\. 

If you are using Amazon S3 server access logs, see [ Using Amazon S3 access logs to identify object access requests](using-s3-access-logs-to-identify-requests.md#using-s3-access-logs-to-identify-objects-access)\.

## Respond/Recover
### 1. Utilize Amazon SNS topics or SQS queue
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|RS.CO-2|Events are reported consistent with established criteria|
|RS.co-3|Information is shared consistent with response plans|
|RS.AN-4| Incidents are categorized consistent with response plans|
|RS.AN-1|Notifications from detection systems are investigated|

**Why?** 

**How?** 
Configure a bucket for notifications \(SNS topic or SQS queue\)<a name="ways-to-add-notification-config-to-bucket"></a>

You can receive Amazon S3 notifications using Amazon Simple Notification Service; or Amazon Simple Queue Service\. In the following walkthrough, you will add a notification configuration to your bucket using an Amazon SNS topic and Amazon SQS queue\.


#### Walkthrough summary<a name="notification-walkthrough-summary"><!-- omit in toc -->
+ Publish events of the `s3:ObjectCreated:*` type to an Amazon SQS queue\.
+ Publish events of the `s3:ReducedRedundancyLostObject` type to an Amazon SNS topic\.

For information about notification configuration, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.

You can do all these steps using the console, without writing any code\. In addition, code examples using AWS SDKs for Java and \.NET are also provided to help you add notification configurations programmatically\.

You can do the following with this walkthrough:

1. Create an Amazon SQS queue\.

   Using the Amazon SQS console, you create an SQS queue\. You can access any messages Amazon S3 sends to the queue programmatically\. But for this walkthrough, you verify notification messages in the console\. 

   You attach an access policy to the topic to grant Amazon S3 permission to post messages\.

1. Create an Amazon SNS topic\.

   Using the Amazon SNS console, you create an SNS topic and subscribe to the topic so that any events posted to it are delivered to you\. You specify email as the communications protocol\. After you create a topic, Amazon SNS sends an email\. You must click a link in the email to confirm the topic subscription\. 

   You attach an access policy to the topic to grant Amazon S3 permission to post messages\. 

1. Add notification configuration to a bucket\. 

#### Step 1: Create an Amazon SQS queue<a name="step1-create-sqs-queue-for-notification"><!-- omit in toc -->

Follow the steps to create and subscribe to an Amazon Simple Queue Service \(Amazon SQS\) queue\.

1. Using the Amazon SQS console, create a queue\. For instructions, see [Getting Started with Amazon SQS](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-getting-started.html) in the *Amazon Simple Queue Service Developer Guide*\. 

1. Replace the access policy attached to the queue with the following policy\. \(In the Amazon SQS console, select the queue, and in the **Permissions** tab, choose **Edit Policy Document \(Advanced\)**\.

   ```
   {
    "Version": "2012-10-17",
    "Id": "example-ID",
    "Statement": [
     {
      "Sid": "example-statement-ID",
      "Effect": "Allow",
      "Principal": {
       "AWS":"*"  
      },
      "Action": [
       "SQS:SendMessage"
      ],
      "Resource": "SQS-queue-ARN",
      "Condition": {
         "ArnLike": { "aws:SourceArn": "arn:aws:s3:*:*:bucket-name" },
         "StringEquals": { "aws:SourceAccount": "bucket-owner-account-id" }
      }
     }
    ]
   }
   ```

1. \(Optional\) If the Amazon SQS queue or the Amazon SNS topic are server\-side encryption enabled with AWS Key Management Service \(AWS KMS\), add the following policy to the associated symmetric customer managed AWS KMS CMK\. 

   You must add the policy to a customer managed CMK because you cannot modify the AWS managed CMK for Amazon SQS or Amazon SNS\. 

   ```
   {
       "Version": "2012-10-17",
       "Id": "example-ID",
       "Statement": [
           {
               "Sid": "example-statement-ID",
               "Effect": "Allow",
               "Principal": {
                   "Service": "s3.amazonaws.com"
               },
               "Action": [
                   "kms:GenerateDataKey",
                   "kms:Decrypt"
               ],
               "Resource": "*"
           }
       ]
   }
   ```

   For more information about using SSE for Amazon SQS and Amazon SNS with AWS KMS, see the following:
   + [Configuring AWS KMS Permissions for Amazon SNS](https://docs.aws.amazon.com/sns/latest/dg/sns-key-management.html) in the *Amazon Simple Notification Service Developer Guide*\.
   + [Configuring AWS KMS Permissions for Amazon SQS](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-key-management.html) in the *Amazon Simple Queue Service Developer Guide*\.

1. Note the queue ARN\. 

   The SQS queue you created is another resource in your AWS account, and it has a unique Amazon Resource Name \(ARN\)\. You will need this ARN in the next step\. The ARN will be of the following format:

   ```
   arn:aws:sqs:aws-region:account-id:queue-name
   ```

#### Step 2: Create an Amazon SNS topic<a name="step1-create-sns-topic-for-notification"><!-- omit in toc -->

Follow the steps to create and subscribe to an Amazon Simple Notification Service \(Amazon SNS\) topic\.

1. Using Amazon SNS console create a topic\. For instructions, see [Create a Topic](https://docs.aws.amazon.com/sns/latest/dg/CreateTopic.html) in the *Amazon Simple Notification Service Developer Guide*\. 

1. Subscribe to the topic\. For this exercise, use email as the communications protocol\. For instructions, see [Subscribe to a Topic](https://docs.aws.amazon.com/sns/latest/dg/SubscribeTopic.html) in the *Amazon Simple Notification Service Developer Guide*\. 

   You will get email requesting you to confirm your subscription to the topic\. Confirm the subscription\. 

1. Replace the access policy attached to the topic with the following policy\. You must update the policy by providing your SNS topic Amazon Resource Name \(ARN\), bucket name, and bucket owner's account ID\.

   ```
   {
    "Version": "2012-10-17",
    "Id": "example-ID",
    "Statement": [
     {
      "Sid": "example-statement-ID",
      "Effect": "Allow",
      "Principal": {
       "AWS":"*"  
      },
      "Action": [
       "SNS:Publish"
      ],
      "Resource": "SNS-topic-ARN",
      "Condition": {
         "ArnLike": { "aws:SourceArn": "arn:aws:s3:*:*:bucket-name" },
         "StringEquals": { "aws:SourceAccount": "bucket-owner-account-id" }
      }
     }
    ]
   }
   ```

1. Note the topic ARN\.

   The SNS topic you created is another resource in your AWS account, and it has a unique Amazon Resource Name \(ARN\)\. You will need this ARN in the next step\. The ARN will be of the following format:

   ```
   arn:aws:sns:aws-region:account-id:topic-name
   ```

#### Step 3: Add a notification configuration to your bucket<a name="step2-enable-notification"><!-- omit in toc -->

You can enable bucket notifications either by using the Amazon S3 console or programmatically by using AWS SDKs\. Choose any one of the options to configure notifications on your bucket\. This section provides code examples using the AWS SDKs for Java and \.NET\.

#### Step 3 \(option a\): Enable notifications on a bucket using the console<a name="step2-enable-notification-using-console"><!-- omit in toc -->

Using the Amazon S3 console, add a notification configuration requesting Amazon S3 to:
+ Publish events of the **All object create events** type to your Amazon SQS queue\.
+ Publish events of the **Object in RRS lost** type to your Amazon SNS topic\.

After you save the notification configuration, Amazon S3 posts a test message, which you get via email\. 

For instructions, see [How Do I Enable and Configure Event Notifications for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-event-notifications.html) in the *Amazon Simple Storage Service Console User Guide*\. 

#### Step 3 \(option b\): Enable notifications on a bucket using the AWS SDK for \.NET<a name="step2-enable-notification-using-awssdk-dotnet"><!-- omit in toc -->

The following C\# code example provides a complete code listing that adds a notification configuration to a bucket\. You need to update the code and provide your bucket name and SNS topic ARN\. 

**Example**  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class EnableNotificationsTest
    {
        private const string bucketName = "*** bucket name ***";
        private const string snsTopic = "*** SNS topic ARN ***";
        private const string sqsQueue = "*** SQS topic ARN ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            EnableNotificationAsync().Wait();
        }

        static async Task EnableNotificationAsync()
        {
            try
            {
               PutBucketNotificationRequest request = new PutBucketNotificationRequest
                {
                    BucketName = bucketName
                };

                TopicConfiguration c = new TopicConfiguration
                {
                    Events = new List<EventType> { EventType.ObjectCreatedCopy },
                    Topic = snsTopic
                };
                request.TopicConfigurations = new List<TopicConfiguration>();
                request.TopicConfigurations.Add(c);
                request.QueueConfigurations = new List<QueueConfiguration>();
                request.QueueConfigurations.Add(new QueueConfiguration()
                {
                    Events = new List<EventType> { EventType.ObjectCreatedPut },
                    Queue = sqsQueue
                });
                
                PutBucketNotificationResponse response = await client.PutBucketNotificationAsync(request);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' ", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown error encountered on server. Message:'{0}' ", e.Message);
            }
        }
    }
}
```

#### Step 3 \(option c\): Enable notifications on a bucket using the AWS SDK for Java<a name="step2-enable-notification-using-java"><!-- omit in toc -->

The following example shows how to add a notification configuration to a bucket\.
**Example**  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.*;

import java.io.IOException;
import java.util.EnumSet;

public class EnableNotificationOnABucket {

    public static void main(String[] args) throws IOException {
        String bucketName = "*** Bucket name ***";
        Regions clientRegion = Regions.DEFAULT_REGION;
        String snsTopicARN = "*** SNS Topic ARN ***";
        String sqsQueueARN = "*** SQS Queue ARN ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();
            BucketNotificationConfiguration notificationConfiguration = new BucketNotificationConfiguration();

            // Add an SNS topic notification.
            notificationConfiguration.addConfiguration("snsTopicConfig",
                    new TopicConfiguration(snsTopicARN, EnumSet.of(S3Event.ObjectCreated)));

            // Add an SQS queue notification.
            notificationConfiguration.addConfiguration("sqsQueueConfig",
                    new QueueConfiguration(sqsQueueARN, EnumSet.of(S3Event.ObjectCreated)));

            // Create the notification configuration request and set the bucket notification configuration.
            SetBucketNotificationConfigurationRequest request = new SetBucketNotificationConfigurationRequest(
                    bucketName, notificationConfiguration);
            s3Client.setBucketNotificationConfiguration(request);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

#### Step 4: Test the setup<a name="notification-walkthrough-1-test"><!-- omit in toc -->

Now you can test the setup by uploading an object to your bucket and verifying the event notification in the Amazon SQS console\. For instructions, see [Receiving a Message](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-getting-started.htmlReceiveMessage.html) in the *Amazon Simple Queue Service Developer Guide "Getting Started" section*\. 


## Endnotes
### Endnote 1 <!-- omit in toc -->
[https://aws.amazon.com/mfa/](https://aws.amazon.com/mfa/)

### Endnote 2 <!-- omit in toc -->
[https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access.html](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access.html)

### Endnote 3 <!-- omit in toc -->
[https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-example-walkthroughs.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-example-walkthroughs.html)

## Capital Group Control Statements 
1. All Data-at-rest must be encrypted and use a CG BYOK encryption key.
2. All Data-in-transit must be encrypted using certificates using CG Certificate Authority.
3. Keys storied in a Key Management System (KMS) should be created by Capital Group's hardware security module (HSM) and are a minimum of AES-256.
4. AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch.
5. AWS IAM User accounts are only to be created for use by services or products that do not support IAM Roles. Services are not allowed to create local accounts for human use within the service. All human user authentication will take place within CG’s Identity Provider.
6. Any AWS service used by CG should not be directly available to the Internet and the default route is always the CG gateway.
7. Use of AWS IAM accounts are restricted to CG networks.
8. AWS IAM User secrets, including passwords and secret access keys, are to be rotated every 90 days. Accounts created locally within any service must also have their secrets rotated every 90 days.
9. Encryption keys are rotated annually.
10. Administrative access to AWS resources will have MFA enabled

## Glossary
**Data** - Digital pieces of information stored or transmitted for use with an information system from which understandable information is derived. Items considered to be data are: Source code, meta-data, build artifacts, information input and output.  
 
**Information System** - An organized assembly of resources and procedures for the collection, processing, maintenance, use, sharing, dissemination, or disposition of information. All systems, platforms, compute instances including and not limited to physical and virtual client endpoints, physical and virtual servers, software containers, databases, Internet of Things (IoT) devices, network devices, applications (internal and external), Serverless computing instances (i.e. AWS Lambda), vendor provided appliances, and third-party platforms, connected to the Capital Group network or used by Capital Group users or customers.
 
**Log** - a record of the events occurring within information systems and networks. Logs are composed of log entries; each entry contains information related to a specific event that has occurred within a system or network.
 
**Information** - communication or representation of knowledge such as facts, data, or opinions in any medium or form, including textual, numerical, graphic, cartographic, narrative, or audiovisual. 
 
**Cloud Computing** - A model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.
 
**Vulnerability**  - Weakness in an information system, system security procedures, internal controls, or implementation that could be exploited or triggered by a threat source. Note: The term weakness is synonymous for deficiency. Weakness may result in security and/or privacy risks.