<img src="https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png" alt="AWS" width="250"/>

<<<<<<< HEAD
# AWS Redshift - Security Playbook <!-- omit in toc -->

## NIST Cybersecurity Framework Alignment <!-- omit in toc -->

**Generated By:**  
[John Soto (JHVS)](https://cgweb3/profile/JHVS)
<br>
Security Engineering

**Last Update:** *04/16/2021*

## Table of Contents <!-- omit in toc -->
- [Disclaimer](#disclaimer)
- [Overview](#overview)
- [Preventative]
- [Detective]
- [Responsive]
- [Endnotes](#endnotes)
- [Capital Group Control Statements](#capital-group-control-statements)
- [Glossary](#glossary)
<br><br>

## Overview
Amazon Redshift is a fully managed, cloud-based, petabyte-scale data warehouse service by Amazon Web Services (AWS). It is an efficient solution to collect and store all your data and enables you to analyze it using various business intelligence tools to acquire new insights for your business and customers.


<img src="/docs/img/redshift/redshift.png" width="800"><br>

### **Core Features of Redshift**
 - Redshift is very fast and performant when it comes to loading data and querying it for analytical and reporting purposes
 - Horizontally scalable
 - Massive Storage Capacity
 - VPC network isolation
 - No servers to manage
 - Various data encryption options are available in multiple places in Redshift


<br>

## Preventative Controls
<img src="/docs/img/Prevent.png" width="50">

### 1. Network controls are restrictive
Amazon Redshift cluster is locked down by default so nobody has access to it. To grant other users inbound access to an Amazon Redshift cluster, you associate the cluster with a security group. If you are on the EC2-VPC platform, you can either use an existing Amazon VPC security group or define a new one. You then associate it with a cluster as described following. If you are on the EC2-Classic platform, you define a cluster security group and associate it with a cluster. For more information on using cluster security groups on the EC2-Classic platform, see [Amazon Redshift Cluster Security Groups](https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-security-groups.html).  This section details the process of deploying a Redshift Cluster in a private VPC on a CG internal subnet. 

**NIST CSF:** <br>

|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.PT-4|Communications and control networks are protected|
|PR.PT-5|Mechanisms (e.g., failsafe, load balancing, hot swap) are implemented to achieve resilience requirements in normal and adverse situations|
|PR.AC-3|Remote access is managed|
|PR.AC-5|Network integrity is protected (e.g., network segregation, network segmentation)|
<br>

**Capital Group:** <br>

|Control Statement|Description|
|------|----------------------|
|6|Any AWS service used by CG should not be directly available to the Internet and the default route is always the CG gateway.|
|7|Use of AWS IAM accounts are restricted to CG networks.|
<br>

**Why?** Multiple layers of security are needed to help ensure that resources are safe from unwanted access. In addition to IAM policies, having strong network controls in place isolates your instances from outside threats. If traffic is not able to reach an instance, then remote threats can be mitigated.

**How?** 
1. Set up a VPC:
    
    - When you launch a Redshift Cluster, you can utilize the VPC that has been generated for your AWS account upon creation of the account.

    - To Determine your VPC identifier: 
        ``` 
        aws ec2 describe-vpcs 
        ```

2. Create / Connect an Amazon Redshift Cluster subnet group to your Amazon Redshift Cluster

    - Your VPC that was created at the creation of your account will have subnets, you can either use the already created subnets or create your own. 
    
    - Why to create your own?
        - A cluster subnet group allows you to specify a set of subnets in your VPC for only your Redshift cluster. When provisioning a cluster you provide the subnet group and Amazon Redshift creates the cluster on one of the subnets in the group.
    
    - To create a subnet:
        
        - [AWS Console](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-getting-started.html)

        - AWS CLI
            ```
            aws ec2 create-subnet --vpc-id vpc-07e8ffd50fEXAMPLE --cidr-block 10.211.169.x/26
            ``` 
    
    - To get the subnets in VPC:
        
        - AWS Console: https://console.aws.amazon.com/vpc/home?region=REGION-HERE#subnets:

        - AWS CLI
            ```
            aws ec2 describe-subnets --filters "Name=vpc-id,Values=vpc-3EXAMPLE"
            ``` 

3. Create / Connect a Security Group to the Redshift Cluster
    - ***NOTE:*** You can create up to 100 VPC security groups for a VPC and associate a VPC security group with many clusters. However, you can only associate up to five VPC security groups with a given cluster.

    - ***Recommended:*** Create your own Security Group for your Redshift Cluster 
        
        - To create your own Security Group:
        
            - [AWS Console](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)
            
            - AWS CLI
                ```
                aws ec2 create-security-group --group-name MySecurityGroup --description "My security group" --vpc-id vpc-1a2b3c4d
                ```

    - To get the ID of a pre-existing Security Group:

        -  AWS Console: https://console.aws.amazon.com/vpc/home?region=REGION-HERE#securityGroups:

        - AWS CLI
            ```
            aws ec2 describe-security-groups --filters "Name=vpc-id,Values=vpc-3EXAMPLE
            ```

4. Create the Redshift Cluster, make the following modifications:

    -  Via AWS Console:
        - Follow the steps in [Getting started with AWS Redshift](https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html)
        - To display the Additional configurations section, switch off Use defaults.
        - In the Network and security section, specify the Virtual private cloud (VPC), Cluster subnet group, and VPC security group that you set up.
    - Via AWS CLI:
        ```
        aws redshift create-cluster --node-type NODE-TYPE-HERE --number-of-nodes x --master-username example-name --master-user-password SuperSecretPasswordHere --cluster-security-groups securityGroup-ID --cluster-subnet-group-name SubnetGroupNames --cluster-identifier mycluster
        ```
<br></br>

### 2. Data is protected at-rest and in-transit- 
By default, Amazon Redshift selects your default key as the master key, however please use CG encryption keys when encrypting your Redshift instance that is package with your AWS account.  In Amazon Redshift, you can enable database encryption for your clusters to help protect data at rest. When you enable encryption for a cluster, the data blocks and system metadata are encrypted for the cluster and its snapshots.  Redshift also allows encryption for data-in-transit please see [Redshift Encryption in Transit](https://docs.aws.amazon.com/redshift/latest/mgmt/security-encryption-in-transit.html).  For more information and learning on encryption with Redshift please read the following documentation by AWS [AWS Redshift Encryption](https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#working-with-aws-kms).
<br></br>

NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|PR.DS-1|Data-at-rest is protected|
|PR.DS-2|Data-in-transit is protected|

<br></br>
Capital Group:
|Control Statement|Description|
|------|----------------------|
|1|All Data-at-rest must be encrypted and use a CG BYOK encryption key.|
|2|All Data-in-transit must be encrypted using certificates using CG Certificate Authority.|
|3|Keys storied in a Key Management System (KMS) should be created by Capital Group's hardware security module (HSM) and are a minimum of AES-256.|

<br></br>
**Why?** AWS Redshift encrypted DB instances provide an additional layer of data protection by securing your data from unauthorized access to the underlying storage. You can use AWS Redshift encryption to increase data protection of your applications deployed in the cloud, and to fulfill compliance requirements for data-at-rest encryption.  

**How?** 
#### Enable encryption of data-at-rest <!-- omit in toc -->
For Amazon Redshift, you can enable database encryption for your clusters to help protect data at rest. When you enable encryption for a cluster, the data blocks and system metadata are encrypted for the cluster and its snapshots.

**In the Console:**  
To enable encryption for a new Redshift Cluster, please follow the steps outlined [here](https://docs.aws.amazon.com/redshift/latest/mgmt/configuring-db-encryption-console.html) by AWS.  Also, please **utilize CG CMKs to encrypt the Cluster** in order to stay compliant with CG standards.

**In the AWS CLI:**  
If you use the `create-cluster` AWS CLI command to create an encrypted DB instance, set the `--encrypted` parameter to `true`. Set the `--kms-key-id` parameter to the Amazon Resource Name (ARN) for the AWS KMS encryption key for the Redshift Cluster instance. 

- Example:
    ```
    aws redshift create-cluster --node-type NODE-TYPE-HERE --number-of-nodes x --master-username example-name /
    --master-user-password SuperSecretPasswordHere --cluster-security-groups securityGroup-ID /
    --cluster-subnet-group-name SubnetGroupNames --cluster-identifier mycluster /
    --encrypted true --kms-key-id arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab
    ```

<br>

#### Enable encryption of data-in-transit <!-- omit in toc -->
You can configure your environment to protect the confidentiality and integrity data in transit.  One of the perks to AWS Redshift is that the service protects your data in transit within the AWS Cloud, Amazon Redshift uses hardware accelerated SSL to communicate with Amazon S3 or Amazon DynamoDB for COPY, UNLOAD, backup, and restore operations.

- [Encryption of data in transit between an Amazon Redshift cluster and SQL clients over JDBC/ODBC](#using-tls-with-mariadb)  
- [Encryption of data in transit between an Amazon Redshift cluster and Amazon S3 or DynamoDB](#using-tls-with-mysql)  
- [Encryption and signing of data in transit between AWS CLI, SDK, or API clients and Amazon Redshift endpoints](#using-tls-with-oracle)  
- [Encryption of data in transit between Amazon Redshift clusters and AQUA](#using-tls-with-postgresql)

<br>

##### Encryption of data in transit between an Amazon Redshift cluster and SQL clients over JDBC/ODBC <!-- omit in toc -->
- You can connect to Amazon Redshift clusters from SQL client tools over Java Database Connectivity (JDBC) and Open Database Connectivity (ODBC) connections.  This connection will be secured upon connection.

- Redshift also supports Secure Sockets Layer (SSL) connections to encrypt data and server certificates to validate the server certificate that the client connects to at the time of connection. The client connects to the leader node of an Amazon Redshift cluster to connect to all instances of the database. The following links below detail how to setup SSL and certificates for Redshift:
    - [Configuring SSL](https://docs.aws.amazon.com/redshift/latest/mgmt/connecting-ssl-support.html)

    - To support SSL connections, Amazon Redshift creates and installs AWS Certificate Manager (ACM) issued certificates on each cluster. To set this up please refere to
    [Using ACM for securing data-in-transit](https://docs.aws.amazon.com/redshift/latest/mgmt/connecting-transitioning-to-acm-certs.html)

    
##### Encryption of data in transit between an Amazon Redshift cluster and Amazon S3 or DynamoDB <!-- omit in toc -->
- Amazon Redshift uses hardware accelerated SSL to communicate with Amazon S3 or DynamoDB for COPY, UNLOAD, backup, and restore operations.

    - Redshift Spectrum supports the Amazon S3 server-side encryption (SSE) using your account's encryption key managed by the AWS Key Management Service (KMS).

    - Encrypt Amazon Redshift loads with Amazon S3 and AWS KMS, for more info please refere to [AWS Documentation](https://aws.amazon.com/blogs/big-data/encrypt-your-amazon-redshift-loads-with-amazon-s3-and-aws-kms/)


##### Encryption and signing of data in transit between AWS CLI, SDK, or API clients and Amazon Redshift endpoints <!-- omit in toc -->
- Amazon Redshift provides HTTPS endpoints for encrypting data in transit.

-   To protect the integrity of API requests to Amazon Redshift, API calls must be signed by the caller. Calls are signed by an X.509 certificate or the customer's AWS secret access key according to the Signature Version 4 Signing Process (Sigv4). 
    - Documentation: [Signature Version 4 Signing Process](https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html) in the AWS General Reference.

- Use the AWS CLI or one of the AWS SDKs to make requests to AWS. These tools automatically sign the requests for you with the access key that you specify when you configure the tools.

##### Encryption of data in transit between Amazon Redshift clusters and AQUA <!-- omit in toc -->
- Data is transmitted between AQUA and Amazon Redshift clusters over a TLS-encrypted channel. This channel is signed according to the Signature Version 4 Signing Process (Sigv4).
    - For what AWS AQUA is, please refere to [Advance Query Accelerator Documentation](https://aws.amazon.com/blogs/aws/new-aqua-advanced-query-accelerator-for-amazon-redshift/)

## Detective
### 1. Amazon Redshift should have automatic upgrades to major versions enabled

**Why?** 
This control checks whether automatic major version upgrades are enabled for the Amazon Redshift cluster\.

Enabling automatic major version upgrades ensures that the latest major version updates to Amazon Redshift clusters are installed during the maintenance window\. These updates might include security patches and bug fixes\. Keeping up-to-date with patch installation is an important step in securing systems\.

**How?** 
Enable Redshift allow version upgrades\.

To enable this option from the AWS CLI, use the Amazon Redshift modify-cluster command to set the --allow-version-upgrade attribute\.

aws redshift modify-cluster --cluster-identifier clustername --allow-version-upgrade

Where clustername is the name of your Amazon Redshift cluster

## Respond/Recover
### 1. Create CloudWatch Alarms to monitor DynamoDB
NIST CSF:
|NIST Subcategory Control|Description|
|-----------|------------------------|
|DE.AE-2|Detected events are analyzed to understand attack targets and methods|
|DE.AE-3|Event data are aggregated and correlated from multiple sources and sensors|
|DE.AE-4|Impact of events is determined|
|DE.AE-5|Incident alert thresholds are established|
|DE.CM-1|The network is monitored to detect potential cybersecurity events|

Capital Group:
|Control Statement|Description|
|------|----------------------|
|4|AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch|

**Why?** 
Understanding instance behavior and having the ability to be notified of anomalies to that behavior is key to securin your DynamoDB deployments

**How?** 
#### Creating CloudWatch Alarms to Monitor DynamoDB<a name="creating-alarms"> <!-- omit in toc -->

You can create a CloudWatch alarm that sends an Amazon SNS message when the alarm changes state\. An alarm watches a single metric over a time period you specify, and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods\. The action is a notification sent to an Amazon SNS topic or Auto Scaling policy\. Alarms invoke actions for sustained state changes only\. CloudWatch alarms do not invoke actions simply because they are in a particular state; the state must have changed and been maintained for a specified number of periods\.

#### How can I be notified before I consume my entire read capacity?<a name="notify-reach-capacity"> <!-- omit in toc -->

1. Create an Amazon SNS topic, `arn:aws:sns:us-east-1:123456789012:capacity-alarm`\.

   For more information, see [Set Up Amazon Simple Notification Service](http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/US_SetupSNS.html)\.

1. Create the alarm\. In this example, we assume a provisioned capacity of five read capacity units\.

   ```
   Prompt>aws cloudwatch put-metric-alarm \
       --alarm-name ReadCapacityUnitsLimitAlarm \
       --alarm-description "Alarm when read capacity reaches 80% of my provisioned read capacity" \
       --namespace AWS/DynamoDB \
       --metric-name ConsumedReadCapacityUnits \
       --dimensions Name=TableName,Value=myTable \
       --statistic Sum \
       --threshold 240 \
       --comparison-operator GreaterThanOrEqualToThreshold \
       --period 60 \                           
       --evaluation-periods 1 \
       --alarm-actions arn:aws:sns:us-east-1:123456789012:capacity-alarm
   ```

1. Test the alarm\.

   ```
   Prompt>aws cloudwatch set-alarm-state --alarm-name ReadCapacityUnitsLimitAlarm --state-reason "initializing" --state-value OK
   ```

   ```
    Prompt>aws cloudwatch set-alarm-state --alarm-name ReadCapacityUnitsLimitAlarm --state-reason "initializing" --state-value ALARM
   ```

**Note**  
The alarm is activated whenever the consumed read capacity is at least 4 units per second \(80% of provisioned read capacity of 5\) for 1 minute \(60 seconds\)\. So the `threshold` is 240 read capacity units \(4 units/sec \* 60 seconds\)\. Any time the read capacity is updated you should update the alarm calculations appropriately\. You can avoid this process by creating alarms through the DynamoDB Console\. In this way, the alarms are automatically updated for you\. 

#### How can I be notified if any requests exceed the provisioned throughput quotas of a table?<a name="notify-exceed-throughput"> <!-- omit in toc -->

1. Create an Amazon SNS topic, `arn:aws:sns:us-east-1:123456789012:requests-exceeding-throughput`\.

   For more information, see [Set Up Amazon Simple Notification Service](http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/US_SetupSNS.html)\.

1. Create the alarm\.

   ```
   Prompt>aws cloudwatch put-metric-alarm \
       --alarm-name RequestsExceedingThroughputAlarm\
       --alarm-description "Alarm when my requests are exceeding provisioned throughput quotas of a table" \
       --namespace AWS/DynamoDB \
       --metric-name ThrottledRequests \
       --dimensions Name=TableName,Value=myTable \
       --statistic Sum \
       --threshold 0 \
       --comparison-operator GreaterThanThreshold \
       --period 300 \
       --unit Count \
       --evaluation-periods 1 \
       --alarm-actions arn:aws:sns:us-east-1:123456789012:requests-exceeding-throughput
   ```

1. Test the alarm\.

   ```
   Prompt>aws cloudwatch set-alarm-state --alarm-name RequestsExceedingThroughputAlarm --state-reason "initializing" --state-value OK
   ```

   ```
   Prompt>aws cloudwatch set-alarm-state --alarm-name RequestsExceedingThroughputAlarm --state-reason "initializing" --state-value ALARM
   ```

#### How can I be notified if any system errors occurred?<a name="notify-system-errors"> <!-- omit in toc -->

1. Create an Amazon SNS topic, `arn:aws:sns:us-east-1:123456789012:notify-on-system-errors`\.

   For more information, see [Set Up Amazon Simple Notification Service](http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/US_SetupSNS.html)\.

1. Create the alarm\.

   ```
   Prompt>aws cloudwatch put-metric-alarm \
       --alarm-name SystemErrorsAlarm \
       --alarm-description "Alarm when system errors occur" \
       --namespace AWS/DynamoDB \
       --metric-name SystemErrors \
       --dimensions Name=TableName,Value=myTable \
       --statistic Sum \
       --threshold 0 \
       --comparison-operator GreaterThanThreshold \
       --period 60 \
       --unit Count \
       --evaluation-periods 1 \
       --alarm-actions arn:aws:sns:us-east-1:123456789012:notify-on-system-errors
   ```

1. Test the alarm\.

   ```
   Prompt>aws cloudwatch set-alarm-state --alarm-name SystemErrorsAlarm --state-reason "initializing" --state-value OK
   ```

   ```
   Prompt>aws cloudwatch set-alarm-state --alarm-name SystemErrorsAlarm --state-reason "initializing" --state-value ALARM
   ```
#### DynamoDB Metrics and Dimensions<a name="metrics-dimensions"> <!-- omit in toc -->

When you interact with DynamoDB, it sends the following metrics and dimensions to CloudWatch\. You can use the following procedures to view the metrics for DynamoDB\.

**To view metrics \(console\)**

Metrics are grouped first by the service namespace, and then by the various dimension combinations within each namespace\.

1. Open the CloudWatch console at [https://console\.aws\.amazon\.com/cloudwatch/](https://console.aws.amazon.com/cloudwatch/)\.

1. In the navigation pane, choose **Metrics**\.

1. Select the **DynamoDB** namespace\.

**To view metrics \(CLI\)**
+ At a command prompt, use the following command:

  ```
  1. aws cloudwatch list-metrics --namespace "AWS/DynamoDB"
  ```

CloudWatch displays the following metrics for DynamoDB:

#### DynamoDB Dimensions and Metrics<a name="dynamodb-metrics-dimensions"> <!-- omit in toc -->

The metrics and dimensions that DynamoDB sends to Amazon CloudWatch are listed here\.

#### DynamoDB Metrics<a name="dynamodb-metrics"> <!-- omit in toc -->

**Note**  
Amazon CloudWatch aggregates the following DynamoDB metrics at one\-minute intervals:  
`ConditionalCheckFailedRequests`
`ConsumedReadCapacityUnits`
`ConsumedWriteCapacityUnits`
`ReadThrottleEvents`
`ReturnedBytes`
`ReturnedItemCount`
`ReturnedRecordsCount`
`SuccessfulRequestLatency`
`SystemErrors`
`TimeToLiveDeletedItemCount`
`ThrottledRequests`
`TransactionConflict`
`UserErrors`
`WriteThrottleEvents`
For all other DynamoDB metrics, the aggregation granularity is five minutes\.

Not all statistics, such as *Average* or *Sum*, are applicable for every metric\. However, all of these values are available through the Amazon DynamoDB console, or by using the CloudWatch console, AWS CLI, or AWS SDKs for all metrics\. In the following table, each metric has a list of valid statistics that are applicable to that metric\.


| Metric | Description | 
| --- | --- | 
| AccountMaxReads |  The maximum number of read capacity units that can be used by an account\. This limit does not apply to on\-demand tables or global secondary indexes\. Units: `Count` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| AccountMaxTableLevelReads |  The maximum number of read capacity units that can be used by a table or global secondary index of an account\. For on\-demand tables this limit caps the maximum read request units a table or a global secondary index can use\. Units: `Count` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| AccountMaxTableLevelWrites |  The maximum number of write capacity units that can be used by a table or global secondary index of an account\. For on\-demand tables this limit caps the maximum write request units a table or a global secondary index can use\. Units: `Count` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| AccountMaxWrites |  The maximum number of write capacity units that can be used by an account\. This limit does not apply to on\-demand tables or global secondary indexes\. Units: `Count` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| AccountProvisionedReadCapacityUtilization |  The percentage of provisioned read capacity units utilized by an account\. Units: `Percent` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| AccountProvisionedWriteCapacityUtilization |  The percentage of provisioned write capacity units utilized by an account\. Units: `Percent` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ConditionalCheckFailedRequests |  The number of failed attempts to perform conditional writes\. The `PutItem`, `UpdateItem`, and `DeleteItem` operations let you provide a logical condition that must evaluate to true before the operation can proceed\. If this condition evaluates to false, `ConditionalCheckFailedRequests` is incremented by one\.  A failed conditional write will result in an HTTP 400 error \(Bad Request\)\. These events are reflected in the `ConditionalCheckFailedRequests` metric, but not in the `UserErrors` metric\.  Units: `Count` Dimensions: `TableName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ConsumedReadCapacityUnits |  The number of read capacity units consumed over the specified time period, so you can track how much of your provisioned throughput is used\. You can retrieve the total consumed read capacity for a table and all of its global secondary indexes, or for a particular global secondary index\. For more information, see [Read/Write Capacity Mode](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughputIntro.html)\.  Use the `Sum` statistic to calculate the consumed throughput\. For example, get the `Sum` value over a span of one minute, and divide it by the number of seconds in a minute \(60\) to calculate the average `ConsumedReadCapacityUnits` per second \(recognizing that this average does not highlight any large but brief spikes in read activity that occurred during that minute\)\. You can compare the calculated value to the provisioned throughput value that you provide DynamoDB\.  Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ConsumedWriteCapacityUnits |  The number of write capacity units consumed over the specified time period, so you can track how much of your provisioned throughput is used\. You can retrieve the total consumed write capacity for a table and all of its global secondary indexes, or for a particular global secondary index\. For more information, see [Read/Write Capacity Mode](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughputIntro.html)\.  Use the `Sum` statistic to calculate the consumed throughput\. For example, get the `Sum` value over a span of one minute, and divide it by the number of seconds in a minute \(60\) to calculate the average `ConsumedWriteCapacityUnits` per second \(recognizing that this average does not highlight any large but brief spikes in write activity that occurred during that minute\)\. You can compare the calculated value to the provisioned throughput value that you provide DynamoDB\.  Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics:  [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| MaxProvisionedTableReadCapacityUtilization |  The percentage of provisioned read capacity units utilized by the highest provisioned read table or global secondary index of an account\. Units: `Percent` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| MaxProvisionedTableWriteCapacityUtilization |  The percentage of provisioned write capacity utilized by the highest provisioned write table or global secondary index of an account\. Units: `Percent` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| OnlineIndexConsumedWriteCapacity |  The number of write capacity units consumed when adding a new global secondary index to a table\. If the write capacity of the index is too low, incoming write activity during the backfill phase might be throttled\. This can increase the time it takes to create the index\. You should monitor this statistic while the index is being built to determine whether the write capacity of the index is underprovisioned\. You can adjust the write capacity of the index using the `UpdateTable` operation, even while the index is still being built\. The `ConsumedWriteCapacityUnits` metric for the index does not include the write throughput consumed during index creation\.  Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| OnlineIndexPercentageProgress |  The percentage of completion when a new global secondary index is being added to a table\. DynamoDB must first allocate resources for the new index, and then backfill attributes from the table into the index\. For large tables, this process might take a long time\. You should monitor this statistic to view the relative progress as DynamoDB builds the index\. Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| OnlineIndexThrottleEvents |  The number of write throttle events that occur when adding a new global secondary index to a table\. These events indicate that the index creation will take longer to complete, because incoming write activity is exceeding the provisioned write throughput of the index\. You can adjust the write capacity of the index using the `UpdateTable` operation, even while the index is still being built\. The `WriteThrottleEvents` metric for the index does not include any throttle events that occur during index creation\.  Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| PendingReplicationCount |  \(This metric is for DynamoDB global tables\.\) The number of item updates that are written to one replica table, but that have not yet been written to another replica in the global table\. Units: `Count`  Dimensions: `TableName, ReceivingRegion` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ProvisionedReadCapacityUnits | The number of provisioned read capacity units for a table or a global secondary index\.The TableName dimension returns the ProvisionedReadCapacityUnits for the table, but not for any global secondary indexes\. To view ProvisionedReadCapacityUnits for a global secondary index, you must specify both TableName and GlobalSecondaryIndex\.Units: `Count`Dimensions: `TableName, GlobalSecondaryIndexName`Valid Statistics:[\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html) | 
| ProvisionedWriteCapacityUnits |  The number of provisioned write capacity units for a table or a global secondary index\. The `TableName` dimension returns the `ProvisionedWriteCapacityUnits` for the table, but not for any global secondary indexes\. To view `ProvisionedWriteCapacityUnits` for a global secondary index, you must specify both `TableName` and `GlobalSecondaryIndex`\. Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ReadThrottleEvents |  Requests to DynamoDB that exceed the provisioned read capacity units for a table or a global secondary index\. A single request can result in multiple events\. For example, a `BatchGetItem` that reads 10 items is processed as 10 `GetItem` events\. For each event, `ReadThrottleEvents` is incremented by one if that event is throttled\. The `ThrottledRequests` metric for the entire `BatchGetItem` is not incremented unless *all 10* of the `GetItem` events are throttled\. The `TableName` dimension returns the `ReadThrottleEvents` for the table, but not for any global secondary indexes\. To view `ReadThrottleEvents` for a global secondary index, you must specify both `TableName` and `GlobalSecondaryIndex`\.  Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ReplicationLatency |  \(This metric is for DynamoDB global tables\.\) The elapsed time between an updated item appearing in the DynamoDB stream for one replica table, and that item appearing in another replica in the global table\.  Units: `Milliseconds`  Dimensions: `TableName, ReceivingRegion` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ReturnedBytes |  The number of bytes returned by `GetRecords` operations \(Amazon DynamoDB Streams\) during the specified time period\. Units: `Bytes` Dimensions: `Operation, StreamLabel, TableName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ReturnedItemCount |  The number of items returned by `Query` or `Scan` operations during the specified time period\. The number of items *returned* is not necessarily the same as the number of items that were evaluated\. For example, suppose that you requested a `Scan` on a table that had 100 items, but specified a `FilterExpression` that narrowed the results so that only 15 items were returned\. In this case, the response from `Scan` would contain a `ScanCount` of 100 and a `Count` of 15 returned items\. Units: `Count` Dimensions: `TableName, Operation` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ReturnedRecordsCount |  The number of stream records returned by `GetRecords` operations \(Amazon DynamoDB Streams\) during the specified time period\. Units: `Count` Dimensions: `Operation, StreamLabel, TableName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| SuccessfulRequestLatency |  The successful requests to DynamoDB or Amazon DynamoDB Streams during the specified time period\. `SuccessfulRequestLatency` can provide two different kinds of information: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html) `SuccessfulRequestLatency` reflects activity only within DynamoDB or Amazon DynamoDB Streams, and does not take into account network latency or client\-side activity\.  Units: `Milliseconds`  Dimensions: `TableName, Operation` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| SystemErrors |  The requests to DynamoDB or Amazon DynamoDB Streams that generate an HTTP 500 status code during the specified time period\. An HTTP 500 usually indicates an internal service error\. Units: `Count` Dimensions: `TableName, Operation` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| TimeToLiveDeletedItemCount |  The number of items deleted by Time to Live \(TTL\) during the specified time period\. This metric helps you monitor the rate of TTL deletions on your table\.  Units: `Count` Dimensions: TableName Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| ThrottledRequests |  Requests to DynamoDB that exceed the provisioned throughput limits on a resource \(such as a table or an index\)\. `ThrottledRequests` is incremented by one if any event within a request exceeds a provisioned throughput limit\. For example, if you update an item in a table with global secondary indexes, there are multiple events—a write to the table, and a write to each index\. If one or more of these events are throttled, then `ThrottledRequests` is incremented by one\.  In a batch request \(`BatchGetItem` or `BatchWriteItem`\), `ThrottledRequests` is incremented only if *every* request in the batch is throttled\. If any individual request within the batch is throttled, one of the following metrics is incremented:   `ReadThrottleEvents` – For a throttled `GetItem` event within `BatchGetItem`\.   `WriteThrottleEvents` – For a throttled `PutItem` or `DeleteItem` event within `BatchWriteItem`\.    To gain insight into which event is throttling a request, compare `ThrottledRequests` with the `ReadThrottleEvents` and `WriteThrottleEvents` for the table and its indexes\.  A throttled request will result in an HTTP 400 status code\. All such events are reflected in the `ThrottledRequests` metric, but not in the `UserErrors` metric\.  Units: `Count` Dimensions: `TableName, Operation` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| TransactionConflict |  Rejected item\-level requests due to transactional conflicts between concurrent requests on the same items\. For more information, see [Transaction Conflict Handling in DynamoDB](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html#transaction-conflict-handling)\.  Units: `Count` Dimensions: `TableName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| UserErrors |  Requests to DynamoDB or Amazon DynamoDB Streams that generate an HTTP 400 status code during the specified time period\. An HTTP 400 usually indicates a client\-side error, such as an invalid combination of parameters, an attempt to update a nonexistent table, or an incorrect request signature\. All such events are reflected in the `UserErrors` metric, except for the following: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html) `UserErrors` represents the aggregate of HTTP 400 errors for DynamoDB or Amazon DynamoDB Streams requests for the current AWS Region and the current AWS account\. Units: `Count` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
| WriteThrottleEvents |  Requests to DynamoDB that exceed the provisioned write capacity units for a table or a global secondary index\. A single request can result in multiple events\. For example, a `PutItem` request on a table with three global secondary indexes would result in four events—the table write, and each of the three index writes\. For each event, the `WriteThrottleEvents` metric is incremented by one if that event is throttled\. For single `PutItem` requests, if any of the events are throttled, `ThrottledRequests` is also incremented by one\. For `BatchWriteItem`, the `ThrottledRequests` metric for the entire `BatchWriteItem` is not incremented unless all of the individual `PutItem` or `DeleteItem` events are throttled\. The `TableName` dimension returns the `WriteThrottleEvents` for the table, but not for any global secondary indexes\. To view `WriteThrottleEvents` for a global secondary index, you must specify both `TableName` and `GlobalSecondaryIndex`\.  Units: `Count` Dimensions: `TableName, GlobalSecondaryIndexName` Valid Statistics: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 

#### Dimensions for DynamoDB Metrics<a name="dynamodb-metric-dimensions"> <!-- omit in toc -->

The metrics for DynamoDB are qualified by the values for the account, table name, global secondary index name, or operation\. You can use the CloudWatch console to retrieve DynamoDB data along any of the dimensions in the table below\.


|  Dimension  |  Description  | 
| --- | --- | 
|  GlobalSecondaryIndexName  |  This dimension limits the data to a global secondary index on a table\. If you specify `GlobalSecondaryIndexName`, you must also specify `TableName`\.  | 
|  Operation  |  This dimension limits the data to one of the following DynamoDB operations: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html) In addition, you can limit the data to the following Amazon DynamoDB Streams operation: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html)  | 
|  ReceivingRegion  |  This dimension limits the data to a particular AWS region\. It is used with metrics originating from replica tables within a DynamoDB global table\.  | 
|  StreamLabel  |  This dimension limits the data to a specific stream label\. It is used with metrics originating from Amazon DynamoDB Streams `GetRecords` operations\.  | 
|  TableName  |  This dimension limits the data to a specific table\. This value can be any table name in the current region and the current AWS account\.  | 


## Endnotes


## Capital Group Control Statements 
1. All Data-at-rest must be encrypted and use a CG BYOK encryption key.
2. All Data-in-transit must be encrypted using certificates using CG Certificate Authority.
3. Keys storied in a Key Management System (KMS) should be created by Capital Group's hardware security module (HSM) and are a minimum of AES-256.
4. AWS services should have logging enabled and those logs delivered to CloudTrail or Cloud Watch.
5. AWS IAM User accounts are only to be created for use by services or products that do not support IAM Roles. Services are not allowed to create local accounts for human use within the service. All human user authentication will take place within CG’s Identity Provider.
6. Any AWS service used by CG should not be directly available to the Internet and the default route is always the CG gateway.
7. Use of AWS IAM accounts are restricted to CG networks.
8. AWS IAM User secrets, including passwords and secret access keys, are to be rotated every 90 days. Accounts created locally within any service must also have their secrets rotated every 90 days.
9. Encryption keys are rotated annually.
10. Administrative access to AWS resources will have MFA enabled

## Glossary
**Data** - Digital pieces of information stored or transmitted for use with an information system from which understandable information is derived. Items considered to be data are: Source code, meta-data, build artifacts, information input and output.  
 
**Information System** - An organized assembly of resources and procedures for the collection, processing, maintenance, use, sharing, dissemination, or disposition of information. All systems, platforms, compute instances including and not limited to physical and virtual client endpoints, physical and virtual servers, software containers, databases, Internet of Things (IoT) devices, network devices, applications (internal and external), Serverless computing instances (i.e. AWS Lambda), vendor provided appliances, and third-party platforms, connected to the Capital Group network or used by Capital Group users or customers.
 
**Log** - a record of the events occurring within information systems and networks. Logs are composed of log entries; each entry contains information related to a specific event that has occurred within a system or network.
 
**Information** - communication or representation of knowledge such as facts, data, or opinions in any medium or form, including textual, numerical, graphic, cartographic, narrative, or audiovisual. 
 
**Cloud Computing** - A model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.
 
**Vulnerability**  - Weakness in an information system, system security procedures, internal controls, or implementation that could be exploited or triggered by a threat source. Note: The term weakness is synonymous for deficiency. Weakness may result in security and/or privacy risks.
=======